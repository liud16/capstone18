{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_smoothing\n",
    "import find_peaks\n",
    "import peak_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nm, data_time, data_z = data_smoothing.load_data('20180418_twogaussian_spectralshfit.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn_contrib_py_earth-0.1.0-py3.5-linux-x86_64.egg/pyearth/earth.py:802: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  pruning_passer.run()\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn_contrib_py_earth-0.1.0-py3.5-linux-x86_64.egg/pyearth/earth.py:1055: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n"
     ]
    }
   ],
   "source": [
    "smooth_matx = data_smoothing.earth_smooth_matrix(data_nm, data_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0c641da691b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmindist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwhm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeak_character\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeak_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_nm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmooth_matx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmindist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/peakaboo/peakaboo/peak_character.py\u001b[0m in \u001b[0;36mpeak_matrix\u001b[0;34m(nm_array, data_matrix, threshold, mindist)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_timeslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdata_timeslice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mpeak_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindpeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_timeslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmindist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "threshold = 0\n",
    "mindist = 0\n",
    "idx, height, fwhm = peak_character.peak_matrix(data_nm,smooth_matx, threshold, mindist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import find_peaks as findpeak\n",
    "\n",
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    " # get peak height and fwhm info\n",
    "def peakchar(data_nm, data_z_array, peak_index):\n",
    "    \"\"\"find the peak width, and intensity\"\"\"\n",
    "    num_peaks = len(peak_index)\n",
    "    \n",
    "    #array of peak height\n",
    "    height = [data_z_array[idx] for idx in peak_index]\n",
    "    \n",
    "    #array of peak width\n",
    "    half_height = [ht / 2 for ht in height]\n",
    "\n",
    "    fwhm_idx_1 = np.empty_like(half_height)\n",
    "    fwhm_idx_2 = np.empty_like(fwhm_idx_1)\n",
    "    fwhm_nm_1 = np.empty_like(fwhm_idx_1)\n",
    "    fwhm_nm_2 = np.empty_like(fwhm_idx_1)\n",
    "    \n",
    "    for i in range(num_peaks):\n",
    "        #find the index and nmof the left side of the fwhm\n",
    "        if i == 0:\n",
    "            fwhm_idx_1[i] = find_nearest(data_z_array[0:peak_index[i]], half_height[i])\n",
    "        else:\n",
    "            fwhm_idx_1[i] = find_nearest(data_z_array[peak_index[i-1]:peak_index[i]], half_height[i]) + peak_index[i-1]\n",
    "\n",
    "        fwhm_nm_1[i] = data_nm[int(fwhm_idx_1[i])]\n",
    "        \n",
    "        #find the index and nm of the right side of the fwhm   \n",
    "        fwhm_idx_2[i] = find_nearest(data_z_array[peak_index[i]:], half_height[i]) + peak_index[i]\n",
    "\n",
    "        fwhm_nm_2[i] = data_nm[int(fwhm_idx_2[i])]\n",
    "    \n",
    "    #find fwhm\n",
    "    fwhm = fwhm_nm_2 - fwhm_nm_1\n",
    "\n",
    "    return height, fwhm\n",
    "\n",
    "\n",
    "def peak_matrix(nm_array,data_matrix, threshold, mindist):\n",
    "    \"\"\"find peaks in a data matrix\n",
    "    and calculate the height and width of the peaks\"\"\"\n",
    "    \n",
    "    peak_idx_matx = []\n",
    "    peak_height_matx = []\n",
    "    peak_fwhm_matx = []\n",
    "    \n",
    "    num_timeslice = np.shape(data_matrix)[1]\n",
    "    \n",
    "    for i in range(num_timeslice):\n",
    "        data_timeslice = data_matrix.values[:, i]\n",
    "        \n",
    "        peak_idx = findpeak.indexes(data_timeslice, threshold, mindist).tolist()\n",
    "        peak_idx_matx.append(peak_idx)\n",
    "        \n",
    "        \n",
    "        peak_height, peak_fwhm = peakchar(nm_array, data_timeslice, peak_idx)\n",
    "        \n",
    "        peak_height_matx.append(peak_height)\n",
    "        peak_fwhm_matx.append(peak_fwhm)\n",
    "        \n",
    "        # transfer to dataframe\n",
    "        peak_idx_df=pd.DataFrame(peak_idx_matx)\n",
    "        peak_height_df=pd.DataFrame(peak_height_matx)\n",
    "        peak_fwhm_df=pd.DataFrame(peak_fwhm_matx)\n",
    "        \n",
    "    return peak_idx_df, peak_height_df, peak_fwhm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "mindist = 0\n",
    "idx, height, fwhm = peak_matrix(data_nm,smooth_matx, threshold, mindist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def data_grouping(index_df, height_df, fwhm_df):\n",
    "    peak_list = []\n",
    "    \n",
    "    for i in range(index_df.shape[0]):\n",
    "        for j in range(index_df.shape[1]):\n",
    "            peak_list.append(\n",
    "            [index_df.loc[i,j], height_df.loc[i,j], fwhm_df.loc[i,j], i])\n",
    "        \n",
    "    all_points = pd.DataFrame(peak_list, \n",
    "    columns=['Position', 'Height', 'Width', 'Time'])\n",
    "    corrected_output = all_points.fillna(value=0)\n",
    "    \n",
    "    return corrected_output\n",
    "\n",
    "def cluster_classifier(index_df, corrected_output):\n",
    "    found_peak = index_df.shape[1]\n",
    "    cluster = KMeans(n_clusters=found_peak).fit(corrected_output.iloc[:,:-1])\n",
    "    peak_dict = {}\n",
    "    \n",
    "    for i in range(found_peak):\n",
    "            peak_dict['peak_%s' % i] = []\n",
    "            \n",
    "    for j in range(index_df.shape[0]):\n",
    "        peak = cluster.predict([corrected_output.values[j,:-1]])\n",
    "        signal = corrected_output.loc[j][1]\n",
    "        for k in range(found_peak):\n",
    "            if (peak == k and (signal >= 0.001 or signal <= -0.001)):\n",
    "                peak_dict['peak_%s' % k].append(corrected_output.values[j])\n",
    "\n",
    "    return peak_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_output = data_grouping(idx, height, fwhm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_dict = cluster_classifier(idx, corrected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
