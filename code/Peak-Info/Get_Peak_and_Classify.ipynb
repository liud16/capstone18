{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import peakutils\n",
    "import syntheticdata\n",
    "import threegaussians\n",
    "import lorentzian\n",
    "from peakutils.plot import plot as pplot\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import interpolate\n",
    "from astropy.modeling import models, fitting\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Earth_Peakutils(nm_array, timedelay,threshold,min_dist):\n",
    "    import numpy\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pyearth import Earth\n",
    "    \n",
    "    \"\"\"\n",
    "     ============================================\n",
    "     Plotting derivatives of simple sine function\n",
    "    ============================================\n",
    "\n",
    "     A simple example plotting a fit of the sine function\n",
    "    and the derivatives computed by Earth.\n",
    "    \n",
    "    Notes\n",
    "    -----   \n",
    "    generates a denoise curve from the TA data\n",
    "    Parameters\n",
    "    ----------\n",
    "        nm_array: wavelength array\n",
    "        timedelay: time delay array\n",
    "        noise_coefficient: the noise coefficients that user want to generate\n",
    "    Returns\n",
    "    -------\n",
    "        a smoothing curve from the original noise curve   \n",
    "    \"\"\"\n",
    "    # Create some fake data\n",
    "    # generate some noisy data from syntheticdata:\n",
    "    np.random.seed(1729)\n",
    "    y_noise = 0.1 * np.random.normal(size=nm_array.size)\n",
    "    ydata = timedelay + y_noise\n",
    "    \n",
    "   # Fit an Earth model\n",
    "    model = Earth(max_degree=2, minspan_alpha=.5, smooth=True)\n",
    "    model.fit(nm_array, ydata)\n",
    "    \n",
    "   # Get the predicted values and derivatives\n",
    "    y_hat = model.predict(nm_array)\n",
    "   \n",
    "    # use peakutils to find peak indexs\n",
    "    peak_indices_true = peakutils.indexes(timedelay, thres=threshold, min_dist=min_dist)\n",
    "    peak_indices_smooth = peakutils.indexes(y_hat, thres=threshold, min_dist=min_dist)\n",
    "    \n",
    "    return peak_indices_true,peak_indices_smooth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astropy_Peakutils(nm_array,timedelay,gg_init,threshold,min_dist):\n",
    "    # Generate fake data\n",
    "    np.random.seed(42)\n",
    "    ydata = timedelay + 0.1*np.random.normal(size=nm_array.size)\n",
    "    # Now to fit the data create a new superposition with initial\n",
    "    # guesses for the parameters:\n",
    "    fitter = fitting.SLSQPLSQFitter()\n",
    "    gg_fit = fitter(gg_init, nm_array, ydata)\n",
    "    # use peakutils to find peak\n",
    "    peak_indices_true = peakutils.indexes(timedelay, thres=threshold, min_dist=min_dist)\n",
    "    peak_indices_smooth = peakutils.indexes(gg_fit(nm_array), thres=threshold, min_dist=min_dist)\n",
    "  \n",
    "    return peak_indices_true,peak_indices_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astropy_Smoothing(nm_array,timedelay,noise_coefficient,gg_init):\n",
    "    # Generate fake data\n",
    "    np.random.seed(42)\n",
    "    ydata = timedelay + noise_coefficient*np.random.normal(size=nm_array.size)\n",
    "    # Now to fit the data create a new superposition with initial\n",
    "    # guesses for the parameters:\n",
    "    fitter = fitting.SLSQPLSQFitter()\n",
    "    gg_fit = fitter(gg_init, nm_array, ydata)\n",
    "  \n",
    "    return gg_fit(nm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Earth_Smoothing(nm_array, y_array,noise_coefficient):        \n",
    "    \"\"\"\n",
    "    ============================================\n",
    "     Plotting derivatives of simple sine function\n",
    "    ============================================\n",
    "\n",
    "     A simple example plotting a fit of the sine function\n",
    "    and the derivatives computed by Earth.\n",
    "    \n",
    "    Notes\n",
    "    -----   \n",
    "    generates a denoise curve from the TA data\n",
    "    Parameters\n",
    "    ----------\n",
    "        nm_array: wavelength array\n",
    "        timedelay: time delay array\n",
    "        noise_coefficient: the noise coefficients that user want to generate\n",
    "    Returns\n",
    "    -------\n",
    "        a smoothing curve from the original noise curve   \n",
    "    \"\"\"\n",
    "    from pyearth import Earth\n",
    "   # Fit an Earth model\n",
    "    model = Earth(smooth=True)\n",
    "    np.random.seed(42)\n",
    "    ydata = y_array + noise_coefficient*np.random.normal(size=nm_array.size)\n",
    "    model.fit(nm_array, ydata)\n",
    "\n",
    "   # Print the model\n",
    "    #print(model.trace())\n",
    "    #print(model.summary())\n",
    "\n",
    "   # Get the predicted values and derivatives\n",
    "    y_hat = model.predict(nm_array)\n",
    "    \n",
    "    return  y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get noise data and smoothing data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astropy_smooth_matrix(nm_array,data_matrix,noise_coefficient,gg_init):\n",
    "    num_array = np.shape(data_matrix)[0]\n",
    "    \n",
    "    smooth_matx = pd.DataFrame(np.empty((num_array,1)), columns = ['a'])\n",
    "    noise_matx = pd.DataFrame(np.empty((num_array,1)), columns = ['a'])\n",
    "    \n",
    "    for i in range(num_array):\n",
    "        data_array = data_matrix[:, i]\n",
    "        \n",
    "        # get noise and smooth list\n",
    "        noise_array = add_noise(nm_array, data_array, noise_coefficient).tolist()\n",
    "        smooth_array = astropy_Smoothing(nm_array,data_array,noise_coefficient,gg_init).tolist()\n",
    "        \n",
    "        # get noise dataframe\n",
    "        DF = pd.DataFrame(noise_array,columns = [i])\n",
    "        noise_matx = noise_matx.join(DF)\n",
    "        \n",
    "        # get smooth dataframe\n",
    "        df = pd.DataFrame(smooth_array,columns = [i])\n",
    "        smooth_matx = smooth_matx.join(df)\n",
    "        \n",
    "    # drop the first columns  \n",
    "    noise_matx = noise_matx.drop(columns='a')\n",
    "    smooth_matx = smooth_matx.drop(columns='a')\n",
    "        \n",
    "    return noise_matx, smooth_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7495624261835365\n",
      "            Iterations: 35\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749416520979112\n",
      "            Iterations: 26\n",
      "            Function evaluations: 213\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494332869998\n",
      "            Iterations: 35\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749375198644923\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749431472355961\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749409344211661\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749200312059827\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749305522959776\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749341610032365\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749320229479094\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74940495470656\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749556724187272\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749288978788597\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749487159993337\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749579139915033\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749348830978332\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749364793534793\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749548188186964\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494724609215515\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749476131372704\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749514754553356\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749526311860512\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74936134379268\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749290476983642\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74925353050398\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749440478034478\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749443685229259\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749262798133592\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749341492081382\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749546103169484\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749321122140511\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749392845514771\n",
      "            Iterations: 35\n",
      "            Function evaluations: 284\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749352302250043\n",
      "            Iterations: 35\n",
      "            Function evaluations: 284\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749345670041293\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749337053264757\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749386666903677\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749403253119601\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749439371319275\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749231583366804\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749372558471001\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749372368287485\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494020076627805\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749120629652938\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749316680378654\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749372267100967\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749412094476595\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749271343513335\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749136488777219\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749301469166571\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749428430760332\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7491513620241\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749236462303185\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749212430677059\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749338069790612\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749245082708288\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749256913800864\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749183444840559\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749462115113701\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749416384528893\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749270546505583\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7492522447814345\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749408976089211\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749192207775534\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749135940666902\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749260313412371\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749185573649983\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749408035574321\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749106048569001\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748951735478756\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749184349268065\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749094885001178\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74920560103935\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749111825641649\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749382666153899\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494059033261555\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749165153153649\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749190553616535\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749211004356601\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749181534395437\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74909389975479\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748982964647777\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749130093999115\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749289068725178\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749215960027268\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7491290293996045\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749187656014968\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7491637043155635\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749159484840783\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7492333082193685\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749272296502928\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749027049491023\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749070979223582\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749260426702647\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749079381412177\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749131220692076\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748944538691722\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748953338807989\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7490043112325715\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74901032280612\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7490293789098335\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74906888696148\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749091380440834\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74899359612705\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749064972231479\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748976949956735\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749055580667931\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748995725763191\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748982712686965\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748896961305074\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748946670206924\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748905525260417\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748865030148847\n",
      "            Iterations: 44\n",
      "            Function evaluations: 358\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748945989357448\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748907217415153\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7489006894493055\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7487818630324705\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748996623462666\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74888844192722\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74879976179185\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748782798182194\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748754145004412\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748900892926467\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748801029238395\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748746895082606\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748711363398641\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748880853869604\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748752416764757\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748661141246047\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748842284263059\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748800957799142\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74874219556267\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74879151595751\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748605768845157\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74865350621814\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748480403121148\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748641106192051\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748586215762827\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748612925128557\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7486866060964035\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748515350125947\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748582160291011\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748604476768035\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748535036931351\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748493557242421\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748494553435412\n",
      "            Iterations: 45\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74866793761032\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748521223952126\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748383480228792\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748426698735318\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748398432246161\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748345432962308\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748429324446976\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7483030746068335\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748428493581717\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74826170887004\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748366768275457\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748337115588772\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748210239120597\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748194345699799\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74832111065758\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748252099354339\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74820392696251\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748257131080866\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748320284191468\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74818832518957\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7482354620186875\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748063618417856\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748193671656978\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748163449402492\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748093303325129\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74796916770591\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747916153442661\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7481260610394305\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7479990118310464\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7478899443030365\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7479975762430495\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747986711530788\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748082809598378\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747879808626496\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747916339284938\n",
      "            Iterations: 47\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747846953213111\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747849960802007\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747823668332578\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747770413081325\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74768671936373\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747855703210931\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7476844645990015\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7476277313108906\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747831596706291\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747749359092728\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747608532795676\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747598379839674\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747657547224416\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747684996663169\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747651492625795\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747426959566901\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747541288579951\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747526753270345\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7475736323967626\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747478833351527\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74753339253865\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7473897525888535\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747363785054652\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747430629208637\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747317194444093\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747349974970602\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747372115786428\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747276952984455\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747259901652029\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747251806878633\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74720190978801\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747095326089495\n",
      "            Iterations: 45\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747185754238129\n",
      "            Iterations: 47\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747161905215407\n",
      "            Iterations: 48\n",
      "            Function evaluations: 391\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747165470136345\n",
      "            Iterations: 50\n",
      "            Function evaluations: 406\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747213473170337\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747101075716355\n",
      "            Iterations: 51\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7472346078466074\n",
      "            Iterations: 50\n",
      "            Function evaluations: 405\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747150564418916\n",
      "            Iterations: 42\n",
      "            Function evaluations: 343\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747168324666045\n",
      "            Iterations: 49\n",
      "            Function evaluations: 399\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747077209858885\n",
      "            Iterations: 51\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747128881752004\n",
      "            Iterations: 51\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747196783018877\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747113668258866\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74719371061318\n",
      "            Iterations: 52\n",
      "            Function evaluations: 423\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747123568681408\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747212291286938\n",
      "            Iterations: 51\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747115154603977\n",
      "            Iterations: 51\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.750784644247817\n",
      "            Iterations: 37\n",
      "            Function evaluations: 303\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747046028868947\n",
      "            Iterations: 49\n",
      "            Function evaluations: 399\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747016779596911\n",
      "            Iterations: 51\n",
      "            Function evaluations: 415\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74719675546109\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747226977064121\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747027838685488\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747121162495429\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747045021356566\n",
      "            Iterations: 50\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74711401180571\n",
      "            Iterations: 51\n",
      "            Function evaluations: 415\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7471956789396685\n",
      "            Iterations: 51\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747117121925669\n",
      "            Iterations: 49\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747102761675739\n",
      "            Iterations: 49\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7512856370174745\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747392746246069\n",
      "            Iterations: 54\n",
      "            Function evaluations: 437\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74718397002313\n",
      "            Iterations: 54\n",
      "            Function evaluations: 437\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747215990424024\n",
      "            Iterations: 55\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747227714960811\n",
      "            Iterations: 55\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7471537547071865\n",
      "            Iterations: 53\n",
      "            Function evaluations: 429\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747239618810001\n",
      "            Iterations: 50\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7514626663894965\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751609119756706\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7472998440776175\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751665438147538\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747423072809673\n",
      "            Iterations: 54\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747393272543821\n",
      "            Iterations: 54\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7517479274923\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75184929586931\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751895991126965\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752018637158235\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751885938948294\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752031701492686\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752079915902906\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752144484494704\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747760562294637\n",
      "            Iterations: 55\n",
      "            Function evaluations: 447\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75222644242307\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752176021150273\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75231996231403\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752387861634235\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7524175457280045\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752497823182225\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752425796896402\n",
      "            Iterations: 40\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752685115957461\n",
      "            Iterations: 40\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748114648964981\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748148042592748\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7481594000508895\n",
      "            Iterations: 57\n",
      "            Function evaluations: 462\n",
      "            Gradient evaluations: 57\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748203266311899\n",
      "            Iterations: 56\n",
      "            Function evaluations: 455\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752874090009354\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75289329308394\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75290130638535\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753073885496827\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753115752745154\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748707388234957\n",
      "            Iterations: 56\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748767697070375\n",
      "            Iterations: 57\n",
      "            Function evaluations: 462\n",
      "            Gradient evaluations: 57\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7487205795910725\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753346898094247\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75330267004631\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753310979964867\n",
      "            Iterations: 35\n",
      "            Function evaluations: 287\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748993419730564\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753625012449016\n",
      "            Iterations: 40\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753651799654451\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753706780541108\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753743463787355\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753834393868917\n",
      "            Iterations: 42\n",
      "            Function evaluations: 342\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749378044095565\n",
      "            Iterations: 58\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 58\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753851921381365\n",
      "            Iterations: 37\n",
      "            Function evaluations: 303\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754057983785377\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75408687833972\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754138762020551\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754211271061097\n",
      "            Iterations: 37\n",
      "            Function evaluations: 304\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754265798627566\n",
      "            Iterations: 40\n",
      "            Function evaluations: 328\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754405242881142\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7544590255311245\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749991957410234\n",
      "            Iterations: 56\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754560566889541\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754586734484815\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754719582057401\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754783768614982\n",
      "            Iterations: 38\n",
      "            Function evaluations: 311\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754913858495211\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754952621622719\n",
      "            Iterations: 43\n",
      "            Function evaluations: 352\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754962077313234\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755014187662138\n",
      "            Iterations: 41\n",
      "            Function evaluations: 337\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755118647079288\n",
      "            Iterations: 38\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75503238573207\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7552122159279815\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7553800572646505\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755363150325243\n",
      "            Iterations: 38\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755553711604055\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755500506137761\n",
      "            Iterations: 42\n",
      "            Function evaluations: 342\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755633165255419\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75551338043862\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7556611827891935\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755724212414469\n",
      "            Iterations: 47\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755840220095371\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755814705337782\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755922975238969\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756080657413387\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7560975783965\n",
      "            Iterations: 45\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7561540878547355\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756364594640262\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772846321082769\n",
      "            Iterations: 29\n",
      "            Function evaluations: 237\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772830138071013\n",
      "            Iterations: 27\n",
      "            Function evaluations: 223\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75651372572233\n",
      "            Iterations: 45\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772896775392429\n",
      "            Iterations: 30\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756663961706618\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7729167886220925\n",
      "            Iterations: 27\n",
      "            Function evaluations: 223\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756761095645025\n",
      "            Iterations: 44\n",
      "            Function evaluations: 359\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772895604833524\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756960026947854\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772936031927706\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7728940553616255\n",
      "            Iterations: 30\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772908338731893\n",
      "            Iterations: 29\n",
      "            Function evaluations: 237\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77286948480983\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772933774512708\n",
      "            Iterations: 30\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773068172536438\n",
      "            Iterations: 28\n",
      "            Function evaluations: 231\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772913853572449\n",
      "            Iterations: 29\n",
      "            Function evaluations: 239\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.757526712892505\n",
      "            Iterations: 45\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7730043141486975\n",
      "            Iterations: 29\n",
      "            Function evaluations: 239\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773008228934704\n",
      "            Iterations: 30\n",
      "            Function evaluations: 248\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7730224268167465\n",
      "            Iterations: 28\n",
      "            Function evaluations: 231\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773059377501074\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773041385229693\n",
      "            Iterations: 31\n",
      "            Function evaluations: 256\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773017374025967\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772961576311316\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773097477755426\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7731375147945485\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773176143999249\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773139237893447\n",
      "            Iterations: 28\n",
      "            Function evaluations: 232\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773114966154575\n",
      "            Iterations: 29\n",
      "            Function evaluations: 238\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773114129475248\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773318663207655\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773146310854472\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773147992252777\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773329489650065\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773165324260232\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773290200179722\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773199245019052\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773219385397935\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773427697987311\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773377931280914\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773228568014874\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773297133330447\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7734169577235015\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773374047566849\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773367601758247\n",
      "            Iterations: 31\n",
      "            Function evaluations: 256\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773359871963092\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773465018011027\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773454735987348\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773380799530295\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773381319411396\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7735008458642865\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7734995569070096\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7735196138831775\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773558746080569\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773410254567464\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773493946537339\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773590818251513\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773557319446657\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77359683320087\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773565249698628\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773655239056619\n",
      "            Iterations: 34\n",
      "            Function evaluations: 280\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7736719155268075\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773625909102958\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773696208353428\n",
      "            Iterations: 30\n",
      "            Function evaluations: 248\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773754705096071\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773731143512379\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773676879914525\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773758199182567\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773585270818314\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77387972918667\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773681707468513\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773816475474296\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773783293994764\n",
      "            Iterations: 31\n",
      "            Function evaluations: 256\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773840196975427\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7737964928605106\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7738112430682165\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773852554336816\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773806594523434\n",
      "            Iterations: 31\n",
      "            Function evaluations: 258\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773933338217876\n",
      "            Iterations: 32\n",
      "            Function evaluations: 266\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773851300083674\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773909152720805\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773947712885036\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773940508827465\n",
      "            Iterations: 32\n",
      "            Function evaluations: 266\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773910178312707\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773954635413543\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774027854803709\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774031903899405\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774102402831328\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774015817032727\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773959731300182\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774066855640488\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774152025297214\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774092898163669\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774032002620352\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774139141445698\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774147515534414\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774182268611911\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774213700052295\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774323315235808\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77430592454492\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774147894310373\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774278412607508\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774284421567199\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774279829081984\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774255553306047\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774285404601775\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774254719478771\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774320854050344\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774403905042709\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774284561574538\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774356732235045\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774407306811416\n",
      "            Iterations: 32\n",
      "            Function evaluations: 266\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774465312610029\n",
      "            Iterations: 34\n",
      "            Function evaluations: 279\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774384021895722\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774395990397547\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774412121023855\n",
      "            Iterations: 29\n",
      "            Function evaluations: 242\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774513145250202\n",
      "            Iterations: 34\n",
      "            Function evaluations: 279\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774439560127461\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774520515175775\n",
      "            Iterations: 34\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774489585989851\n",
      "            Iterations: 34\n",
      "            Function evaluations: 282\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774484805827692\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774498809442136\n",
      "            Iterations: 34\n",
      "            Function evaluations: 280\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774536204159903\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774509354642108\n",
      "            Iterations: 34\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774516068974865\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774548013676938\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774554310124903\n",
      "            Iterations: 35\n",
      "            Function evaluations: 288\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774544111305115\n",
      "            Iterations: 35\n",
      "            Function evaluations: 288\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7746134902315305\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77460197974766\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774523575105703\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774654555242093\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774663156431706\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774601987977107\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774652238885522\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7746717439487565\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774667667494004\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774689713141294\n",
      "            Iterations: 36\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774689548882403\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77471568263059\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7747407480563595\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774739352996761\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774754057859287\n",
      "            Iterations: 36\n",
      "            Function evaluations: 297\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774769774421261\n",
      "            Iterations: 37\n",
      "            Function evaluations: 308\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774781691885833\n",
      "            Iterations: 36\n",
      "            Function evaluations: 300\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774782198734451\n",
      "            Iterations: 36\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774781185725278\n",
      "            Iterations: 35\n",
      "            Function evaluations: 290\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774778681510387\n",
      "            Iterations: 37\n",
      "            Function evaluations: 307\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774796725889008\n",
      "            Iterations: 37\n",
      "            Function evaluations: 307\n",
      "            Gradient evaluations: 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774820424037973\n",
      "            Iterations: 36\n",
      "            Function evaluations: 300\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774826078750889\n",
      "            Iterations: 35\n",
      "            Function evaluations: 290\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774841925358913\n",
      "            Iterations: 35\n",
      "            Function evaluations: 292\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774845225488919\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774851504280549\n",
      "            Iterations: 36\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774854701263748\n",
      "            Iterations: 34\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774885732615523\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774879286064013\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77488114433636\n",
      "            Iterations: 37\n",
      "            Function evaluations: 307\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774897549563491\n",
      "            Iterations: 37\n",
      "            Function evaluations: 308\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774899367988725\n",
      "            Iterations: 38\n",
      "            Function evaluations: 315\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774920393626444\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774931235444745\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774942877008542\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77495545178495\n",
      "            Iterations: 40\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7749503260616075\n",
      "            Iterations: 39\n",
      "            Function evaluations: 323\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7749635209348\n",
      "            Iterations: 38\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774981036601162\n",
      "            Iterations: 37\n",
      "            Function evaluations: 303\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7749909497776795\n",
      "            Iterations: 39\n",
      "            Function evaluations: 322\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775014466572431\n",
      "            Iterations: 43\n",
      "            Function evaluations: 359\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7750121302733985\n",
      "            Iterations: 43\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775016537879161\n",
      "            Iterations: 39\n",
      "            Function evaluations: 322\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775045470466733\n",
      "            Iterations: 42\n",
      "            Function evaluations: 347\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775052005884613\n",
      "            Iterations: 38\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 38\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 500 is out of bounds for axis 1 with size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0e5856768489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnoise_matx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_matx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastropy_smooth_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatanm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataz_matx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgg_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-972652ba7702>\u001b[0m in \u001b[0;36mastropy_smooth_matrix\u001b[0;34m(nm_array, data_matrix, noise_coefficient, gg_init)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdata_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# get noise and smooth list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 500 is out of bounds for axis 1 with size 500"
     ]
    }
   ],
   "source": [
    "noise_matx, smooth_matx = astropy_smooth_matrix(datanm,dataz_matx,0.1,gg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* py-earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earth_smooth_matrix(nm_array,data_matrix,noise_coefficient):\n",
    "    num_array = np.shape(data_matrix)[0]\n",
    "    \n",
    "    smooth_matx = pd.DataFrame(np.empty((num_array,1)), columns = ['a'])\n",
    "    noise_matx = pd.DataFrame(np.empty((num_array,1)), columns = ['a'])\n",
    "    \n",
    "    for i in range(500):\n",
    "        data_array = data_matrix[:, i]\n",
    "        \n",
    "        # get noise and smooth list\n",
    "        noise_array = add_noise(nm_array, data_array, noise_coefficient).tolist()\n",
    "        smooth_array = Earth_Smoothing(nm_array,data_array,noise_coefficient).tolist()\n",
    "        \n",
    "        # get noise dataframe\n",
    "        DF = pd.DataFrame(noise_array,columns = [i])\n",
    "        noise_matx = noise_matx.join(DF)\n",
    "        \n",
    "        # get smooth dataframe\n",
    "        df = pd.DataFrame(smooth_array,columns = [i])\n",
    "        smooth_matx = smooth_matx.join(df)\n",
    "        \n",
    "    # drop the first columns  \n",
    "    noise_matx = noise_matx.drop(columns='a')\n",
    "    smooth_matx = smooth_matx.drop(columns='a')\n",
    "        \n",
    "    return noise_matx, smooth_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn_contrib_py_earth-0.1.0-py3.5-linux-x86_64.egg/pyearth/earth.py:802: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  pruning_passer.run()\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn_contrib_py_earth-0.1.0-py3.5-linux-x86_64.egg/pyearth/earth.py:1055: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n"
     ]
    }
   ],
   "source": [
    "noisez_matx, smooth_matx = earth_smooth_matrix(datanm,dataz_matx,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.642958</td>\n",
       "      <td>0.637558</td>\n",
       "      <td>0.632158</td>\n",
       "      <td>0.626758</td>\n",
       "      <td>0.621458</td>\n",
       "      <td>0.616158</td>\n",
       "      <td>0.610958</td>\n",
       "      <td>0.605758</td>\n",
       "      <td>0.600558</td>\n",
       "      <td>0.595458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037439</td>\n",
       "      <td>0.037421</td>\n",
       "      <td>0.037403</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037368</td>\n",
       "      <td>0.037351</td>\n",
       "      <td>0.037335</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>0.037302</td>\n",
       "      <td>0.037286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750254</td>\n",
       "      <td>0.744754</td>\n",
       "      <td>0.739254</td>\n",
       "      <td>0.733854</td>\n",
       "      <td>0.728454</td>\n",
       "      <td>0.723154</td>\n",
       "      <td>0.717754</td>\n",
       "      <td>0.712554</td>\n",
       "      <td>0.707354</td>\n",
       "      <td>0.702154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.132581</td>\n",
       "      <td>0.132563</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.132526</td>\n",
       "      <td>0.132508</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.132473</td>\n",
       "      <td>0.132456</td>\n",
       "      <td>0.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.516295</td>\n",
       "      <td>0.510695</td>\n",
       "      <td>0.505195</td>\n",
       "      <td>0.499695</td>\n",
       "      <td>0.494295</td>\n",
       "      <td>0.488895</td>\n",
       "      <td>0.483495</td>\n",
       "      <td>0.478195</td>\n",
       "      <td>0.472895</td>\n",
       "      <td>0.467695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113389</td>\n",
       "      <td>-0.113410</td>\n",
       "      <td>-0.113429</td>\n",
       "      <td>-0.113449</td>\n",
       "      <td>-0.113468</td>\n",
       "      <td>-0.113487</td>\n",
       "      <td>-0.113505</td>\n",
       "      <td>-0.113524</td>\n",
       "      <td>-0.113542</td>\n",
       "      <td>-0.113559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.594156</td>\n",
       "      <td>0.588556</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>0.577456</td>\n",
       "      <td>0.571956</td>\n",
       "      <td>0.566456</td>\n",
       "      <td>0.561056</td>\n",
       "      <td>0.555656</td>\n",
       "      <td>0.550356</td>\n",
       "      <td>0.545056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047556</td>\n",
       "      <td>-0.047577</td>\n",
       "      <td>-0.047598</td>\n",
       "      <td>-0.047618</td>\n",
       "      <td>-0.047638</td>\n",
       "      <td>-0.047658</td>\n",
       "      <td>-0.047678</td>\n",
       "      <td>-0.047697</td>\n",
       "      <td>-0.047716</td>\n",
       "      <td>-0.047735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.727119</td>\n",
       "      <td>0.721419</td>\n",
       "      <td>0.715819</td>\n",
       "      <td>0.710219</td>\n",
       "      <td>0.704619</td>\n",
       "      <td>0.699119</td>\n",
       "      <td>0.693619</td>\n",
       "      <td>0.688219</td>\n",
       "      <td>0.682819</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073384</td>\n",
       "      <td>0.073362</td>\n",
       "      <td>0.073340</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>0.073297</td>\n",
       "      <td>0.073276</td>\n",
       "      <td>0.073255</td>\n",
       "      <td>0.073235</td>\n",
       "      <td>0.073214</td>\n",
       "      <td>0.073195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.660581</td>\n",
       "      <td>0.654881</td>\n",
       "      <td>0.649181</td>\n",
       "      <td>0.643581</td>\n",
       "      <td>0.637981</td>\n",
       "      <td>0.632381</td>\n",
       "      <td>0.626881</td>\n",
       "      <td>0.621381</td>\n",
       "      <td>0.615881</td>\n",
       "      <td>0.610481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>-0.005097</td>\n",
       "      <td>-0.005120</td>\n",
       "      <td>-0.005143</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.005188</td>\n",
       "      <td>-0.005209</td>\n",
       "      <td>-0.005231</td>\n",
       "      <td>-0.005252</td>\n",
       "      <td>-0.005273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.863220</td>\n",
       "      <td>0.857420</td>\n",
       "      <td>0.851720</td>\n",
       "      <td>0.846020</td>\n",
       "      <td>0.840320</td>\n",
       "      <td>0.834720</td>\n",
       "      <td>0.829220</td>\n",
       "      <td>0.823620</td>\n",
       "      <td>0.818120</td>\n",
       "      <td>0.812620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185650</td>\n",
       "      <td>0.185626</td>\n",
       "      <td>0.185601</td>\n",
       "      <td>0.185577</td>\n",
       "      <td>0.185553</td>\n",
       "      <td>0.185530</td>\n",
       "      <td>0.185507</td>\n",
       "      <td>0.185484</td>\n",
       "      <td>0.185462</td>\n",
       "      <td>0.185440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.703915</td>\n",
       "      <td>0.698115</td>\n",
       "      <td>0.692315</td>\n",
       "      <td>0.686615</td>\n",
       "      <td>0.680915</td>\n",
       "      <td>0.675215</td>\n",
       "      <td>0.669615</td>\n",
       "      <td>0.664015</td>\n",
       "      <td>0.658515</td>\n",
       "      <td>0.652915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.014483</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.014313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.871805</td>\n",
       "      <td>0.865905</td>\n",
       "      <td>0.860105</td>\n",
       "      <td>0.854305</td>\n",
       "      <td>0.848605</td>\n",
       "      <td>0.842805</td>\n",
       "      <td>0.837205</td>\n",
       "      <td>0.831505</td>\n",
       "      <td>0.825905</td>\n",
       "      <td>0.820405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170618</td>\n",
       "      <td>0.170591</td>\n",
       "      <td>0.170564</td>\n",
       "      <td>0.170537</td>\n",
       "      <td>0.170511</td>\n",
       "      <td>0.170485</td>\n",
       "      <td>0.170459</td>\n",
       "      <td>0.170434</td>\n",
       "      <td>0.170409</td>\n",
       "      <td>0.170385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.807060</td>\n",
       "      <td>0.801160</td>\n",
       "      <td>0.795360</td>\n",
       "      <td>0.789460</td>\n",
       "      <td>0.783760</td>\n",
       "      <td>0.777960</td>\n",
       "      <td>0.772260</td>\n",
       "      <td>0.766560</td>\n",
       "      <td>0.760960</td>\n",
       "      <td>0.755360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094271</td>\n",
       "      <td>0.094243</td>\n",
       "      <td>0.094214</td>\n",
       "      <td>0.094186</td>\n",
       "      <td>0.094158</td>\n",
       "      <td>0.094131</td>\n",
       "      <td>0.094104</td>\n",
       "      <td>0.094078</td>\n",
       "      <td>0.094052</td>\n",
       "      <td>0.094026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.689850</td>\n",
       "      <td>0.683950</td>\n",
       "      <td>0.678050</td>\n",
       "      <td>0.672150</td>\n",
       "      <td>0.666350</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.654750</td>\n",
       "      <td>0.649050</td>\n",
       "      <td>0.643350</td>\n",
       "      <td>0.637750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034535</td>\n",
       "      <td>-0.034566</td>\n",
       "      <td>-0.034596</td>\n",
       "      <td>-0.034625</td>\n",
       "      <td>-0.034655</td>\n",
       "      <td>-0.034683</td>\n",
       "      <td>-0.034711</td>\n",
       "      <td>-0.034739</td>\n",
       "      <td>-0.034767</td>\n",
       "      <td>-0.034794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.893187</td>\n",
       "      <td>0.887187</td>\n",
       "      <td>0.881287</td>\n",
       "      <td>0.875387</td>\n",
       "      <td>0.869487</td>\n",
       "      <td>0.863687</td>\n",
       "      <td>0.857887</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.846387</td>\n",
       "      <td>0.840687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157310</td>\n",
       "      <td>0.157278</td>\n",
       "      <td>0.157246</td>\n",
       "      <td>0.157215</td>\n",
       "      <td>0.157185</td>\n",
       "      <td>0.157154</td>\n",
       "      <td>0.157125</td>\n",
       "      <td>0.157095</td>\n",
       "      <td>0.157067</td>\n",
       "      <td>0.157038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.818009</td>\n",
       "      <td>0.812009</td>\n",
       "      <td>0.806009</td>\n",
       "      <td>0.800109</td>\n",
       "      <td>0.794209</td>\n",
       "      <td>0.788409</td>\n",
       "      <td>0.782509</td>\n",
       "      <td>0.776709</td>\n",
       "      <td>0.771009</td>\n",
       "      <td>0.765309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070845</td>\n",
       "      <td>0.070812</td>\n",
       "      <td>0.070779</td>\n",
       "      <td>0.070746</td>\n",
       "      <td>0.070714</td>\n",
       "      <td>0.070682</td>\n",
       "      <td>0.070651</td>\n",
       "      <td>0.070620</td>\n",
       "      <td>0.070590</td>\n",
       "      <td>0.070560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.947519</td>\n",
       "      <td>0.941419</td>\n",
       "      <td>0.935419</td>\n",
       "      <td>0.929419</td>\n",
       "      <td>0.923519</td>\n",
       "      <td>0.917619</td>\n",
       "      <td>0.911819</td>\n",
       "      <td>0.905919</td>\n",
       "      <td>0.900219</td>\n",
       "      <td>0.894419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189074</td>\n",
       "      <td>0.189039</td>\n",
       "      <td>0.189004</td>\n",
       "      <td>0.188970</td>\n",
       "      <td>0.188936</td>\n",
       "      <td>0.188903</td>\n",
       "      <td>0.188870</td>\n",
       "      <td>0.188837</td>\n",
       "      <td>0.188806</td>\n",
       "      <td>0.188774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.561685</td>\n",
       "      <td>0.555685</td>\n",
       "      <td>0.549585</td>\n",
       "      <td>0.543685</td>\n",
       "      <td>0.537685</td>\n",
       "      <td>0.531785</td>\n",
       "      <td>0.525885</td>\n",
       "      <td>0.519985</td>\n",
       "      <td>0.514185</td>\n",
       "      <td>0.508385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207735</td>\n",
       "      <td>-0.207772</td>\n",
       "      <td>-0.207809</td>\n",
       "      <td>-0.207845</td>\n",
       "      <td>-0.207880</td>\n",
       "      <td>-0.207915</td>\n",
       "      <td>-0.207950</td>\n",
       "      <td>-0.207984</td>\n",
       "      <td>-0.208017</td>\n",
       "      <td>-0.208050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.696854</td>\n",
       "      <td>0.690754</td>\n",
       "      <td>0.684654</td>\n",
       "      <td>0.678654</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.666754</td>\n",
       "      <td>0.660854</td>\n",
       "      <td>0.654954</td>\n",
       "      <td>0.649054</td>\n",
       "      <td>0.643254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083535</td>\n",
       "      <td>-0.083574</td>\n",
       "      <td>-0.083613</td>\n",
       "      <td>-0.083651</td>\n",
       "      <td>-0.083688</td>\n",
       "      <td>-0.083725</td>\n",
       "      <td>-0.083761</td>\n",
       "      <td>-0.083797</td>\n",
       "      <td>-0.083832</td>\n",
       "      <td>-0.083866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.635469</td>\n",
       "      <td>0.629369</td>\n",
       "      <td>0.623269</td>\n",
       "      <td>0.617169</td>\n",
       "      <td>0.611169</td>\n",
       "      <td>0.605269</td>\n",
       "      <td>0.599269</td>\n",
       "      <td>0.593369</td>\n",
       "      <td>0.587469</td>\n",
       "      <td>0.581669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155684</td>\n",
       "      <td>-0.155725</td>\n",
       "      <td>-0.155765</td>\n",
       "      <td>-0.155805</td>\n",
       "      <td>-0.155844</td>\n",
       "      <td>-0.155882</td>\n",
       "      <td>-0.155920</td>\n",
       "      <td>-0.155958</td>\n",
       "      <td>-0.155995</td>\n",
       "      <td>-0.156031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.874754</td>\n",
       "      <td>0.868654</td>\n",
       "      <td>0.862554</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>0.850454</td>\n",
       "      <td>0.844454</td>\n",
       "      <td>0.838454</td>\n",
       "      <td>0.832554</td>\n",
       "      <td>0.826654</td>\n",
       "      <td>0.820754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073044</td>\n",
       "      <td>0.073001</td>\n",
       "      <td>0.072959</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.072876</td>\n",
       "      <td>0.072836</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>0.072757</td>\n",
       "      <td>0.072718</td>\n",
       "      <td>0.072680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.938265</td>\n",
       "      <td>0.932165</td>\n",
       "      <td>0.926065</td>\n",
       "      <td>0.919965</td>\n",
       "      <td>0.913865</td>\n",
       "      <td>0.907865</td>\n",
       "      <td>0.901865</td>\n",
       "      <td>0.895965</td>\n",
       "      <td>0.890065</td>\n",
       "      <td>0.884165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126205</td>\n",
       "      <td>0.126160</td>\n",
       "      <td>0.126115</td>\n",
       "      <td>0.126072</td>\n",
       "      <td>0.126029</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.125944</td>\n",
       "      <td>0.125903</td>\n",
       "      <td>0.125863</td>\n",
       "      <td>0.125823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.938089</td>\n",
       "      <td>0.931889</td>\n",
       "      <td>0.925789</td>\n",
       "      <td>0.919689</td>\n",
       "      <td>0.913589</td>\n",
       "      <td>0.907589</td>\n",
       "      <td>0.901589</td>\n",
       "      <td>0.895589</td>\n",
       "      <td>0.889689</td>\n",
       "      <td>0.883789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115785</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>0.115691</td>\n",
       "      <td>0.115646</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.115556</td>\n",
       "      <td>0.115512</td>\n",
       "      <td>0.115469</td>\n",
       "      <td>0.115426</td>\n",
       "      <td>0.115384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.717956</td>\n",
       "      <td>0.711756</td>\n",
       "      <td>0.705656</td>\n",
       "      <td>0.699556</td>\n",
       "      <td>0.693456</td>\n",
       "      <td>0.687456</td>\n",
       "      <td>0.681456</td>\n",
       "      <td>0.675456</td>\n",
       "      <td>0.669556</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114285</td>\n",
       "      <td>-0.114334</td>\n",
       "      <td>-0.114383</td>\n",
       "      <td>-0.114431</td>\n",
       "      <td>-0.114478</td>\n",
       "      <td>-0.114525</td>\n",
       "      <td>-0.114570</td>\n",
       "      <td>-0.114616</td>\n",
       "      <td>-0.114660</td>\n",
       "      <td>-0.114704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.755214</td>\n",
       "      <td>0.749014</td>\n",
       "      <td>0.742914</td>\n",
       "      <td>0.736814</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.724714</td>\n",
       "      <td>0.718614</td>\n",
       "      <td>0.712714</td>\n",
       "      <td>0.706714</td>\n",
       "      <td>0.700814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086756</td>\n",
       "      <td>-0.086808</td>\n",
       "      <td>-0.086859</td>\n",
       "      <td>-0.086909</td>\n",
       "      <td>-0.086959</td>\n",
       "      <td>-0.087007</td>\n",
       "      <td>-0.087055</td>\n",
       "      <td>-0.087103</td>\n",
       "      <td>-0.087150</td>\n",
       "      <td>-0.087196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.673962</td>\n",
       "      <td>0.667762</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.655562</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.643362</td>\n",
       "      <td>0.637362</td>\n",
       "      <td>0.631362</td>\n",
       "      <td>0.625462</td>\n",
       "      <td>0.619462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177530</td>\n",
       "      <td>-0.177584</td>\n",
       "      <td>-0.177638</td>\n",
       "      <td>-0.177690</td>\n",
       "      <td>-0.177742</td>\n",
       "      <td>-0.177793</td>\n",
       "      <td>-0.177843</td>\n",
       "      <td>-0.177893</td>\n",
       "      <td>-0.177942</td>\n",
       "      <td>-0.177990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.942117</td>\n",
       "      <td>0.935917</td>\n",
       "      <td>0.929817</td>\n",
       "      <td>0.923617</td>\n",
       "      <td>0.917617</td>\n",
       "      <td>0.911517</td>\n",
       "      <td>0.905517</td>\n",
       "      <td>0.899517</td>\n",
       "      <td>0.893517</td>\n",
       "      <td>0.887617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081310</td>\n",
       "      <td>0.081254</td>\n",
       "      <td>0.081198</td>\n",
       "      <td>0.081143</td>\n",
       "      <td>0.081089</td>\n",
       "      <td>0.081036</td>\n",
       "      <td>0.080983</td>\n",
       "      <td>0.080931</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>0.080829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.887098</td>\n",
       "      <td>0.880898</td>\n",
       "      <td>0.874798</td>\n",
       "      <td>0.868698</td>\n",
       "      <td>0.862598</td>\n",
       "      <td>0.856498</td>\n",
       "      <td>0.850498</td>\n",
       "      <td>0.844498</td>\n",
       "      <td>0.838598</td>\n",
       "      <td>0.832598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>0.017110</td>\n",
       "      <td>0.017054</td>\n",
       "      <td>0.016998</td>\n",
       "      <td>0.016943</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.016782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.080096</td>\n",
       "      <td>1.073896</td>\n",
       "      <td>1.067796</td>\n",
       "      <td>1.061696</td>\n",
       "      <td>1.055596</td>\n",
       "      <td>1.049496</td>\n",
       "      <td>1.043496</td>\n",
       "      <td>1.037496</td>\n",
       "      <td>1.031596</td>\n",
       "      <td>1.025596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201485</td>\n",
       "      <td>0.201423</td>\n",
       "      <td>0.201362</td>\n",
       "      <td>0.201302</td>\n",
       "      <td>0.201243</td>\n",
       "      <td>0.201185</td>\n",
       "      <td>0.201127</td>\n",
       "      <td>0.201070</td>\n",
       "      <td>0.201014</td>\n",
       "      <td>0.200959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.891969</td>\n",
       "      <td>0.885769</td>\n",
       "      <td>0.879669</td>\n",
       "      <td>0.873569</td>\n",
       "      <td>0.867469</td>\n",
       "      <td>0.861469</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>0.849469</td>\n",
       "      <td>0.843469</td>\n",
       "      <td>0.837569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.004740</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.004319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.865321</td>\n",
       "      <td>0.859121</td>\n",
       "      <td>0.853021</td>\n",
       "      <td>0.846921</td>\n",
       "      <td>0.840921</td>\n",
       "      <td>0.834921</td>\n",
       "      <td>0.828921</td>\n",
       "      <td>0.822921</td>\n",
       "      <td>0.816921</td>\n",
       "      <td>0.811021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029961</td>\n",
       "      <td>-0.030028</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>-0.030160</td>\n",
       "      <td>-0.030225</td>\n",
       "      <td>-0.030288</td>\n",
       "      <td>-0.030351</td>\n",
       "      <td>-0.030413</td>\n",
       "      <td>-0.030475</td>\n",
       "      <td>-0.030535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.896608</td>\n",
       "      <td>0.890508</td>\n",
       "      <td>0.884408</td>\n",
       "      <td>0.878308</td>\n",
       "      <td>0.872308</td>\n",
       "      <td>0.866308</td>\n",
       "      <td>0.860308</td>\n",
       "      <td>0.854308</td>\n",
       "      <td>0.848408</td>\n",
       "      <td>0.842508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006546</td>\n",
       "      <td>-0.006616</td>\n",
       "      <td>-0.006685</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>-0.006821</td>\n",
       "      <td>-0.006888</td>\n",
       "      <td>-0.006953</td>\n",
       "      <td>-0.007018</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>-0.007145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.929026</td>\n",
       "      <td>0.922926</td>\n",
       "      <td>0.916826</td>\n",
       "      <td>0.910726</td>\n",
       "      <td>0.904726</td>\n",
       "      <td>0.898726</td>\n",
       "      <td>0.892726</td>\n",
       "      <td>0.886826</td>\n",
       "      <td>0.880926</td>\n",
       "      <td>0.875026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018210</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>0.018064</td>\n",
       "      <td>0.017993</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.017650</td>\n",
       "      <td>0.017584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>-0.069598</td>\n",
       "      <td>-0.069676</td>\n",
       "      <td>-0.069753</td>\n",
       "      <td>-0.069829</td>\n",
       "      <td>-0.069905</td>\n",
       "      <td>-0.069980</td>\n",
       "      <td>-0.070054</td>\n",
       "      <td>-0.070127</td>\n",
       "      <td>-0.070200</td>\n",
       "      <td>-0.070272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077376</td>\n",
       "      <td>-0.077377</td>\n",
       "      <td>-0.077377</td>\n",
       "      <td>-0.077378</td>\n",
       "      <td>-0.077378</td>\n",
       "      <td>-0.077379</td>\n",
       "      <td>-0.077379</td>\n",
       "      <td>-0.077380</td>\n",
       "      <td>-0.077380</td>\n",
       "      <td>-0.077381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.000683</td>\n",
       "      <td>-0.000683</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>-0.112389</td>\n",
       "      <td>-0.112463</td>\n",
       "      <td>-0.112536</td>\n",
       "      <td>-0.112608</td>\n",
       "      <td>-0.112680</td>\n",
       "      <td>-0.112751</td>\n",
       "      <td>-0.112821</td>\n",
       "      <td>-0.112891</td>\n",
       "      <td>-0.112960</td>\n",
       "      <td>-0.113028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119757</td>\n",
       "      <td>-0.119758</td>\n",
       "      <td>-0.119758</td>\n",
       "      <td>-0.119759</td>\n",
       "      <td>-0.119759</td>\n",
       "      <td>-0.119760</td>\n",
       "      <td>-0.119760</td>\n",
       "      <td>-0.119761</td>\n",
       "      <td>-0.119761</td>\n",
       "      <td>-0.119762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.015058</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.014919</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.008098</td>\n",
       "      <td>0.008098</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0.042936</td>\n",
       "      <td>0.042866</td>\n",
       "      <td>0.042796</td>\n",
       "      <td>0.042728</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.042460</td>\n",
       "      <td>0.042395</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.035957</td>\n",
       "      <td>0.035957</td>\n",
       "      <td>0.035956</td>\n",
       "      <td>0.035956</td>\n",
       "      <td>0.035955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0.128387</td>\n",
       "      <td>0.128319</td>\n",
       "      <td>0.128252</td>\n",
       "      <td>0.128185</td>\n",
       "      <td>0.128119</td>\n",
       "      <td>0.128054</td>\n",
       "      <td>0.127989</td>\n",
       "      <td>0.127925</td>\n",
       "      <td>0.127862</td>\n",
       "      <td>0.127799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.121599</td>\n",
       "      <td>0.121599</td>\n",
       "      <td>0.121598</td>\n",
       "      <td>0.121598</td>\n",
       "      <td>0.121597</td>\n",
       "      <td>0.121597</td>\n",
       "      <td>0.121596</td>\n",
       "      <td>0.121596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0.107364</td>\n",
       "      <td>0.107298</td>\n",
       "      <td>0.107233</td>\n",
       "      <td>0.107168</td>\n",
       "      <td>0.107104</td>\n",
       "      <td>0.107040</td>\n",
       "      <td>0.106977</td>\n",
       "      <td>0.106915</td>\n",
       "      <td>0.106853</td>\n",
       "      <td>0.106792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100762</td>\n",
       "      <td>0.100761</td>\n",
       "      <td>0.100761</td>\n",
       "      <td>0.100760</td>\n",
       "      <td>0.100760</td>\n",
       "      <td>0.100759</td>\n",
       "      <td>0.100759</td>\n",
       "      <td>0.100758</td>\n",
       "      <td>0.100758</td>\n",
       "      <td>0.100758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>-0.048265</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>-0.048393</td>\n",
       "      <td>-0.048456</td>\n",
       "      <td>-0.048519</td>\n",
       "      <td>-0.048581</td>\n",
       "      <td>-0.048642</td>\n",
       "      <td>-0.048702</td>\n",
       "      <td>-0.048763</td>\n",
       "      <td>-0.048822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.054689</td>\n",
       "      <td>-0.054689</td>\n",
       "      <td>-0.054690</td>\n",
       "      <td>-0.054690</td>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.054692</td>\n",
       "      <td>-0.054692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0.031933</td>\n",
       "      <td>0.031870</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>0.031747</td>\n",
       "      <td>0.031686</td>\n",
       "      <td>0.031626</td>\n",
       "      <td>0.031566</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>0.031449</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>0.025685</td>\n",
       "      <td>0.025685</td>\n",
       "      <td>0.025684</td>\n",
       "      <td>0.025684</td>\n",
       "      <td>0.025683</td>\n",
       "      <td>0.025683</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>0.025682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0.070909</td>\n",
       "      <td>0.070848</td>\n",
       "      <td>0.070788</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>0.070669</td>\n",
       "      <td>0.070611</td>\n",
       "      <td>0.070553</td>\n",
       "      <td>0.070495</td>\n",
       "      <td>0.070438</td>\n",
       "      <td>0.070382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064833</td>\n",
       "      <td>0.064833</td>\n",
       "      <td>0.064833</td>\n",
       "      <td>0.064832</td>\n",
       "      <td>0.064832</td>\n",
       "      <td>0.064831</td>\n",
       "      <td>0.064831</td>\n",
       "      <td>0.064830</td>\n",
       "      <td>0.064830</td>\n",
       "      <td>0.064830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>0.009118</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.008948</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.003382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.014095</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.013980</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.013868</td>\n",
       "      <td>0.013812</td>\n",
       "      <td>0.013758</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.013597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.008349</td>\n",
       "      <td>0.008349</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.008346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>-0.134642</td>\n",
       "      <td>-0.134698</td>\n",
       "      <td>-0.134753</td>\n",
       "      <td>-0.134808</td>\n",
       "      <td>-0.134863</td>\n",
       "      <td>-0.134916</td>\n",
       "      <td>-0.134970</td>\n",
       "      <td>-0.135022</td>\n",
       "      <td>-0.135075</td>\n",
       "      <td>-0.135126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140227</td>\n",
       "      <td>-0.140228</td>\n",
       "      <td>-0.140228</td>\n",
       "      <td>-0.140229</td>\n",
       "      <td>-0.140229</td>\n",
       "      <td>-0.140230</td>\n",
       "      <td>-0.140230</td>\n",
       "      <td>-0.140230</td>\n",
       "      <td>-0.140231</td>\n",
       "      <td>-0.140231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>-0.031896</td>\n",
       "      <td>-0.031950</td>\n",
       "      <td>-0.032004</td>\n",
       "      <td>-0.032057</td>\n",
       "      <td>-0.032110</td>\n",
       "      <td>-0.032162</td>\n",
       "      <td>-0.032214</td>\n",
       "      <td>-0.032266</td>\n",
       "      <td>-0.032316</td>\n",
       "      <td>-0.032367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037325</td>\n",
       "      <td>-0.037325</td>\n",
       "      <td>-0.037326</td>\n",
       "      <td>-0.037326</td>\n",
       "      <td>-0.037327</td>\n",
       "      <td>-0.037327</td>\n",
       "      <td>-0.037327</td>\n",
       "      <td>-0.037328</td>\n",
       "      <td>-0.037328</td>\n",
       "      <td>-0.037328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>-0.068313</td>\n",
       "      <td>-0.068366</td>\n",
       "      <td>-0.068418</td>\n",
       "      <td>-0.068470</td>\n",
       "      <td>-0.068521</td>\n",
       "      <td>-0.068572</td>\n",
       "      <td>-0.068622</td>\n",
       "      <td>-0.068672</td>\n",
       "      <td>-0.068721</td>\n",
       "      <td>-0.068770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073590</td>\n",
       "      <td>-0.073591</td>\n",
       "      <td>-0.073591</td>\n",
       "      <td>-0.073591</td>\n",
       "      <td>-0.073592</td>\n",
       "      <td>-0.073592</td>\n",
       "      <td>-0.073592</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.073594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0.153460</td>\n",
       "      <td>0.153409</td>\n",
       "      <td>0.153358</td>\n",
       "      <td>0.153307</td>\n",
       "      <td>0.153258</td>\n",
       "      <td>0.153208</td>\n",
       "      <td>0.153159</td>\n",
       "      <td>0.153111</td>\n",
       "      <td>0.153063</td>\n",
       "      <td>0.153015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148331</td>\n",
       "      <td>0.148330</td>\n",
       "      <td>0.148330</td>\n",
       "      <td>0.148330</td>\n",
       "      <td>0.148329</td>\n",
       "      <td>0.148329</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>0.148327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>-0.067402</td>\n",
       "      <td>-0.067452</td>\n",
       "      <td>-0.067502</td>\n",
       "      <td>-0.067551</td>\n",
       "      <td>-0.067599</td>\n",
       "      <td>-0.067647</td>\n",
       "      <td>-0.067695</td>\n",
       "      <td>-0.067742</td>\n",
       "      <td>-0.067788</td>\n",
       "      <td>-0.067835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072387</td>\n",
       "      <td>-0.072388</td>\n",
       "      <td>-0.072388</td>\n",
       "      <td>-0.072388</td>\n",
       "      <td>-0.072389</td>\n",
       "      <td>-0.072389</td>\n",
       "      <td>-0.072389</td>\n",
       "      <td>-0.072390</td>\n",
       "      <td>-0.072390</td>\n",
       "      <td>-0.072390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>-0.065712</td>\n",
       "      <td>-0.065761</td>\n",
       "      <td>-0.065809</td>\n",
       "      <td>-0.065856</td>\n",
       "      <td>-0.065903</td>\n",
       "      <td>-0.065950</td>\n",
       "      <td>-0.065996</td>\n",
       "      <td>-0.066042</td>\n",
       "      <td>-0.066087</td>\n",
       "      <td>-0.066132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070557</td>\n",
       "      <td>-0.070557</td>\n",
       "      <td>-0.070557</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>-0.070559</td>\n",
       "      <td>-0.070559</td>\n",
       "      <td>-0.070559</td>\n",
       "      <td>-0.070560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.178001</td>\n",
       "      <td>0.177953</td>\n",
       "      <td>0.177907</td>\n",
       "      <td>0.177860</td>\n",
       "      <td>0.177815</td>\n",
       "      <td>0.177769</td>\n",
       "      <td>0.177724</td>\n",
       "      <td>0.177680</td>\n",
       "      <td>0.177636</td>\n",
       "      <td>0.177592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173293</td>\n",
       "      <td>0.173293</td>\n",
       "      <td>0.173292</td>\n",
       "      <td>0.173292</td>\n",
       "      <td>0.173292</td>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.173290</td>\n",
       "      <td>0.173290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0.037454</td>\n",
       "      <td>0.037408</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>0.037273</td>\n",
       "      <td>0.037229</td>\n",
       "      <td>0.037185</td>\n",
       "      <td>0.037142</td>\n",
       "      <td>0.037099</td>\n",
       "      <td>0.037057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032879</td>\n",
       "      <td>0.032879</td>\n",
       "      <td>0.032879</td>\n",
       "      <td>0.032878</td>\n",
       "      <td>0.032878</td>\n",
       "      <td>0.032878</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.032877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.048236</td>\n",
       "      <td>0.048192</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.048104</td>\n",
       "      <td>0.048061</td>\n",
       "      <td>0.048018</td>\n",
       "      <td>0.047976</td>\n",
       "      <td>0.047934</td>\n",
       "      <td>0.047892</td>\n",
       "      <td>0.047851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.043792</td>\n",
       "      <td>0.043792</td>\n",
       "      <td>0.043792</td>\n",
       "      <td>0.043791</td>\n",
       "      <td>0.043791</td>\n",
       "      <td>0.043791</td>\n",
       "      <td>0.043790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>0.102988</td>\n",
       "      <td>0.102945</td>\n",
       "      <td>0.102902</td>\n",
       "      <td>0.102860</td>\n",
       "      <td>0.102818</td>\n",
       "      <td>0.102776</td>\n",
       "      <td>0.102735</td>\n",
       "      <td>0.102694</td>\n",
       "      <td>0.102654</td>\n",
       "      <td>0.102614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098673</td>\n",
       "      <td>0.098672</td>\n",
       "      <td>0.098672</td>\n",
       "      <td>0.098672</td>\n",
       "      <td>0.098671</td>\n",
       "      <td>0.098671</td>\n",
       "      <td>0.098671</td>\n",
       "      <td>0.098670</td>\n",
       "      <td>0.098670</td>\n",
       "      <td>0.098670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>-0.038899</td>\n",
       "      <td>-0.038941</td>\n",
       "      <td>-0.038982</td>\n",
       "      <td>-0.039024</td>\n",
       "      <td>-0.039064</td>\n",
       "      <td>-0.039105</td>\n",
       "      <td>-0.039145</td>\n",
       "      <td>-0.039184</td>\n",
       "      <td>-0.039224</td>\n",
       "      <td>-0.039262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043091</td>\n",
       "      <td>-0.043091</td>\n",
       "      <td>-0.043092</td>\n",
       "      <td>-0.043092</td>\n",
       "      <td>-0.043092</td>\n",
       "      <td>-0.043093</td>\n",
       "      <td>-0.043093</td>\n",
       "      <td>-0.043093</td>\n",
       "      <td>-0.043093</td>\n",
       "      <td>-0.043094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0.173325</td>\n",
       "      <td>0.173285</td>\n",
       "      <td>0.173244</td>\n",
       "      <td>0.173204</td>\n",
       "      <td>0.173165</td>\n",
       "      <td>0.173125</td>\n",
       "      <td>0.173087</td>\n",
       "      <td>0.173048</td>\n",
       "      <td>0.173010</td>\n",
       "      <td>0.172972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.169254</td>\n",
       "      <td>0.169253</td>\n",
       "      <td>0.169253</td>\n",
       "      <td>0.169253</td>\n",
       "      <td>0.169252</td>\n",
       "      <td>0.169252</td>\n",
       "      <td>0.169252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.097708</td>\n",
       "      <td>0.097668</td>\n",
       "      <td>0.097629</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>0.097552</td>\n",
       "      <td>0.097514</td>\n",
       "      <td>0.097476</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>0.097402</td>\n",
       "      <td>0.097365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.093754</td>\n",
       "      <td>0.093754</td>\n",
       "      <td>0.093754</td>\n",
       "      <td>0.093753</td>\n",
       "      <td>0.093753</td>\n",
       "      <td>0.093753</td>\n",
       "      <td>0.093753</td>\n",
       "      <td>0.093752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.034224</td>\n",
       "      <td>0.034186</td>\n",
       "      <td>0.034148</td>\n",
       "      <td>0.034110</td>\n",
       "      <td>0.034073</td>\n",
       "      <td>0.034036</td>\n",
       "      <td>0.033999</td>\n",
       "      <td>0.033963</td>\n",
       "      <td>0.033927</td>\n",
       "      <td>0.033891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030386</td>\n",
       "      <td>0.030386</td>\n",
       "      <td>0.030386</td>\n",
       "      <td>0.030385</td>\n",
       "      <td>0.030385</td>\n",
       "      <td>0.030385</td>\n",
       "      <td>0.030385</td>\n",
       "      <td>0.030384</td>\n",
       "      <td>0.030384</td>\n",
       "      <td>0.030384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.194378</td>\n",
       "      <td>0.194341</td>\n",
       "      <td>0.194305</td>\n",
       "      <td>0.194268</td>\n",
       "      <td>0.194232</td>\n",
       "      <td>0.194197</td>\n",
       "      <td>0.194162</td>\n",
       "      <td>0.194127</td>\n",
       "      <td>0.194092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190689</td>\n",
       "      <td>0.190689</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.190687</td>\n",
       "      <td>0.190687</td>\n",
       "      <td>0.190687</td>\n",
       "      <td>0.190687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.004310</td>\n",
       "      <td>-0.004346</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>-0.004417</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>-0.004486</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>-0.004554</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007892</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.007895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.051392</td>\n",
       "      <td>0.051357</td>\n",
       "      <td>0.051322</td>\n",
       "      <td>0.051288</td>\n",
       "      <td>0.051253</td>\n",
       "      <td>0.051220</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>0.051153</td>\n",
       "      <td>0.051120</td>\n",
       "      <td>0.051088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047880</td>\n",
       "      <td>0.047880</td>\n",
       "      <td>0.047880</td>\n",
       "      <td>0.047880</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.047878</td>\n",
       "      <td>0.047878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-0.121125</td>\n",
       "      <td>-0.121159</td>\n",
       "      <td>-0.121193</td>\n",
       "      <td>-0.121226</td>\n",
       "      <td>-0.121260</td>\n",
       "      <td>-0.121292</td>\n",
       "      <td>-0.121325</td>\n",
       "      <td>-0.121357</td>\n",
       "      <td>-0.121389</td>\n",
       "      <td>-0.121421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124534</td>\n",
       "      <td>-0.124534</td>\n",
       "      <td>-0.124534</td>\n",
       "      <td>-0.124534</td>\n",
       "      <td>-0.124535</td>\n",
       "      <td>-0.124535</td>\n",
       "      <td>-0.124535</td>\n",
       "      <td>-0.124535</td>\n",
       "      <td>-0.124536</td>\n",
       "      <td>-0.124536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows  500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.642958  0.637558  0.632158  0.626758  0.621458  0.616158  0.610958   \n",
       "1    0.750254  0.744754  0.739254  0.733854  0.728454  0.723154  0.717754   \n",
       "2    0.516295  0.510695  0.505195  0.499695  0.494295  0.488895  0.483495   \n",
       "3    0.594156  0.588556  0.582956  0.577456  0.571956  0.566456  0.561056   \n",
       "4    0.727119  0.721419  0.715819  0.710219  0.704619  0.699119  0.693619   \n",
       "5    0.660581  0.654881  0.649181  0.643581  0.637981  0.632381  0.626881   \n",
       "6    0.863220  0.857420  0.851720  0.846020  0.840320  0.834720  0.829220   \n",
       "7    0.703915  0.698115  0.692315  0.686615  0.680915  0.675215  0.669615   \n",
       "8    0.871805  0.865905  0.860105  0.854305  0.848605  0.842805  0.837205   \n",
       "9    0.807060  0.801160  0.795360  0.789460  0.783760  0.777960  0.772260   \n",
       "10   0.689850  0.683950  0.678050  0.672150  0.666350  0.660550  0.654750   \n",
       "11   0.893187  0.887187  0.881287  0.875387  0.869487  0.863687  0.857887   \n",
       "12   0.818009  0.812009  0.806009  0.800109  0.794209  0.788409  0.782509   \n",
       "13   0.947519  0.941419  0.935419  0.929419  0.923519  0.917619  0.911819   \n",
       "14   0.561685  0.555685  0.549585  0.543685  0.537685  0.531785  0.525885   \n",
       "15   0.696854  0.690754  0.684654  0.678654  0.672654  0.666754  0.660854   \n",
       "16   0.635469  0.629369  0.623269  0.617169  0.611169  0.605269  0.599269   \n",
       "17   0.874754  0.868654  0.862554  0.856454  0.850454  0.844454  0.838454   \n",
       "18   0.938265  0.932165  0.926065  0.919965  0.913865  0.907865  0.901865   \n",
       "19   0.938089  0.931889  0.925789  0.919689  0.913589  0.907589  0.901589   \n",
       "20   0.717956  0.711756  0.705656  0.699556  0.693456  0.687456  0.681456   \n",
       "21   0.755214  0.749014  0.742914  0.736814  0.730714  0.724714  0.718614   \n",
       "22   0.673962  0.667762  0.661662  0.655562  0.649462  0.643362  0.637362   \n",
       "23   0.942117  0.935917  0.929817  0.923617  0.917617  0.911517  0.905517   \n",
       "24   0.887098  0.880898  0.874798  0.868698  0.862598  0.856498  0.850498   \n",
       "25   1.080096  1.073896  1.067796  1.061696  1.055596  1.049496  1.043496   \n",
       "26   0.891969  0.885769  0.879669  0.873569  0.867469  0.861469  0.855469   \n",
       "27   0.865321  0.859121  0.853021  0.846921  0.840921  0.834921  0.828921   \n",
       "28   0.896608  0.890508  0.884408  0.878308  0.872308  0.866308  0.860308   \n",
       "29   0.929026  0.922926  0.916826  0.910726  0.904726  0.898726  0.892726   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "670 -0.069598 -0.069676 -0.069753 -0.069829 -0.069905 -0.069980 -0.070054   \n",
       "671  0.006889  0.006813  0.006738  0.006664  0.006590  0.006517  0.006445   \n",
       "672 -0.112389 -0.112463 -0.112536 -0.112608 -0.112680 -0.112751 -0.112821   \n",
       "673  0.015271  0.015200  0.015128  0.015058  0.014988  0.014919  0.014851   \n",
       "674  0.042936  0.042866  0.042796  0.042728  0.042660  0.042593  0.042526   \n",
       "675  0.128387  0.128319  0.128252  0.128185  0.128119  0.128054  0.127989   \n",
       "676  0.107364  0.107298  0.107233  0.107168  0.107104  0.107040  0.106977   \n",
       "677 -0.048265 -0.048329 -0.048393 -0.048456 -0.048519 -0.048581 -0.048642   \n",
       "678  0.031933  0.031870  0.031808  0.031747  0.031686  0.031626  0.031566   \n",
       "679  0.070909  0.070848  0.070788  0.070728  0.070669  0.070611  0.070553   \n",
       "680  0.009294  0.009235  0.009176  0.009118  0.009061  0.009004  0.008948   \n",
       "681  0.014095  0.014037  0.013980  0.013924  0.013868  0.013812  0.013758   \n",
       "682 -0.134642 -0.134698 -0.134753 -0.134808 -0.134863 -0.134916 -0.134970   \n",
       "683 -0.031896 -0.031950 -0.032004 -0.032057 -0.032110 -0.032162 -0.032214   \n",
       "684 -0.068313 -0.068366 -0.068418 -0.068470 -0.068521 -0.068572 -0.068622   \n",
       "685  0.153460  0.153409  0.153358  0.153307  0.153258  0.153208  0.153159   \n",
       "686 -0.067402 -0.067452 -0.067502 -0.067551 -0.067599 -0.067647 -0.067695   \n",
       "687 -0.065712 -0.065761 -0.065809 -0.065856 -0.065903 -0.065950 -0.065996   \n",
       "688  0.178001  0.177953  0.177907  0.177860  0.177815  0.177769  0.177724   \n",
       "689  0.037454  0.037408  0.037362  0.037318  0.037273  0.037229  0.037185   \n",
       "690  0.048236  0.048192  0.048148  0.048104  0.048061  0.048018  0.047976   \n",
       "691  0.102988  0.102945  0.102902  0.102860  0.102818  0.102776  0.102735   \n",
       "692 -0.038899 -0.038941 -0.038982 -0.039024 -0.039064 -0.039105 -0.039145   \n",
       "693  0.173325  0.173285  0.173244  0.173204  0.173165  0.173125  0.173087   \n",
       "694  0.097708  0.097668  0.097629  0.097590  0.097552  0.097514  0.097476   \n",
       "695  0.034224  0.034186  0.034148  0.034110  0.034073  0.034036  0.033999   \n",
       "696  0.194416  0.194378  0.194341  0.194305  0.194268  0.194232  0.194197   \n",
       "697 -0.004274 -0.004310 -0.004346 -0.004382 -0.004417 -0.004452 -0.004486   \n",
       "698  0.051392  0.051357  0.051322  0.051288  0.051253  0.051220  0.051186   \n",
       "699 -0.121125 -0.121159 -0.121193 -0.121226 -0.121260 -0.121292 -0.121325   \n",
       "\n",
       "          7         8         9      ...          490       491       492  \\\n",
       "0    0.605758  0.600558  0.595458    ...     0.037439  0.037421  0.037403   \n",
       "1    0.712554  0.707354  0.702154    ...     0.132600  0.132581  0.132563   \n",
       "2    0.478195  0.472895  0.467695    ...    -0.113389 -0.113410 -0.113429   \n",
       "3    0.555656  0.550356  0.545056    ...    -0.047556 -0.047577 -0.047598   \n",
       "4    0.688219  0.682819  0.677419    ...     0.073384  0.073362  0.073340   \n",
       "5    0.621381  0.615881  0.610481    ...    -0.005073 -0.005097 -0.005120   \n",
       "6    0.823620  0.818120  0.812620    ...     0.185650  0.185626  0.185601   \n",
       "7    0.664015  0.658515  0.652915    ...     0.014535  0.014508  0.014483   \n",
       "8    0.831505  0.825905  0.820405    ...     0.170618  0.170591  0.170564   \n",
       "9    0.766560  0.760960  0.755360    ...     0.094271  0.094243  0.094214   \n",
       "10   0.649050  0.643350  0.637750    ...    -0.034535 -0.034566 -0.034596   \n",
       "11   0.852087  0.846387  0.840687    ...     0.157310  0.157278  0.157246   \n",
       "12   0.776709  0.771009  0.765309    ...     0.070845  0.070812  0.070779   \n",
       "13   0.905919  0.900219  0.894419    ...     0.189074  0.189039  0.189004   \n",
       "14   0.519985  0.514185  0.508385    ...    -0.207735 -0.207772 -0.207809   \n",
       "15   0.654954  0.649054  0.643254    ...    -0.083535 -0.083574 -0.083613   \n",
       "16   0.593369  0.587469  0.581669    ...    -0.155684 -0.155725 -0.155765   \n",
       "17   0.832554  0.826654  0.820754    ...     0.073044  0.073001  0.072959   \n",
       "18   0.895965  0.890065  0.884165    ...     0.126205  0.126160  0.126115   \n",
       "19   0.895589  0.889689  0.883789    ...     0.115785  0.115738  0.115691   \n",
       "20   0.675456  0.669556  0.663556    ...    -0.114285 -0.114334 -0.114383   \n",
       "21   0.712714  0.706714  0.700814    ...    -0.086756 -0.086808 -0.086859   \n",
       "22   0.631362  0.625462  0.619462    ...    -0.177530 -0.177584 -0.177638   \n",
       "23   0.899517  0.893517  0.887617    ...     0.081310  0.081254  0.081198   \n",
       "24   0.844498  0.838598  0.832598    ...     0.017285  0.017226  0.017168   \n",
       "25   1.037496  1.031596  1.025596    ...     0.201485  0.201423  0.201362   \n",
       "26   0.849469  0.843469  0.837569    ...     0.004868  0.004804  0.004740   \n",
       "27   0.822921  0.816921  0.811021    ...    -0.029961 -0.030028 -0.030094   \n",
       "28   0.854308  0.848408  0.842508    ...    -0.006546 -0.006616 -0.006685   \n",
       "29   0.886826  0.880926  0.875026    ...     0.018210  0.018136  0.018064   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "670 -0.070127 -0.070200 -0.070272    ...    -0.077376 -0.077377 -0.077377   \n",
       "671  0.006374  0.006303  0.006233    ...    -0.000681 -0.000681 -0.000682   \n",
       "672 -0.112891 -0.112960 -0.113028    ...    -0.119757 -0.119758 -0.119758   \n",
       "673  0.014783  0.014716  0.014650    ...     0.008102  0.008101  0.008100   \n",
       "674  0.042460  0.042395  0.042331    ...     0.035960  0.035959  0.035959   \n",
       "675  0.127925  0.127862  0.127799    ...     0.121600  0.121600  0.121599   \n",
       "676  0.106915  0.106853  0.106792    ...     0.100762  0.100761  0.100761   \n",
       "677 -0.048702 -0.048763 -0.048822    ...    -0.054688 -0.054688 -0.054689   \n",
       "678  0.031507  0.031449  0.031391    ...     0.025686  0.025685  0.025685   \n",
       "679  0.070495  0.070438  0.070382    ...     0.064833  0.064833  0.064833   \n",
       "680  0.008892  0.008837  0.008782    ...     0.003386  0.003386  0.003385   \n",
       "681  0.013703  0.013650  0.013597    ...     0.008350  0.008349  0.008349   \n",
       "682 -0.135022 -0.135075 -0.135126    ...    -0.140227 -0.140228 -0.140228   \n",
       "683 -0.032266 -0.032316 -0.032367    ...    -0.037325 -0.037325 -0.037326   \n",
       "684 -0.068672 -0.068721 -0.068770    ...    -0.073590 -0.073591 -0.073591   \n",
       "685  0.153111  0.153063  0.153015    ...     0.148331  0.148330  0.148330   \n",
       "686 -0.067742 -0.067788 -0.067835    ...    -0.072387 -0.072388 -0.072388   \n",
       "687 -0.066042 -0.066087 -0.066132    ...    -0.070557 -0.070557 -0.070557   \n",
       "688  0.177680  0.177636  0.177592    ...     0.173293  0.173293  0.173292   \n",
       "689  0.037142  0.037099  0.037057    ...     0.032880  0.032880  0.032879   \n",
       "690  0.047934  0.047892  0.047851    ...     0.043793  0.043793  0.043793   \n",
       "691  0.102694  0.102654  0.102614    ...     0.098673  0.098672  0.098672   \n",
       "692 -0.039184 -0.039224 -0.039262    ...    -0.043091 -0.043091 -0.043092   \n",
       "693  0.173048  0.173010  0.172972    ...     0.169254  0.169254  0.169254   \n",
       "694  0.097439  0.097402  0.097365    ...     0.093755  0.093755  0.093754   \n",
       "695  0.033963  0.033927  0.033891    ...     0.030386  0.030386  0.030386   \n",
       "696  0.194162  0.194127  0.194092    ...     0.190689  0.190689  0.190688   \n",
       "697 -0.004521 -0.004554 -0.004588    ...    -0.007892 -0.007893 -0.007893   \n",
       "698  0.051153  0.051120  0.051088    ...     0.047880  0.047880  0.047880   \n",
       "699 -0.121357 -0.121389 -0.121421    ...    -0.124534 -0.124534 -0.124534   \n",
       "\n",
       "          493       494       495       496       497       498       499  \n",
       "0    0.037385  0.037368  0.037351  0.037335  0.037318  0.037302  0.037286  \n",
       "1    0.132544  0.132526  0.132508  0.132490  0.132473  0.132456  0.132440  \n",
       "2   -0.113449 -0.113468 -0.113487 -0.113505 -0.113524 -0.113542 -0.113559  \n",
       "3   -0.047618 -0.047638 -0.047658 -0.047678 -0.047697 -0.047716 -0.047735  \n",
       "4    0.073318  0.073297  0.073276  0.073255  0.073235  0.073214  0.073195  \n",
       "5   -0.005143 -0.005165 -0.005188 -0.005209 -0.005231 -0.005252 -0.005273  \n",
       "6    0.185577  0.185553  0.185530  0.185507  0.185484  0.185462  0.185440  \n",
       "7    0.014457  0.014432  0.014408  0.014383  0.014360  0.014336  0.014313  \n",
       "8    0.170537  0.170511  0.170485  0.170459  0.170434  0.170409  0.170385  \n",
       "9    0.094186  0.094158  0.094131  0.094104  0.094078  0.094052  0.094026  \n",
       "10  -0.034625 -0.034655 -0.034683 -0.034711 -0.034739 -0.034767 -0.034794  \n",
       "11   0.157215  0.157185  0.157154  0.157125  0.157095  0.157067  0.157038  \n",
       "12   0.070746  0.070714  0.070682  0.070651  0.070620  0.070590  0.070560  \n",
       "13   0.188970  0.188936  0.188903  0.188870  0.188837  0.188806  0.188774  \n",
       "14  -0.207845 -0.207880 -0.207915 -0.207950 -0.207984 -0.208017 -0.208050  \n",
       "15  -0.083651 -0.083688 -0.083725 -0.083761 -0.083797 -0.083832 -0.083866  \n",
       "16  -0.155805 -0.155844 -0.155882 -0.155920 -0.155958 -0.155995 -0.156031  \n",
       "17   0.072917  0.072876  0.072836  0.072796  0.072757  0.072718  0.072680  \n",
       "18   0.126072  0.126029  0.125986  0.125944  0.125903  0.125863  0.125823  \n",
       "19   0.115646  0.115600  0.115556  0.115512  0.115469  0.115426  0.115384  \n",
       "20  -0.114431 -0.114478 -0.114525 -0.114570 -0.114616 -0.114660 -0.114704  \n",
       "21  -0.086909 -0.086959 -0.087007 -0.087055 -0.087103 -0.087150 -0.087196  \n",
       "22  -0.177690 -0.177742 -0.177793 -0.177843 -0.177893 -0.177942 -0.177990  \n",
       "23   0.081143  0.081089  0.081036  0.080983  0.080931  0.080880  0.080829  \n",
       "24   0.017110  0.017054  0.016998  0.016943  0.016888  0.016835  0.016782  \n",
       "25   0.201302  0.201243  0.201185  0.201127  0.201070  0.201014  0.200959  \n",
       "26   0.004677  0.004615  0.004554  0.004494  0.004435  0.004376  0.004319  \n",
       "27  -0.030160 -0.030225 -0.030288 -0.030351 -0.030413 -0.030475 -0.030535  \n",
       "28  -0.006754 -0.006821 -0.006888 -0.006953 -0.007018 -0.007082 -0.007145  \n",
       "29   0.017993  0.017922  0.017853  0.017784  0.017717  0.017650  0.017584  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "670 -0.077378 -0.077378 -0.077379 -0.077379 -0.077380 -0.077380 -0.077381  \n",
       "671 -0.000683 -0.000683 -0.000684 -0.000684 -0.000685 -0.000685 -0.000686  \n",
       "672 -0.119759 -0.119759 -0.119760 -0.119760 -0.119761 -0.119761 -0.119762  \n",
       "673  0.008100  0.008099  0.008099  0.008098  0.008098  0.008097  0.008097  \n",
       "674  0.035958  0.035958  0.035957  0.035957  0.035956  0.035956  0.035955  \n",
       "675  0.121599  0.121598  0.121598  0.121597  0.121597  0.121596  0.121596  \n",
       "676  0.100760  0.100760  0.100759  0.100759  0.100758  0.100758  0.100758  \n",
       "677 -0.054689 -0.054690 -0.054690 -0.054691 -0.054691 -0.054692 -0.054692  \n",
       "678  0.025684  0.025684  0.025683  0.025683  0.025682  0.025682  0.025682  \n",
       "679  0.064832  0.064832  0.064831  0.064831  0.064830  0.064830  0.064830  \n",
       "680  0.003385  0.003384  0.003384  0.003383  0.003383  0.003383  0.003382  \n",
       "681  0.008348  0.008348  0.008348  0.008347  0.008347  0.008346  0.008346  \n",
       "682 -0.140229 -0.140229 -0.140230 -0.140230 -0.140230 -0.140231 -0.140231  \n",
       "683 -0.037326 -0.037327 -0.037327 -0.037327 -0.037328 -0.037328 -0.037328  \n",
       "684 -0.073591 -0.073592 -0.073592 -0.073592 -0.073593 -0.073593 -0.073594  \n",
       "685  0.148330  0.148329  0.148329  0.148328  0.148328  0.148328  0.148327  \n",
       "686 -0.072388 -0.072389 -0.072389 -0.072389 -0.072390 -0.072390 -0.072390  \n",
       "687 -0.070558 -0.070558 -0.070558 -0.070559 -0.070559 -0.070559 -0.070560  \n",
       "688  0.173292  0.173292  0.173291  0.173291  0.173291  0.173290  0.173290  \n",
       "689  0.032879  0.032879  0.032878  0.032878  0.032878  0.032877  0.032877  \n",
       "690  0.043792  0.043792  0.043792  0.043791  0.043791  0.043791  0.043790  \n",
       "691  0.098672  0.098671  0.098671  0.098671  0.098670  0.098670  0.098670  \n",
       "692 -0.043092 -0.043092 -0.043093 -0.043093 -0.043093 -0.043093 -0.043094  \n",
       "693  0.169254  0.169253  0.169253  0.169253  0.169252  0.169252  0.169252  \n",
       "694  0.093754  0.093754  0.093753  0.093753  0.093753  0.093753  0.093752  \n",
       "695  0.030385  0.030385  0.030385  0.030385  0.030384  0.030384  0.030384  \n",
       "696  0.190688  0.190688  0.190688  0.190687  0.190687  0.190687  0.190687  \n",
       "697 -0.007893 -0.007893 -0.007894 -0.007894 -0.007894 -0.007894 -0.007895  \n",
       "698  0.047880  0.047879  0.047879  0.047879  0.047879  0.047878  0.047878  \n",
       "699 -0.124534 -0.124535 -0.124535 -0.124535 -0.124535 -0.124536 -0.124536  \n",
       "\n",
       "[700 rows x 500 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisez_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.645373</td>\n",
       "      <td>0.639381</td>\n",
       "      <td>0.633468</td>\n",
       "      <td>0.627576</td>\n",
       "      <td>0.611674</td>\n",
       "      <td>0.618547</td>\n",
       "      <td>0.611966</td>\n",
       "      <td>0.634780</td>\n",
       "      <td>0.606053</td>\n",
       "      <td>0.601244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>-0.010655</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.031775</td>\n",
       "      <td>0.067144</td>\n",
       "      <td>0.066869</td>\n",
       "      <td>-0.011714</td>\n",
       "      <td>0.067293</td>\n",
       "      <td>-0.019757</td>\n",
       "      <td>-0.016914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.653034</td>\n",
       "      <td>0.647048</td>\n",
       "      <td>0.641138</td>\n",
       "      <td>0.635249</td>\n",
       "      <td>0.619956</td>\n",
       "      <td>0.626084</td>\n",
       "      <td>0.619535</td>\n",
       "      <td>0.640981</td>\n",
       "      <td>0.613364</td>\n",
       "      <td>0.608437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012922</td>\n",
       "      <td>-0.011706</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.030959</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.059681</td>\n",
       "      <td>-0.012675</td>\n",
       "      <td>0.059804</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>-0.016342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.660695</td>\n",
       "      <td>0.654715</td>\n",
       "      <td>0.648808</td>\n",
       "      <td>0.642922</td>\n",
       "      <td>0.628238</td>\n",
       "      <td>0.633620</td>\n",
       "      <td>0.627105</td>\n",
       "      <td>0.647182</td>\n",
       "      <td>0.620675</td>\n",
       "      <td>0.615629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>-0.012756</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.030143</td>\n",
       "      <td>0.052657</td>\n",
       "      <td>0.052494</td>\n",
       "      <td>-0.013637</td>\n",
       "      <td>0.052315</td>\n",
       "      <td>-0.018556</td>\n",
       "      <td>-0.015771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.668357</td>\n",
       "      <td>0.662381</td>\n",
       "      <td>0.656478</td>\n",
       "      <td>0.650595</td>\n",
       "      <td>0.636520</td>\n",
       "      <td>0.641157</td>\n",
       "      <td>0.634674</td>\n",
       "      <td>0.653383</td>\n",
       "      <td>0.627986</td>\n",
       "      <td>0.622822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>-0.013806</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.029326</td>\n",
       "      <td>0.045413</td>\n",
       "      <td>0.045306</td>\n",
       "      <td>-0.014598</td>\n",
       "      <td>0.044826</td>\n",
       "      <td>-0.017956</td>\n",
       "      <td>-0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.676018</td>\n",
       "      <td>0.670048</td>\n",
       "      <td>0.664147</td>\n",
       "      <td>0.658268</td>\n",
       "      <td>0.644803</td>\n",
       "      <td>0.648694</td>\n",
       "      <td>0.642243</td>\n",
       "      <td>0.659583</td>\n",
       "      <td>0.635297</td>\n",
       "      <td>0.630015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>-0.014856</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.028510</td>\n",
       "      <td>0.038169</td>\n",
       "      <td>0.038119</td>\n",
       "      <td>-0.015560</td>\n",
       "      <td>0.037337</td>\n",
       "      <td>-0.017355</td>\n",
       "      <td>-0.014628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.683680</td>\n",
       "      <td>0.677715</td>\n",
       "      <td>0.671817</td>\n",
       "      <td>0.665941</td>\n",
       "      <td>0.653085</td>\n",
       "      <td>0.656230</td>\n",
       "      <td>0.649813</td>\n",
       "      <td>0.665784</td>\n",
       "      <td>0.642608</td>\n",
       "      <td>0.637208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.027694</td>\n",
       "      <td>0.030925</td>\n",
       "      <td>0.030931</td>\n",
       "      <td>-0.016521</td>\n",
       "      <td>0.029848</td>\n",
       "      <td>-0.016754</td>\n",
       "      <td>-0.014057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.691341</td>\n",
       "      <td>0.685382</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.673614</td>\n",
       "      <td>0.661367</td>\n",
       "      <td>0.663767</td>\n",
       "      <td>0.657382</td>\n",
       "      <td>0.671985</td>\n",
       "      <td>0.649919</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003992</td>\n",
       "      <td>-0.016956</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.026878</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.023744</td>\n",
       "      <td>-0.017483</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>-0.016154</td>\n",
       "      <td>-0.013486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.699003</td>\n",
       "      <td>0.693049</td>\n",
       "      <td>0.687157</td>\n",
       "      <td>0.681287</td>\n",
       "      <td>0.669649</td>\n",
       "      <td>0.671303</td>\n",
       "      <td>0.664952</td>\n",
       "      <td>0.678186</td>\n",
       "      <td>0.657230</td>\n",
       "      <td>0.651593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007375</td>\n",
       "      <td>-0.018006</td>\n",
       "      <td>-0.023073</td>\n",
       "      <td>-0.026062</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>-0.018444</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.015553</td>\n",
       "      <td>-0.012915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.706664</td>\n",
       "      <td>0.700716</td>\n",
       "      <td>0.694827</td>\n",
       "      <td>0.688960</td>\n",
       "      <td>0.677931</td>\n",
       "      <td>0.678840</td>\n",
       "      <td>0.672521</td>\n",
       "      <td>0.684387</td>\n",
       "      <td>0.664541</td>\n",
       "      <td>0.658786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.019068</td>\n",
       "      <td>-0.023078</td>\n",
       "      <td>-0.025246</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>0.009321</td>\n",
       "      <td>-0.019417</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>-0.014953</td>\n",
       "      <td>-0.012343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.714326</td>\n",
       "      <td>0.708382</td>\n",
       "      <td>0.702496</td>\n",
       "      <td>0.696633</td>\n",
       "      <td>0.686213</td>\n",
       "      <td>0.686377</td>\n",
       "      <td>0.680090</td>\n",
       "      <td>0.690588</td>\n",
       "      <td>0.671852</td>\n",
       "      <td>0.665978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014175</td>\n",
       "      <td>-0.020122</td>\n",
       "      <td>-0.023080</td>\n",
       "      <td>-0.024430</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>-0.020382</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.014352</td>\n",
       "      <td>-0.011772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.721987</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.710166</td>\n",
       "      <td>0.704306</td>\n",
       "      <td>0.694495</td>\n",
       "      <td>0.693913</td>\n",
       "      <td>0.687660</td>\n",
       "      <td>0.696788</td>\n",
       "      <td>0.679163</td>\n",
       "      <td>0.673171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017445</td>\n",
       "      <td>-0.021122</td>\n",
       "      <td>-0.023057</td>\n",
       "      <td>-0.023614</td>\n",
       "      <td>-0.005147</td>\n",
       "      <td>-0.004860</td>\n",
       "      <td>-0.021296</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>-0.013751</td>\n",
       "      <td>-0.011201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.729649</td>\n",
       "      <td>0.723716</td>\n",
       "      <td>0.717836</td>\n",
       "      <td>0.711979</td>\n",
       "      <td>0.702777</td>\n",
       "      <td>0.701450</td>\n",
       "      <td>0.695229</td>\n",
       "      <td>0.702989</td>\n",
       "      <td>0.686474</td>\n",
       "      <td>0.680364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020489</td>\n",
       "      <td>-0.022022</td>\n",
       "      <td>-0.022991</td>\n",
       "      <td>-0.022798</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>-0.011416</td>\n",
       "      <td>-0.022116</td>\n",
       "      <td>-0.015086</td>\n",
       "      <td>-0.013151</td>\n",
       "      <td>-0.010629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.737310</td>\n",
       "      <td>0.731383</td>\n",
       "      <td>0.725506</td>\n",
       "      <td>0.719652</td>\n",
       "      <td>0.711059</td>\n",
       "      <td>0.708986</td>\n",
       "      <td>0.702799</td>\n",
       "      <td>0.709190</td>\n",
       "      <td>0.693785</td>\n",
       "      <td>0.687557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023204</td>\n",
       "      <td>-0.022775</td>\n",
       "      <td>-0.022860</td>\n",
       "      <td>-0.021982</td>\n",
       "      <td>-0.017745</td>\n",
       "      <td>-0.017357</td>\n",
       "      <td>-0.022798</td>\n",
       "      <td>-0.021871</td>\n",
       "      <td>-0.012550</td>\n",
       "      <td>-0.010058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.744972</td>\n",
       "      <td>0.739050</td>\n",
       "      <td>0.733176</td>\n",
       "      <td>0.727325</td>\n",
       "      <td>0.719342</td>\n",
       "      <td>0.716523</td>\n",
       "      <td>0.710368</td>\n",
       "      <td>0.715391</td>\n",
       "      <td>0.701096</td>\n",
       "      <td>0.694749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025486</td>\n",
       "      <td>-0.023335</td>\n",
       "      <td>-0.022644</td>\n",
       "      <td>-0.021165</td>\n",
       "      <td>-0.022920</td>\n",
       "      <td>-0.022488</td>\n",
       "      <td>-0.023299</td>\n",
       "      <td>-0.027249</td>\n",
       "      <td>-0.011950</td>\n",
       "      <td>-0.009487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.752633</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.740845</td>\n",
       "      <td>0.734998</td>\n",
       "      <td>0.727624</td>\n",
       "      <td>0.724060</td>\n",
       "      <td>0.717937</td>\n",
       "      <td>0.721592</td>\n",
       "      <td>0.708407</td>\n",
       "      <td>0.701942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027229</td>\n",
       "      <td>-0.023655</td>\n",
       "      <td>-0.022323</td>\n",
       "      <td>-0.020349</td>\n",
       "      <td>-0.027085</td>\n",
       "      <td>-0.026615</td>\n",
       "      <td>-0.023574</td>\n",
       "      <td>-0.031220</td>\n",
       "      <td>-0.011349</td>\n",
       "      <td>-0.008916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.760295</td>\n",
       "      <td>0.754383</td>\n",
       "      <td>0.748515</td>\n",
       "      <td>0.742671</td>\n",
       "      <td>0.735906</td>\n",
       "      <td>0.731596</td>\n",
       "      <td>0.725507</td>\n",
       "      <td>0.727793</td>\n",
       "      <td>0.715718</td>\n",
       "      <td>0.709135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028331</td>\n",
       "      <td>-0.023690</td>\n",
       "      <td>-0.021875</td>\n",
       "      <td>-0.019533</td>\n",
       "      <td>-0.030044</td>\n",
       "      <td>-0.029543</td>\n",
       "      <td>-0.023581</td>\n",
       "      <td>-0.033783</td>\n",
       "      <td>-0.010748</td>\n",
       "      <td>-0.008344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.767956</td>\n",
       "      <td>0.762050</td>\n",
       "      <td>0.756185</td>\n",
       "      <td>0.750344</td>\n",
       "      <td>0.744188</td>\n",
       "      <td>0.739133</td>\n",
       "      <td>0.733076</td>\n",
       "      <td>0.733994</td>\n",
       "      <td>0.723029</td>\n",
       "      <td>0.716327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028687</td>\n",
       "      <td>-0.023394</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.018717</td>\n",
       "      <td>-0.031603</td>\n",
       "      <td>-0.031080</td>\n",
       "      <td>-0.023276</td>\n",
       "      <td>-0.034939</td>\n",
       "      <td>-0.010148</td>\n",
       "      <td>-0.007773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.775618</td>\n",
       "      <td>0.769717</td>\n",
       "      <td>0.763855</td>\n",
       "      <td>0.758017</td>\n",
       "      <td>0.752470</td>\n",
       "      <td>0.746670</td>\n",
       "      <td>0.740646</td>\n",
       "      <td>0.740194</td>\n",
       "      <td>0.730340</td>\n",
       "      <td>0.723520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028194</td>\n",
       "      <td>-0.022718</td>\n",
       "      <td>-0.020521</td>\n",
       "      <td>-0.017901</td>\n",
       "      <td>-0.031565</td>\n",
       "      <td>-0.031029</td>\n",
       "      <td>-0.022615</td>\n",
       "      <td>-0.034688</td>\n",
       "      <td>-0.009547</td>\n",
       "      <td>-0.007202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.783279</td>\n",
       "      <td>0.777384</td>\n",
       "      <td>0.771525</td>\n",
       "      <td>0.765690</td>\n",
       "      <td>0.760752</td>\n",
       "      <td>0.754206</td>\n",
       "      <td>0.748215</td>\n",
       "      <td>0.746395</td>\n",
       "      <td>0.737651</td>\n",
       "      <td>0.730713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027240</td>\n",
       "      <td>-0.021839</td>\n",
       "      <td>-0.019671</td>\n",
       "      <td>-0.017085</td>\n",
       "      <td>-0.030665</td>\n",
       "      <td>-0.030120</td>\n",
       "      <td>-0.021762</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>-0.008947</td>\n",
       "      <td>-0.006631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.790941</td>\n",
       "      <td>0.785051</td>\n",
       "      <td>0.779194</td>\n",
       "      <td>0.773363</td>\n",
       "      <td>0.769034</td>\n",
       "      <td>0.761743</td>\n",
       "      <td>0.755784</td>\n",
       "      <td>0.752596</td>\n",
       "      <td>0.744962</td>\n",
       "      <td>0.737906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026287</td>\n",
       "      <td>-0.020959</td>\n",
       "      <td>-0.018821</td>\n",
       "      <td>-0.016269</td>\n",
       "      <td>-0.029764</td>\n",
       "      <td>-0.029212</td>\n",
       "      <td>-0.020908</td>\n",
       "      <td>-0.032777</td>\n",
       "      <td>-0.008346</td>\n",
       "      <td>-0.006059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.798602</td>\n",
       "      <td>0.792718</td>\n",
       "      <td>0.786864</td>\n",
       "      <td>0.781036</td>\n",
       "      <td>0.777316</td>\n",
       "      <td>0.769279</td>\n",
       "      <td>0.763354</td>\n",
       "      <td>0.758797</td>\n",
       "      <td>0.752273</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025333</td>\n",
       "      <td>-0.020080</td>\n",
       "      <td>-0.017970</td>\n",
       "      <td>-0.015453</td>\n",
       "      <td>-0.028863</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>-0.020055</td>\n",
       "      <td>-0.031822</td>\n",
       "      <td>-0.007745</td>\n",
       "      <td>-0.005488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.806264</td>\n",
       "      <td>0.800384</td>\n",
       "      <td>0.794534</td>\n",
       "      <td>0.788709</td>\n",
       "      <td>0.785598</td>\n",
       "      <td>0.776816</td>\n",
       "      <td>0.770923</td>\n",
       "      <td>0.764998</td>\n",
       "      <td>0.759584</td>\n",
       "      <td>0.752291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024380</td>\n",
       "      <td>-0.019200</td>\n",
       "      <td>-0.017120</td>\n",
       "      <td>-0.014637</td>\n",
       "      <td>-0.027962</td>\n",
       "      <td>-0.027394</td>\n",
       "      <td>-0.019202</td>\n",
       "      <td>-0.030867</td>\n",
       "      <td>-0.007145</td>\n",
       "      <td>-0.004917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.813925</td>\n",
       "      <td>0.808051</td>\n",
       "      <td>0.802204</td>\n",
       "      <td>0.796382</td>\n",
       "      <td>0.793880</td>\n",
       "      <td>0.784353</td>\n",
       "      <td>0.778493</td>\n",
       "      <td>0.771199</td>\n",
       "      <td>0.766895</td>\n",
       "      <td>0.759484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023426</td>\n",
       "      <td>-0.018320</td>\n",
       "      <td>-0.016269</td>\n",
       "      <td>-0.013821</td>\n",
       "      <td>-0.027061</td>\n",
       "      <td>-0.026485</td>\n",
       "      <td>-0.018348</td>\n",
       "      <td>-0.029911</td>\n",
       "      <td>-0.006544</td>\n",
       "      <td>-0.004345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.821587</td>\n",
       "      <td>0.815718</td>\n",
       "      <td>0.809874</td>\n",
       "      <td>0.804055</td>\n",
       "      <td>0.802163</td>\n",
       "      <td>0.791889</td>\n",
       "      <td>0.786062</td>\n",
       "      <td>0.777400</td>\n",
       "      <td>0.774206</td>\n",
       "      <td>0.766676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022473</td>\n",
       "      <td>-0.017441</td>\n",
       "      <td>-0.015419</td>\n",
       "      <td>-0.013004</td>\n",
       "      <td>-0.026160</td>\n",
       "      <td>-0.025576</td>\n",
       "      <td>-0.017495</td>\n",
       "      <td>-0.028956</td>\n",
       "      <td>-0.005944</td>\n",
       "      <td>-0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.829248</td>\n",
       "      <td>0.823385</td>\n",
       "      <td>0.817543</td>\n",
       "      <td>0.811729</td>\n",
       "      <td>0.810445</td>\n",
       "      <td>0.799426</td>\n",
       "      <td>0.793631</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>0.781517</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021520</td>\n",
       "      <td>-0.016561</td>\n",
       "      <td>-0.014568</td>\n",
       "      <td>-0.012188</td>\n",
       "      <td>-0.025259</td>\n",
       "      <td>-0.024668</td>\n",
       "      <td>-0.016642</td>\n",
       "      <td>-0.028001</td>\n",
       "      <td>-0.005343</td>\n",
       "      <td>-0.003203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.836910</td>\n",
       "      <td>0.831052</td>\n",
       "      <td>0.825213</td>\n",
       "      <td>0.819402</td>\n",
       "      <td>0.818727</td>\n",
       "      <td>0.806963</td>\n",
       "      <td>0.801201</td>\n",
       "      <td>0.789801</td>\n",
       "      <td>0.788828</td>\n",
       "      <td>0.781062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020566</td>\n",
       "      <td>-0.015681</td>\n",
       "      <td>-0.013718</td>\n",
       "      <td>-0.011372</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>-0.023759</td>\n",
       "      <td>-0.015788</td>\n",
       "      <td>-0.027046</td>\n",
       "      <td>-0.004743</td>\n",
       "      <td>-0.002632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.844639</td>\n",
       "      <td>0.838785</td>\n",
       "      <td>0.832948</td>\n",
       "      <td>0.827139</td>\n",
       "      <td>0.827089</td>\n",
       "      <td>0.814557</td>\n",
       "      <td>0.808826</td>\n",
       "      <td>0.796033</td>\n",
       "      <td>0.796188</td>\n",
       "      <td>0.788304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019613</td>\n",
       "      <td>-0.014802</td>\n",
       "      <td>-0.012868</td>\n",
       "      <td>-0.010556</td>\n",
       "      <td>-0.023457</td>\n",
       "      <td>-0.022850</td>\n",
       "      <td>-0.014935</td>\n",
       "      <td>-0.026091</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>-0.002060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.852483</td>\n",
       "      <td>0.846632</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.834987</td>\n",
       "      <td>0.835590</td>\n",
       "      <td>0.822249</td>\n",
       "      <td>0.816545</td>\n",
       "      <td>0.802319</td>\n",
       "      <td>0.803634</td>\n",
       "      <td>0.795629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018659</td>\n",
       "      <td>-0.013922</td>\n",
       "      <td>-0.012017</td>\n",
       "      <td>-0.009740</td>\n",
       "      <td>-0.022556</td>\n",
       "      <td>-0.021941</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>-0.025135</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>-0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.860414</td>\n",
       "      <td>0.854564</td>\n",
       "      <td>0.848727</td>\n",
       "      <td>0.842918</td>\n",
       "      <td>0.844194</td>\n",
       "      <td>0.830015</td>\n",
       "      <td>0.824336</td>\n",
       "      <td>0.808645</td>\n",
       "      <td>0.811144</td>\n",
       "      <td>0.803017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017706</td>\n",
       "      <td>-0.013043</td>\n",
       "      <td>-0.011167</td>\n",
       "      <td>-0.008924</td>\n",
       "      <td>-0.021655</td>\n",
       "      <td>-0.021032</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>-0.024180</td>\n",
       "      <td>-0.002941</td>\n",
       "      <td>-0.000918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.868402</td>\n",
       "      <td>0.862553</td>\n",
       "      <td>0.856714</td>\n",
       "      <td>0.850903</td>\n",
       "      <td>0.852865</td>\n",
       "      <td>0.837830</td>\n",
       "      <td>0.832173</td>\n",
       "      <td>0.814997</td>\n",
       "      <td>0.818695</td>\n",
       "      <td>0.810447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016752</td>\n",
       "      <td>-0.012163</td>\n",
       "      <td>-0.010316</td>\n",
       "      <td>-0.008108</td>\n",
       "      <td>-0.020754</td>\n",
       "      <td>-0.020123</td>\n",
       "      <td>-0.012375</td>\n",
       "      <td>-0.023225</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>-0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0.009055</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.007581</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001677</td>\n",
       "      <td>-0.003978</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>-0.003758</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.012537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.008721</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>-0.003997</td>\n",
       "      <td>-0.004391</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>-0.007974</td>\n",
       "      <td>-0.003775</td>\n",
       "      <td>-0.005970</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.013128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0.008509</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001657</td>\n",
       "      <td>-0.004016</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>-0.008480</td>\n",
       "      <td>-0.003792</td>\n",
       "      <td>-0.006005</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.013718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>0.006791</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001647</td>\n",
       "      <td>-0.004036</td>\n",
       "      <td>-0.004436</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>-0.003809</td>\n",
       "      <td>-0.006039</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.014309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0.007962</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>-0.004055</td>\n",
       "      <td>-0.004458</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>-0.009490</td>\n",
       "      <td>-0.003826</td>\n",
       "      <td>-0.006074</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.014899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0.007689</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001627</td>\n",
       "      <td>-0.004074</td>\n",
       "      <td>-0.004480</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>-0.009996</td>\n",
       "      <td>-0.003843</td>\n",
       "      <td>-0.006109</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.015490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>-0.004094</td>\n",
       "      <td>-0.004502</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>-0.010501</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>-0.006144</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.016080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001606</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>-0.004524</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>-0.011007</td>\n",
       "      <td>-0.003876</td>\n",
       "      <td>-0.006178</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.016671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.006785</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004132</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.014730</td>\n",
       "      <td>-0.011512</td>\n",
       "      <td>-0.003893</td>\n",
       "      <td>-0.006213</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.017262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.006526</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>-0.004151</td>\n",
       "      <td>-0.004568</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>-0.012017</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.017852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>0.006323</td>\n",
       "      <td>0.006305</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.004668</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001576</td>\n",
       "      <td>-0.004171</td>\n",
       "      <td>-0.004590</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>-0.012523</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>-0.006283</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.018443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001566</td>\n",
       "      <td>-0.004190</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.017217</td>\n",
       "      <td>-0.013028</td>\n",
       "      <td>-0.003943</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.019033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001556</td>\n",
       "      <td>-0.004209</td>\n",
       "      <td>-0.004634</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>-0.013533</td>\n",
       "      <td>-0.003960</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.019624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>-0.004228</td>\n",
       "      <td>-0.004656</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.018875</td>\n",
       "      <td>-0.014039</td>\n",
       "      <td>-0.003977</td>\n",
       "      <td>-0.006387</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.020214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>-0.004248</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>-0.014544</td>\n",
       "      <td>-0.003994</td>\n",
       "      <td>-0.006422</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.020805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001526</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>-0.015049</td>\n",
       "      <td>-0.004011</td>\n",
       "      <td>-0.006456</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.021395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001516</td>\n",
       "      <td>-0.004286</td>\n",
       "      <td>-0.004723</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.021361</td>\n",
       "      <td>-0.015555</td>\n",
       "      <td>-0.004027</td>\n",
       "      <td>-0.006491</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.021986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001506</td>\n",
       "      <td>-0.004305</td>\n",
       "      <td>-0.004745</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.022190</td>\n",
       "      <td>-0.016060</td>\n",
       "      <td>-0.004044</td>\n",
       "      <td>-0.006526</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.022577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.004138</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001496</td>\n",
       "      <td>-0.004325</td>\n",
       "      <td>-0.004767</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.023019</td>\n",
       "      <td>-0.016565</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>-0.006561</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.023167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>-0.000438</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.004344</td>\n",
       "      <td>-0.004789</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.023848</td>\n",
       "      <td>-0.017071</td>\n",
       "      <td>-0.004078</td>\n",
       "      <td>-0.006595</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.023758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001476</td>\n",
       "      <td>-0.004363</td>\n",
       "      <td>-0.004811</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.024677</td>\n",
       "      <td>-0.017576</td>\n",
       "      <td>-0.004095</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.024348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001466</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>-0.004833</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.025506</td>\n",
       "      <td>-0.018081</td>\n",
       "      <td>-0.004112</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.024939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>-0.001466</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>-0.004855</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>-0.018587</td>\n",
       "      <td>-0.004128</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.025529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-0.001809</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.004421</td>\n",
       "      <td>-0.004877</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>-0.019092</td>\n",
       "      <td>-0.004145</td>\n",
       "      <td>-0.006734</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.026120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>-0.004899</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.027993</td>\n",
       "      <td>-0.019597</td>\n",
       "      <td>-0.004162</td>\n",
       "      <td>-0.006769</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.026711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.002494</td>\n",
       "      <td>-0.000783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.028822</td>\n",
       "      <td>-0.020103</td>\n",
       "      <td>-0.004179</td>\n",
       "      <td>-0.006804</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.027301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000899</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.002837</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.004479</td>\n",
       "      <td>-0.004943</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.029651</td>\n",
       "      <td>-0.020608</td>\n",
       "      <td>-0.004196</td>\n",
       "      <td>-0.006839</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.027892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.001222</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>-0.003180</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>-0.004965</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.030480</td>\n",
       "      <td>-0.021113</td>\n",
       "      <td>-0.004212</td>\n",
       "      <td>-0.006873</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.028482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>-0.000711</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>-0.003523</td>\n",
       "      <td>-0.001726</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>-0.004517</td>\n",
       "      <td>-0.004987</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.031309</td>\n",
       "      <td>-0.021619</td>\n",
       "      <td>-0.004229</td>\n",
       "      <td>-0.006908</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.029073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.003865</td>\n",
       "      <td>-0.002041</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>-0.022124</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>-0.006943</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.029663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows  500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.645373  0.639381  0.633468  0.627576  0.611674  0.618547  0.611966   \n",
       "1    0.653034  0.647048  0.641138  0.635249  0.619956  0.626084  0.619535   \n",
       "2    0.660695  0.654715  0.648808  0.642922  0.628238  0.633620  0.627105   \n",
       "3    0.668357  0.662381  0.656478  0.650595  0.636520  0.641157  0.634674   \n",
       "4    0.676018  0.670048  0.664147  0.658268  0.644803  0.648694  0.642243   \n",
       "5    0.683680  0.677715  0.671817  0.665941  0.653085  0.656230  0.649813   \n",
       "6    0.691341  0.685382  0.679487  0.673614  0.661367  0.663767  0.657382   \n",
       "7    0.699003  0.693049  0.687157  0.681287  0.669649  0.671303  0.664952   \n",
       "8    0.706664  0.700716  0.694827  0.688960  0.677931  0.678840  0.672521   \n",
       "9    0.714326  0.708382  0.702496  0.696633  0.686213  0.686377  0.680090   \n",
       "10   0.721987  0.716049  0.710166  0.704306  0.694495  0.693913  0.687660   \n",
       "11   0.729649  0.723716  0.717836  0.711979  0.702777  0.701450  0.695229   \n",
       "12   0.737310  0.731383  0.725506  0.719652  0.711059  0.708986  0.702799   \n",
       "13   0.744972  0.739050  0.733176  0.727325  0.719342  0.716523  0.710368   \n",
       "14   0.752633  0.746717  0.740845  0.734998  0.727624  0.724060  0.717937   \n",
       "15   0.760295  0.754383  0.748515  0.742671  0.735906  0.731596  0.725507   \n",
       "16   0.767956  0.762050  0.756185  0.750344  0.744188  0.739133  0.733076   \n",
       "17   0.775618  0.769717  0.763855  0.758017  0.752470  0.746670  0.740646   \n",
       "18   0.783279  0.777384  0.771525  0.765690  0.760752  0.754206  0.748215   \n",
       "19   0.790941  0.785051  0.779194  0.773363  0.769034  0.761743  0.755784   \n",
       "20   0.798602  0.792718  0.786864  0.781036  0.777316  0.769279  0.763354   \n",
       "21   0.806264  0.800384  0.794534  0.788709  0.785598  0.776816  0.770923   \n",
       "22   0.813925  0.808051  0.802204  0.796382  0.793880  0.784353  0.778493   \n",
       "23   0.821587  0.815718  0.809874  0.804055  0.802163  0.791889  0.786062   \n",
       "24   0.829248  0.823385  0.817543  0.811729  0.810445  0.799426  0.793631   \n",
       "25   0.836910  0.831052  0.825213  0.819402  0.818727  0.806963  0.801201   \n",
       "26   0.844639  0.838785  0.832948  0.827139  0.827089  0.814557  0.808826   \n",
       "27   0.852483  0.846632  0.840796  0.834987  0.835590  0.822249  0.816545   \n",
       "28   0.860414  0.854564  0.848727  0.842918  0.844194  0.830015  0.824336   \n",
       "29   0.868402  0.862553  0.856714  0.850903  0.852865  0.837830  0.832173   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "670  0.009055  0.008989  0.008923  0.008859  0.007581  0.007507  0.007700   \n",
       "671  0.008782  0.008721  0.008660  0.008600  0.007285  0.007183  0.007397   \n",
       "672  0.008509  0.008452  0.008396  0.008341  0.006989  0.006860  0.007094   \n",
       "673  0.008236  0.008184  0.008132  0.008082  0.006693  0.006537  0.006791   \n",
       "674  0.007962  0.007916  0.007868  0.007822  0.006397  0.006214  0.006487   \n",
       "675  0.007689  0.007647  0.007604  0.007563  0.006101  0.005890  0.006184   \n",
       "676  0.007416  0.007379  0.007340  0.007304  0.005804  0.005567  0.005881   \n",
       "677  0.007143  0.007110  0.007076  0.007045  0.005508  0.005244  0.005577   \n",
       "678  0.006870  0.006842  0.006812  0.006785  0.005212  0.004920  0.005274   \n",
       "679  0.006596  0.006573  0.006548  0.006526  0.004916  0.004597  0.004971   \n",
       "680  0.006323  0.006305  0.006285  0.006267  0.004620  0.004274  0.004668   \n",
       "681  0.006050  0.006036  0.006021  0.006008  0.004324  0.003950  0.004364   \n",
       "682  0.005777  0.005768  0.005757  0.005749  0.004028  0.003627  0.004061   \n",
       "683  0.005504  0.005499  0.005493  0.005489  0.003731  0.003304  0.003758   \n",
       "684  0.005231  0.005231  0.005229  0.005230  0.003435  0.002981  0.003455   \n",
       "685  0.004957  0.004962  0.004965  0.004971  0.003139  0.002657  0.003151   \n",
       "686  0.004684  0.004694  0.004701  0.004712  0.002843  0.002334  0.002848   \n",
       "687  0.004411  0.004425  0.004437  0.004452  0.002547  0.002011  0.002545   \n",
       "688  0.004138  0.004157  0.004173  0.004193  0.002251  0.001687  0.002241   \n",
       "689  0.003865  0.003888  0.003910  0.003934  0.001954  0.001364  0.001938   \n",
       "690  0.003591  0.003620  0.003646  0.003675  0.001658  0.001041  0.001635   \n",
       "691  0.003318  0.003351  0.003382  0.003416  0.001362  0.000717  0.001332   \n",
       "692  0.003045  0.003083  0.003118  0.003156  0.001066  0.000394  0.001028   \n",
       "693  0.002772  0.002814  0.002854  0.002897  0.000770  0.000071  0.000725   \n",
       "694  0.002499  0.002546  0.002590  0.002638  0.000474 -0.000252  0.000422   \n",
       "695  0.002225  0.002277  0.002326  0.002379  0.000177 -0.000576  0.000118   \n",
       "696  0.001952  0.002009  0.002062  0.002119 -0.000119 -0.000899 -0.000185   \n",
       "697  0.001679  0.001740  0.001798  0.001860 -0.000415 -0.001222 -0.000488   \n",
       "698  0.001406  0.001472  0.001535  0.001601 -0.000711 -0.001546 -0.000791   \n",
       "699  0.001133  0.001203  0.001271  0.001342 -0.001007 -0.001869 -0.001095   \n",
       "\n",
       "          7         8         9      ...          490       491       492  \\\n",
       "0    0.634780  0.606053  0.601244    ...     0.016305 -0.010655 -0.023073   \n",
       "1    0.640981  0.613364  0.608437    ...     0.012922 -0.011706 -0.023073   \n",
       "2    0.647182  0.620675  0.615629    ...     0.009539 -0.012756 -0.023073   \n",
       "3    0.653383  0.627986  0.622822    ...     0.006156 -0.013806 -0.023073   \n",
       "4    0.659583  0.635297  0.630015    ...     0.002774 -0.014856 -0.023073   \n",
       "5    0.665784  0.642608  0.637208    ...    -0.000609 -0.015906 -0.023073   \n",
       "6    0.671985  0.649919  0.644400    ...    -0.003992 -0.016956 -0.023073   \n",
       "7    0.678186  0.657230  0.651593    ...    -0.007375 -0.018006 -0.023073   \n",
       "8    0.684387  0.664541  0.658786    ...    -0.010783 -0.019068 -0.023078   \n",
       "9    0.690588  0.671852  0.665978    ...    -0.014175 -0.020122 -0.023080   \n",
       "10   0.696788  0.679163  0.673171    ...    -0.017445 -0.021122 -0.023057   \n",
       "11   0.702989  0.686474  0.680364    ...    -0.020489 -0.022022 -0.022991   \n",
       "12   0.709190  0.693785  0.687557    ...    -0.023204 -0.022775 -0.022860   \n",
       "13   0.715391  0.701096  0.694749    ...    -0.025486 -0.023335 -0.022644   \n",
       "14   0.721592  0.708407  0.701942    ...    -0.027229 -0.023655 -0.022323   \n",
       "15   0.727793  0.715718  0.709135    ...    -0.028331 -0.023690 -0.021875   \n",
       "16   0.733994  0.723029  0.716327    ...    -0.028687 -0.023394 -0.021282   \n",
       "17   0.740194  0.730340  0.723520    ...    -0.028194 -0.022718 -0.020521   \n",
       "18   0.746395  0.737651  0.730713    ...    -0.027240 -0.021839 -0.019671   \n",
       "19   0.752596  0.744962  0.737906    ...    -0.026287 -0.020959 -0.018821   \n",
       "20   0.758797  0.752273  0.745098    ...    -0.025333 -0.020080 -0.017970   \n",
       "21   0.764998  0.759584  0.752291    ...    -0.024380 -0.019200 -0.017120   \n",
       "22   0.771199  0.766895  0.759484    ...    -0.023426 -0.018320 -0.016269   \n",
       "23   0.777400  0.774206  0.766676    ...    -0.022473 -0.017441 -0.015419   \n",
       "24   0.783600  0.781517  0.773869    ...    -0.021520 -0.016561 -0.014568   \n",
       "25   0.789801  0.788828  0.781062    ...    -0.020566 -0.015681 -0.013718   \n",
       "26   0.796033  0.796188  0.788304    ...    -0.019613 -0.014802 -0.012868   \n",
       "27   0.802319  0.803634  0.795629    ...    -0.018659 -0.013922 -0.012017   \n",
       "28   0.808645  0.811144  0.803017    ...    -0.017706 -0.013043 -0.011167   \n",
       "29   0.814997  0.818695  0.810447    ...    -0.016752 -0.012163 -0.010316   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "670  0.006075  0.007078  0.008397    ...    -0.001677 -0.003978 -0.004369   \n",
       "671  0.005732  0.006764  0.008126    ...    -0.001667 -0.003997 -0.004391   \n",
       "672  0.005389  0.006449  0.007854    ...    -0.001657 -0.004016 -0.004414   \n",
       "673  0.005047  0.006135  0.007583    ...    -0.001647 -0.004036 -0.004436   \n",
       "674  0.004704  0.005820  0.007312    ...    -0.001637 -0.004055 -0.004458   \n",
       "675  0.004361  0.005506  0.007041    ...    -0.001627 -0.004074 -0.004480   \n",
       "676  0.004018  0.005192  0.006769    ...    -0.001617 -0.004094 -0.004502   \n",
       "677  0.003676  0.004877  0.006498    ...    -0.001606 -0.004113 -0.004524   \n",
       "678  0.003333  0.004563  0.006227    ...    -0.001596 -0.004132 -0.004546   \n",
       "679  0.002990  0.004248  0.005956    ...    -0.001586 -0.004151 -0.004568   \n",
       "680  0.002647  0.003934  0.005684    ...    -0.001576 -0.004171 -0.004590   \n",
       "681  0.002304  0.003619  0.005413    ...    -0.001566 -0.004190 -0.004612   \n",
       "682  0.001962  0.003305  0.005142    ...    -0.001556 -0.004209 -0.004634   \n",
       "683  0.001619  0.002990  0.004870    ...    -0.001546 -0.004228 -0.004656   \n",
       "684  0.001276  0.002676  0.004599    ...    -0.001536 -0.004248 -0.004678   \n",
       "685  0.000933  0.002362  0.004328    ...    -0.001526 -0.004267 -0.004701   \n",
       "686  0.000591  0.002047  0.004057    ...    -0.001516 -0.004286 -0.004723   \n",
       "687  0.000248  0.001733  0.003785    ...    -0.001506 -0.004305 -0.004745   \n",
       "688 -0.000095  0.001418  0.003514    ...    -0.001496 -0.004325 -0.004767   \n",
       "689 -0.000438  0.001104  0.003243    ...    -0.001486 -0.004344 -0.004789   \n",
       "690 -0.000781  0.000789  0.002972    ...    -0.001476 -0.004363 -0.004811   \n",
       "691 -0.001123  0.000475  0.002700    ...    -0.001466 -0.004383 -0.004833   \n",
       "692 -0.001466  0.000160  0.002429    ...    -0.001456 -0.004402 -0.004855   \n",
       "693 -0.001809 -0.000154  0.002158    ...    -0.001446 -0.004421 -0.004877   \n",
       "694 -0.002152 -0.000468  0.001886    ...    -0.001436 -0.004440 -0.004899   \n",
       "695 -0.002494 -0.000783  0.001615    ...    -0.001426 -0.004460 -0.004921   \n",
       "696 -0.002837 -0.001097  0.001344    ...    -0.001416 -0.004479 -0.004943   \n",
       "697 -0.003180 -0.001412  0.001073    ...    -0.001406 -0.004498 -0.004965   \n",
       "698 -0.003523 -0.001726  0.000801    ...    -0.001396 -0.004517 -0.004987   \n",
       "699 -0.003865 -0.002041  0.000530    ...    -0.001386 -0.004537 -0.005010   \n",
       "\n",
       "          493       494       495       496       497       498       499  \n",
       "0   -0.031775  0.067144  0.066869 -0.011714  0.067293 -0.019757 -0.016914  \n",
       "1   -0.030959  0.059900  0.059681 -0.012675  0.059804 -0.019157 -0.016342  \n",
       "2   -0.030143  0.052657  0.052494 -0.013637  0.052315 -0.018556 -0.015771  \n",
       "3   -0.029326  0.045413  0.045306 -0.014598  0.044826 -0.017956 -0.015200  \n",
       "4   -0.028510  0.038169  0.038119 -0.015560  0.037337 -0.017355 -0.014628  \n",
       "5   -0.027694  0.030925  0.030931 -0.016521  0.029848 -0.016754 -0.014057  \n",
       "6   -0.026878  0.023682  0.023744 -0.017483  0.022359 -0.016154 -0.013486  \n",
       "7   -0.026062  0.016438  0.016557 -0.018444  0.014870 -0.015553 -0.012915  \n",
       "8   -0.025246  0.009145  0.009321 -0.019417  0.007381 -0.014953 -0.012343  \n",
       "9   -0.024430  0.001885  0.002117 -0.020382 -0.000108 -0.014352 -0.011772  \n",
       "10  -0.023614 -0.005147 -0.004860 -0.021296 -0.007597 -0.013751 -0.011201  \n",
       "11  -0.022798 -0.011755 -0.011416 -0.022116 -0.015086 -0.013151 -0.010629  \n",
       "12  -0.021982 -0.017745 -0.017357 -0.022798 -0.021871 -0.012550 -0.010058  \n",
       "13  -0.021165 -0.022920 -0.022488 -0.023299 -0.027249 -0.011950 -0.009487  \n",
       "14  -0.020349 -0.027085 -0.026615 -0.023574 -0.031220 -0.011349 -0.008916  \n",
       "15  -0.019533 -0.030044 -0.029543 -0.023581 -0.033783 -0.010748 -0.008344  \n",
       "16  -0.018717 -0.031603 -0.031080 -0.023276 -0.034939 -0.010148 -0.007773  \n",
       "17  -0.017901 -0.031565 -0.031029 -0.022615 -0.034688 -0.009547 -0.007202  \n",
       "18  -0.017085 -0.030665 -0.030120 -0.021762 -0.033732 -0.008947 -0.006631  \n",
       "19  -0.016269 -0.029764 -0.029212 -0.020908 -0.032777 -0.008346 -0.006059  \n",
       "20  -0.015453 -0.028863 -0.028303 -0.020055 -0.031822 -0.007745 -0.005488  \n",
       "21  -0.014637 -0.027962 -0.027394 -0.019202 -0.030867 -0.007145 -0.004917  \n",
       "22  -0.013821 -0.027061 -0.026485 -0.018348 -0.029911 -0.006544 -0.004345  \n",
       "23  -0.013004 -0.026160 -0.025576 -0.017495 -0.028956 -0.005944 -0.003774  \n",
       "24  -0.012188 -0.025259 -0.024668 -0.016642 -0.028001 -0.005343 -0.003203  \n",
       "25  -0.011372 -0.024358 -0.023759 -0.015788 -0.027046 -0.004743 -0.002632  \n",
       "26  -0.010556 -0.023457 -0.022850 -0.014935 -0.026091 -0.004142 -0.002060  \n",
       "27  -0.009740 -0.022556 -0.021941 -0.014082 -0.025135 -0.003541 -0.001489  \n",
       "28  -0.008924 -0.021655 -0.021032 -0.013228 -0.024180 -0.002941 -0.000918  \n",
       "29  -0.008108 -0.020754 -0.020123 -0.012375 -0.023225 -0.002340 -0.000346  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "670 -0.000172  0.008099 -0.007469 -0.003758 -0.005935  0.001534 -0.012537  \n",
       "671 -0.000172  0.008928 -0.007974 -0.003775 -0.005970  0.001534 -0.013128  \n",
       "672 -0.000172  0.009756 -0.008480 -0.003792 -0.006005  0.001534 -0.013718  \n",
       "673 -0.000172  0.010585 -0.008985 -0.003809 -0.006039  0.001534 -0.014309  \n",
       "674 -0.000172  0.011414 -0.009490 -0.003826 -0.006074  0.001534 -0.014899  \n",
       "675 -0.000172  0.012243 -0.009996 -0.003843 -0.006109  0.001534 -0.015490  \n",
       "676 -0.000172  0.013072 -0.010501 -0.003859 -0.006144  0.001534 -0.016080  \n",
       "677 -0.000172  0.013901 -0.011007 -0.003876 -0.006178  0.001534 -0.016671  \n",
       "678 -0.000172  0.014730 -0.011512 -0.003893 -0.006213  0.001534 -0.017262  \n",
       "679 -0.000172  0.015559 -0.012017 -0.003910 -0.006248  0.001534 -0.017852  \n",
       "680 -0.000172  0.016388 -0.012523 -0.003927 -0.006283  0.001534 -0.018443  \n",
       "681 -0.000172  0.017217 -0.013028 -0.003943 -0.006317  0.001534 -0.019033  \n",
       "682 -0.000172  0.018046 -0.013533 -0.003960 -0.006352  0.001534 -0.019624  \n",
       "683 -0.000172  0.018875 -0.014039 -0.003977 -0.006387  0.001534 -0.020214  \n",
       "684 -0.000172  0.019704 -0.014544 -0.003994 -0.006422  0.001534 -0.020805  \n",
       "685 -0.000172  0.020532 -0.015049 -0.004011 -0.006456  0.001534 -0.021395  \n",
       "686 -0.000172  0.021361 -0.015555 -0.004027 -0.006491  0.001534 -0.021986  \n",
       "687 -0.000172  0.022190 -0.016060 -0.004044 -0.006526  0.001534 -0.022577  \n",
       "688 -0.000172  0.023019 -0.016565 -0.004061 -0.006561  0.001534 -0.023167  \n",
       "689 -0.000172  0.023848 -0.017071 -0.004078 -0.006595  0.001534 -0.023758  \n",
       "690 -0.000172  0.024677 -0.017576 -0.004095 -0.006630  0.001534 -0.024348  \n",
       "691 -0.000172  0.025506 -0.018081 -0.004112 -0.006665  0.001534 -0.024939  \n",
       "692 -0.000172  0.026335 -0.018587 -0.004128 -0.006700  0.001534 -0.025529  \n",
       "693 -0.000172  0.027164 -0.019092 -0.004145 -0.006734  0.001534 -0.026120  \n",
       "694 -0.000172  0.027993 -0.019597 -0.004162 -0.006769  0.001534 -0.026711  \n",
       "695 -0.000172  0.028822 -0.020103 -0.004179 -0.006804  0.001534 -0.027301  \n",
       "696 -0.000172  0.029651 -0.020608 -0.004196 -0.006839  0.001534 -0.027892  \n",
       "697 -0.000172  0.030480 -0.021113 -0.004212 -0.006873  0.001534 -0.028482  \n",
       "698 -0.000172  0.031309 -0.021619 -0.004229 -0.006908  0.001534 -0.029073  \n",
       "699 -0.000172  0.032137 -0.022124 -0.004246 -0.006943  0.001534 -0.029663  \n",
       "\n",
       "[700 rows x 500 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_matx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------\n",
    "# * Get peak dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "def loaddata(data_filename):\n",
    "    \"\"\"load matrix data\"\"\"\n",
    "    data = np.genfromtxt(data_filename, delimiter='\\t')\n",
    "    data_nm = data[1:,0]    #wavelength in nm\n",
    "    data_time = data[0,1:]\n",
    "    data_z = data[1:, 1:]\n",
    "\n",
    "    return data_nm, data_time, data_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "def add_noise(nm_array, y_array, noise_coefficient):\n",
    "    # Add noise\n",
    "    np.random.seed(1800)\n",
    "    y_noise = noise_coefficient * np.random.normal(size=nm_array.size)\n",
    "    y_proc = y_array + y_noise\n",
    "    \n",
    "    return y_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astropy_smoothing(nm_array, timedelay, noise_coefficient,gg_init):\n",
    "    # Generate fake data\n",
    "    np.random.seed(42)\n",
    "    ydata = timedelay + noise_coefficient*np.random.normal(size=nm_array.size)\n",
    "    # Now to fit the data create a new superposition with initial\n",
    "    # guesses for the parameters:\n",
    "    fitter = fitting.SLSQPLSQFitter()\n",
    "    gg_fit = fitter(gg_init, nm_array, ydata)\n",
    "    return gg_fit(nm_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## astropy and peakutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astropy_peak_matrix(nm_array,data_matrix,noise_coefficient,threshold, min_dist,gg_init):\n",
    "    num_array = np.shape(data_matrix)[1]\n",
    "    true_peak = []\n",
    "    smooth_peak = []\n",
    "    \n",
    "    for i in range(num_array):\n",
    "        data_array = data_matrix[:, i]\n",
    "        noise_array = add_noise(nm_array, data_array, noise_coefficient)\n",
    "        smooth_array = astropy_smoothing(nm_array, data_array,noise_coefficient,gg_init)\n",
    "        \n",
    "        # get true peak matrix\n",
    "        indexes=findpeak(data_array, threshold, min_dist).tolist()\n",
    "        true_peak.append(indexes)\n",
    "        #get smooth peak matrix\n",
    "        \n",
    "        indexes1=findpeak(smooth_array, threshold, min_dist).tolist()\n",
    "        smooth_peak.append(indexes1)\n",
    "        \n",
    "        # transfer to dataframe\n",
    "        true_df=pd.DataFrame(true_peak)\n",
    "        smooth_df=pd.DataFrame(smooth_peak)\n",
    "    \n",
    "    return true_df, smooth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findpeak(data_z_array, threshold, min_dist):\n",
    "    \"\"\"find peaks and return indices of the peaks\"\"\"    \n",
    "    peak_indices = peakutils.indexes(data_z_array, thres=threshold, min_dist=min_dist)\n",
    "    \n",
    "    return peak_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "matx_filename = '20180418_twogaussian_spectralshfit.txt'\n",
    "datanm, datatime, dataz_matx = loaddata(matx_filename)\n",
    "g1 = models.Gaussian1D(1, 950, 30)\n",
    "g2 = models.Gaussian1D(0.3, 1300, 100)\n",
    "gg_init = g1+g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7495624261835365\n",
      "            Iterations: 35\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749416520979112\n",
      "            Iterations: 26\n",
      "            Function evaluations: 213\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494332869998\n",
      "            Iterations: 35\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749375198644923\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749431472355961\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749409344211661\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749200312059827\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749305522959776\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749341610032365\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749320229479094\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74940495470656\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749556724187272\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749288978788597\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749487159993337\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749579139915033\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749348830978332\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749364793534793\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749548188186964\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494724609215515\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749476131372704\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749514754553356\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749526311860512\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74936134379268\n",
      "            Iterations: 27\n",
      "            Function evaluations: 222\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749290476983642\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74925353050398\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749440478034478\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749443685229259\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749262798133592\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749341492081382\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749546103169484\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749321122140511\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749392845514771\n",
      "            Iterations: 35\n",
      "            Function evaluations: 284\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749352302250043\n",
      "            Iterations: 35\n",
      "            Function evaluations: 284\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749345670041293\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749337053264757\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749386666903677\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749403253119601\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749439371319275\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749231583366804\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749372558471001\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749372368287485\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494020076627805\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749120629652938\n",
      "            Iterations: 37\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749316680378654\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749372267100967\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749412094476595\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749271343513335\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749136488777219\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749301469166571\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749428430760332\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7491513620241\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749236462303185\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749212430677059\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749338069790612\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749245082708288\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749256913800864\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749183444840559\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749462115113701\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749416384528893\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749270546505583\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7492522447814345\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749408976089211\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749192207775534\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749135940666902\n",
      "            Iterations: 36\n",
      "            Function evaluations: 293\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749260313412371\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749185573649983\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749408035574321\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749106048569001\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748951735478756\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749184349268065\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749094885001178\n",
      "            Iterations: 39\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74920560103935\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749111825641649\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749382666153899\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7494059033261555\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749165153153649\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749190553616535\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749211004356601\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749181534395437\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74909389975479\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748982964647777\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749130093999115\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749289068725178\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749215960027268\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7491290293996045\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749187656014968\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7491637043155635\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749159484840783\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7492333082193685\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749272296502928\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749027049491023\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749070979223582\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749260426702647\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749079381412177\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749131220692076\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748944538691722\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748953338807989\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7490043112325715\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74901032280612\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7490293789098335\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74906888696148\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749091380440834\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74899359612705\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749064972231479\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748976949956735\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749055580667931\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748995725763191\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748982712686965\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748896961305074\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748946670206924\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748905525260417\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748865030148847\n",
      "            Iterations: 44\n",
      "            Function evaluations: 358\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748945989357448\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748907217415153\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7489006894493055\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7487818630324705\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748996623462666\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74888844192722\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74879976179185\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748782798182194\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748754145004412\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748900892926467\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748801029238395\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748746895082606\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748711363398641\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748880853869604\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748752416764757\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748661141246047\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748842284263059\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748800957799142\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74874219556267\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74879151595751\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748605768845157\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74865350621814\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748480403121148\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748641106192051\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748586215762827\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748612925128557\n",
      "            Iterations: 38\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7486866060964035\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748515350125947\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748582160291011\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748604476768035\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748535036931351\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748493557242421\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748494553435412\n",
      "            Iterations: 45\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74866793761032\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748521223952126\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748383480228792\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748426698735318\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748398432246161\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748345432962308\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748429324446976\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7483030746068335\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748428493581717\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74826170887004\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748366768275457\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748337115588772\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748210239120597\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748194345699799\n",
      "            Iterations: 40\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74832111065758\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748252099354339\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74820392696251\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748257131080866\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748320284191468\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74818832518957\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7482354620186875\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748063618417856\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748193671656978\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748163449402492\n",
      "            Iterations: 43\n",
      "            Function evaluations: 349\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748093303325129\n",
      "            Iterations: 41\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74796916770591\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747916153442661\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7481260610394305\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7479990118310464\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7478899443030365\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7479975762430495\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747986711530788\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748082809598378\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747879808626496\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747916339284938\n",
      "            Iterations: 47\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747846953213111\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747849960802007\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747823668332578\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747770413081325\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74768671936373\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747855703210931\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7476844645990015\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7476277313108906\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747831596706291\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747749359092728\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747608532795676\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747598379839674\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747657547224416\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747684996663169\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747651492625795\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747426959566901\n",
      "            Iterations: 47\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747541288579951\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747526753270345\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7475736323967626\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747478833351527\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74753339253865\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7473897525888535\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747363785054652\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747430629208637\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747317194444093\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747349974970602\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747372115786428\n",
      "            Iterations: 44\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747276952984455\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747259901652029\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747251806878633\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74720190978801\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747095326089495\n",
      "            Iterations: 45\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747185754238129\n",
      "            Iterations: 47\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747161905215407\n",
      "            Iterations: 48\n",
      "            Function evaluations: 391\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747165470136345\n",
      "            Iterations: 50\n",
      "            Function evaluations: 406\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747213473170337\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747101075716355\n",
      "            Iterations: 51\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7472346078466074\n",
      "            Iterations: 50\n",
      "            Function evaluations: 405\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747150564418916\n",
      "            Iterations: 42\n",
      "            Function evaluations: 343\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747168324666045\n",
      "            Iterations: 49\n",
      "            Function evaluations: 399\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747077209858885\n",
      "            Iterations: 51\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747128881752004\n",
      "            Iterations: 51\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747196783018877\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747113668258866\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74719371061318\n",
      "            Iterations: 52\n",
      "            Function evaluations: 423\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747123568681408\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747212291286938\n",
      "            Iterations: 51\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747115154603977\n",
      "            Iterations: 51\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.750784644247817\n",
      "            Iterations: 37\n",
      "            Function evaluations: 303\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747046028868947\n",
      "            Iterations: 49\n",
      "            Function evaluations: 399\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747016779596911\n",
      "            Iterations: 51\n",
      "            Function evaluations: 415\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74719675546109\n",
      "            Iterations: 52\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 52\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747226977064121\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747027838685488\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747121162495429\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747045021356566\n",
      "            Iterations: 50\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74711401180571\n",
      "            Iterations: 51\n",
      "            Function evaluations: 415\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7471956789396685\n",
      "            Iterations: 51\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 51\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747117121925669\n",
      "            Iterations: 49\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747102761675739\n",
      "            Iterations: 49\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7512856370174745\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747392746246069\n",
      "            Iterations: 54\n",
      "            Function evaluations: 437\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.74718397002313\n",
      "            Iterations: 54\n",
      "            Function evaluations: 437\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747215990424024\n",
      "            Iterations: 55\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747227714960811\n",
      "            Iterations: 55\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7471537547071865\n",
      "            Iterations: 53\n",
      "            Function evaluations: 429\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747239618810001\n",
      "            Iterations: 50\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7514626663894965\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751609119756706\n",
      "            Iterations: 37\n",
      "            Function evaluations: 302\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7472998440776175\n",
      "            Iterations: 53\n",
      "            Function evaluations: 430\n",
      "            Gradient evaluations: 53\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751665438147538\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747423072809673\n",
      "            Iterations: 54\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747393272543821\n",
      "            Iterations: 54\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 54\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7517479274923\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75184929586931\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751895991126965\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752018637158235\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.751885938948294\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752031701492686\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752079915902906\n",
      "            Iterations: 39\n",
      "            Function evaluations: 318\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752144484494704\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.747760562294637\n",
      "            Iterations: 55\n",
      "            Function evaluations: 447\n",
      "            Gradient evaluations: 55\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75222644242307\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752176021150273\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75231996231403\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752387861634235\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7524175457280045\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752497823182225\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752425796896402\n",
      "            Iterations: 40\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752685115957461\n",
      "            Iterations: 40\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748114648964981\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748148042592748\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7481594000508895\n",
      "            Iterations: 57\n",
      "            Function evaluations: 462\n",
      "            Gradient evaluations: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748203266311899\n",
      "            Iterations: 56\n",
      "            Function evaluations: 455\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.752874090009354\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75289329308394\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75290130638535\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753073885496827\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753115752745154\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748707388234957\n",
      "            Iterations: 56\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748767697070375\n",
      "            Iterations: 57\n",
      "            Function evaluations: 462\n",
      "            Gradient evaluations: 57\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7487205795910725\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753346898094247\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75330267004631\n",
      "            Iterations: 38\n",
      "            Function evaluations: 310\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753310979964867\n",
      "            Iterations: 35\n",
      "            Function evaluations: 287\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.748993419730564\n",
      "            Iterations: 56\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753625012449016\n",
      "            Iterations: 40\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753651799654451\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753706780541108\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753743463787355\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753834393868917\n",
      "            Iterations: 42\n",
      "            Function evaluations: 342\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749378044095565\n",
      "            Iterations: 58\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 58\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.753851921381365\n",
      "            Iterations: 37\n",
      "            Function evaluations: 303\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754057983785377\n",
      "            Iterations: 36\n",
      "            Function evaluations: 294\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75408687833972\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754138762020551\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754211271061097\n",
      "            Iterations: 37\n",
      "            Function evaluations: 304\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754265798627566\n",
      "            Iterations: 40\n",
      "            Function evaluations: 328\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754405242881142\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7544590255311245\n",
      "            Iterations: 39\n",
      "            Function evaluations: 319\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.749991957410234\n",
      "            Iterations: 56\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 56\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754560566889541\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754586734484815\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754719582057401\n",
      "            Iterations: 41\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754783768614982\n",
      "            Iterations: 38\n",
      "            Function evaluations: 311\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754913858495211\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754952621622719\n",
      "            Iterations: 43\n",
      "            Function evaluations: 352\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.754962077313234\n",
      "            Iterations: 41\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755014187662138\n",
      "            Iterations: 41\n",
      "            Function evaluations: 337\n",
      "            Gradient evaluations: 41\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755118647079288\n",
      "            Iterations: 38\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75503238573207\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7552122159279815\n",
      "            Iterations: 42\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7553800572646505\n",
      "            Iterations: 40\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755363150325243\n",
      "            Iterations: 38\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755553711604055\n",
      "            Iterations: 43\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755500506137761\n",
      "            Iterations: 42\n",
      "            Function evaluations: 342\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755633165255419\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75551338043862\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7556611827891935\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755724212414469\n",
      "            Iterations: 47\n",
      "            Function evaluations: 382\n",
      "            Gradient evaluations: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755840220095371\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755814705337782\n",
      "            Iterations: 45\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.755922975238969\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756080657413387\n",
      "            Iterations: 46\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7560975783965\n",
      "            Iterations: 45\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7561540878547355\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756364594640262\n",
      "            Iterations: 48\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 48\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772846321082769\n",
      "            Iterations: 29\n",
      "            Function evaluations: 237\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772830138071013\n",
      "            Iterations: 27\n",
      "            Function evaluations: 223\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.75651372572233\n",
      "            Iterations: 45\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772896775392429\n",
      "            Iterations: 30\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756663961706618\n",
      "            Iterations: 46\n",
      "            Function evaluations: 373\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7729167886220925\n",
      "            Iterations: 27\n",
      "            Function evaluations: 223\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756761095645025\n",
      "            Iterations: 44\n",
      "            Function evaluations: 359\n",
      "            Gradient evaluations: 44\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772895604833524\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.756960026947854\n",
      "            Iterations: 49\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772936031927706\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7728940553616255\n",
      "            Iterations: 30\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772908338731893\n",
      "            Iterations: 29\n",
      "            Function evaluations: 237\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77286948480983\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772933774512708\n",
      "            Iterations: 30\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773068172536438\n",
      "            Iterations: 28\n",
      "            Function evaluations: 231\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772913853572449\n",
      "            Iterations: 29\n",
      "            Function evaluations: 239\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.757526712892505\n",
      "            Iterations: 45\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 45\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7730043141486975\n",
      "            Iterations: 29\n",
      "            Function evaluations: 239\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773008228934704\n",
      "            Iterations: 30\n",
      "            Function evaluations: 248\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7730224268167465\n",
      "            Iterations: 28\n",
      "            Function evaluations: 231\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773059377501074\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773041385229693\n",
      "            Iterations: 31\n",
      "            Function evaluations: 256\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773017374025967\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.772961576311316\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773097477755426\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7731375147945485\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773176143999249\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773139237893447\n",
      "            Iterations: 28\n",
      "            Function evaluations: 232\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773114966154575\n",
      "            Iterations: 29\n",
      "            Function evaluations: 238\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773114129475248\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773318663207655\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773146310854472\n",
      "            Iterations: 30\n",
      "            Function evaluations: 245\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773147992252777\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773329489650065\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773165324260232\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773290200179722\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773199245019052\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773219385397935\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773427697987311\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773377931280914\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773228568014874\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773297133330447\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7734169577235015\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773374047566849\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773367601758247\n",
      "            Iterations: 31\n",
      "            Function evaluations: 256\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773359871963092\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773465018011027\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773454735987348\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773380799530295\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773381319411396\n",
      "            Iterations: 30\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7735008458642865\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7734995569070096\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7735196138831775\n",
      "            Iterations: 31\n",
      "            Function evaluations: 253\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773558746080569\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773410254567464\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773493946537339\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773590818251513\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773557319446657\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77359683320087\n",
      "            Iterations: 31\n",
      "            Function evaluations: 254\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773565249698628\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773655239056619\n",
      "            Iterations: 34\n",
      "            Function evaluations: 280\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7736719155268075\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773625909102958\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773696208353428\n",
      "            Iterations: 30\n",
      "            Function evaluations: 248\n",
      "            Gradient evaluations: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773754705096071\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773731143512379\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773676879914525\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773758199182567\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773585270818314\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77387972918667\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773681707468513\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773816475474296\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773783293994764\n",
      "            Iterations: 31\n",
      "            Function evaluations: 256\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773840196975427\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7737964928605106\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7738112430682165\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773852554336816\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773806594523434\n",
      "            Iterations: 31\n",
      "            Function evaluations: 258\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773933338217876\n",
      "            Iterations: 32\n",
      "            Function evaluations: 266\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773851300083674\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773909152720805\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773947712885036\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773940508827465\n",
      "            Iterations: 32\n",
      "            Function evaluations: 266\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773910178312707\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773954635413543\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774027854803709\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774031903899405\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774102402831328\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774015817032727\n",
      "            Iterations: 33\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.773959731300182\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774066855640488\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774152025297214\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774092898163669\n",
      "            Iterations: 32\n",
      "            Function evaluations: 262\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774032002620352\n",
      "            Iterations: 32\n",
      "            Function evaluations: 261\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774139141445698\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774147515534414\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774182268611911\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774213700052295\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774323315235808\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77430592454492\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774147894310373\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774278412607508\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774284421567199\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774279829081984\n",
      "            Iterations: 33\n",
      "            Function evaluations: 273\n",
      "            Gradient evaluations: 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774255553306047\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774285404601775\n",
      "            Iterations: 33\n",
      "            Function evaluations: 270\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774254719478771\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774320854050344\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774403905042709\n",
      "            Iterations: 32\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774284561574538\n",
      "            Iterations: 32\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774356732235045\n",
      "            Iterations: 31\n",
      "            Function evaluations: 255\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774407306811416\n",
      "            Iterations: 32\n",
      "            Function evaluations: 266\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774465312610029\n",
      "            Iterations: 34\n",
      "            Function evaluations: 279\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774384021895722\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774395990397547\n",
      "            Iterations: 33\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774412121023855\n",
      "            Iterations: 29\n",
      "            Function evaluations: 242\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774513145250202\n",
      "            Iterations: 34\n",
      "            Function evaluations: 279\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774439560127461\n",
      "            Iterations: 33\n",
      "            Function evaluations: 271\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774520515175775\n",
      "            Iterations: 34\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774489585989851\n",
      "            Iterations: 34\n",
      "            Function evaluations: 282\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774484805827692\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774498809442136\n",
      "            Iterations: 34\n",
      "            Function evaluations: 280\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774536204159903\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774509354642108\n",
      "            Iterations: 34\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774516068974865\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774548013676938\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774554310124903\n",
      "            Iterations: 35\n",
      "            Function evaluations: 288\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774544111305115\n",
      "            Iterations: 35\n",
      "            Function evaluations: 288\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7746134902315305\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77460197974766\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774523575105703\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774654555242093\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774663156431706\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774601987977107\n",
      "            Iterations: 32\n",
      "            Function evaluations: 265\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774652238885522\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7746717439487565\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774667667494004\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774689713141294\n",
      "            Iterations: 36\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774689548882403\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77471568263059\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7747407480563595\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774739352996761\n",
      "            Iterations: 36\n",
      "            Function evaluations: 298\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774754057859287\n",
      "            Iterations: 36\n",
      "            Function evaluations: 297\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774769774421261\n",
      "            Iterations: 37\n",
      "            Function evaluations: 308\n",
      "            Gradient evaluations: 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774781691885833\n",
      "            Iterations: 36\n",
      "            Function evaluations: 300\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774782198734451\n",
      "            Iterations: 36\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774781185725278\n",
      "            Iterations: 35\n",
      "            Function evaluations: 290\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774778681510387\n",
      "            Iterations: 37\n",
      "            Function evaluations: 307\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774796725889008\n",
      "            Iterations: 37\n",
      "            Function evaluations: 307\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774820424037973\n",
      "            Iterations: 36\n",
      "            Function evaluations: 300\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774826078750889\n",
      "            Iterations: 35\n",
      "            Function evaluations: 290\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774841925358913\n",
      "            Iterations: 35\n",
      "            Function evaluations: 292\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774845225488919\n",
      "            Iterations: 35\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774851504280549\n",
      "            Iterations: 36\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774854701263748\n",
      "            Iterations: 34\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774885732615523\n",
      "            Iterations: 35\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774879286064013\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77488114433636\n",
      "            Iterations: 37\n",
      "            Function evaluations: 307\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774897549563491\n",
      "            Iterations: 37\n",
      "            Function evaluations: 308\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774899367988725\n",
      "            Iterations: 38\n",
      "            Function evaluations: 315\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774920393626444\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774931235444745\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774942877008542\n",
      "            Iterations: 37\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.77495545178495\n",
      "            Iterations: 40\n",
      "            Function evaluations: 333\n",
      "            Gradient evaluations: 40\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7749503260616075\n",
      "            Iterations: 39\n",
      "            Function evaluations: 323\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7749635209348\n",
      "            Iterations: 38\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.774981036601162\n",
      "            Iterations: 37\n",
      "            Function evaluations: 303\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7749909497776795\n",
      "            Iterations: 39\n",
      "            Function evaluations: 322\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775014466572431\n",
      "            Iterations: 43\n",
      "            Function evaluations: 359\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.7750121302733985\n",
      "            Iterations: 43\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 43\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775016537879161\n",
      "            Iterations: 39\n",
      "            Function evaluations: 322\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775045470466733\n",
      "            Iterations: 42\n",
      "            Function evaluations: 347\n",
      "            Gradient evaluations: 42\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 6.775052005884613\n",
      "            Iterations: 38\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 38\n"
     ]
    }
   ],
   "source": [
    "# get the peak position dataframe of smooth data set\n",
    "true_df1, smooth_df1 = astropy_peak_matrix(datanm, dataz_matx, 0.1, 0, 10,gg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* true peak position in the two-gaussian_spectralshfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>144</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>144</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>144</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>146</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>146</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>150</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>150</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1\n",
       "0     50  400\n",
       "1     50  400\n",
       "2     50  400\n",
       "3     50  400\n",
       "4     51  400\n",
       "5     51  400\n",
       "6     51  400\n",
       "7     51  400\n",
       "8     52  400\n",
       "9     52  400\n",
       "10    52  400\n",
       "11    52  400\n",
       "12    52  400\n",
       "13    53  400\n",
       "14    53  400\n",
       "15    53  400\n",
       "16    53  400\n",
       "17    53  400\n",
       "18    53  400\n",
       "19    54  400\n",
       "20    54  400\n",
       "21    54  400\n",
       "22    54  400\n",
       "23    55  400\n",
       "24    55  400\n",
       "25    55  400\n",
       "26    55  400\n",
       "27    55  400\n",
       "28    55  400\n",
       "29    56  400\n",
       "..   ...  ...\n",
       "470  144  400\n",
       "471  144  399\n",
       "472  144  400\n",
       "473  145  400\n",
       "474  145  400\n",
       "475  145  400\n",
       "476  145  400\n",
       "477  145  400\n",
       "478  146  399\n",
       "479  146  399\n",
       "480  146  400\n",
       "481  146  400\n",
       "482  146  400\n",
       "483  146  400\n",
       "484  147  400\n",
       "485  147  400\n",
       "486  147  400\n",
       "487  147  400\n",
       "488  147  400\n",
       "489  148  400\n",
       "490  148  400\n",
       "491  148  400\n",
       "492  148  400\n",
       "493  149  400\n",
       "494  149  400\n",
       "495  149  400\n",
       "496  149  400\n",
       "497  149  400\n",
       "498  150  400\n",
       "499  150  400\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* smooth peak position in the two-gaussian_spectralshfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>55</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>55</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>55</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>55</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>55</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>56</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>56</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>56</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>56</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>57</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>152</td>\n",
       "      <td>384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>152</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>152</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>152</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>153</td>\n",
       "      <td>382.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>153</td>\n",
       "      <td>382.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>153</td>\n",
       "      <td>381.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>153</td>\n",
       "      <td>381.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>153</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>154</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>154</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>154</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>154</td>\n",
       "      <td>377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>154</td>\n",
       "      <td>377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>155</td>\n",
       "      <td>376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>155</td>\n",
       "      <td>375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>155</td>\n",
       "      <td>374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>155</td>\n",
       "      <td>374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>156</td>\n",
       "      <td>373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>156</td>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>156</td>\n",
       "      <td>371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>156</td>\n",
       "      <td>369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>156</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>157</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>157</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>157</td>\n",
       "      <td>363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>157</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>157</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>158</td>\n",
       "      <td>355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0     51  395.0\n",
       "1     51  395.0\n",
       "2     51  395.0\n",
       "3     51  395.0\n",
       "4     51  395.0\n",
       "5     52  395.0\n",
       "6     52  394.0\n",
       "7     52  394.0\n",
       "8     52  394.0\n",
       "9     53  394.0\n",
       "10    53  394.0\n",
       "11    53  394.0\n",
       "12    53  394.0\n",
       "13    53  394.0\n",
       "14    54  394.0\n",
       "15    54  394.0\n",
       "16    54  394.0\n",
       "17    54  394.0\n",
       "18    54  394.0\n",
       "19    55  394.0\n",
       "20    55  394.0\n",
       "21    55  394.0\n",
       "22    55  393.0\n",
       "23    55  393.0\n",
       "24    56  393.0\n",
       "25    56  393.0\n",
       "26    56  393.0\n",
       "27    56  393.0\n",
       "28    56  393.0\n",
       "29    57  393.0\n",
       "..   ...    ...\n",
       "470  152  384.0\n",
       "471  152  383.0\n",
       "472  152  383.0\n",
       "473  152  383.0\n",
       "474  153  382.0\n",
       "475  153  382.0\n",
       "476  153  381.0\n",
       "477  153  381.0\n",
       "478  153  380.0\n",
       "479  154  379.0\n",
       "480  154  379.0\n",
       "481  154  378.0\n",
       "482  154  377.0\n",
       "483  154  377.0\n",
       "484  155  376.0\n",
       "485  155  375.0\n",
       "486  155  374.0\n",
       "487  155  374.0\n",
       "488  156  373.0\n",
       "489  156  372.0\n",
       "490  156  371.0\n",
       "491  156  369.0\n",
       "492  156  368.0\n",
       "493  157  367.0\n",
       "494  157  365.0\n",
       "495  157  363.0\n",
       "496  157  361.0\n",
       "497  157  359.0\n",
       "498  158  355.0\n",
       "499  158    NaN\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* py-earth and peakutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earth_peak_matrix(nm_array,data_matrix,noise_coefficient,threshold, min_dist):\n",
    "    num_array = np.shape(data_matrix)[1]\n",
    "    \n",
    "    true_peak = []\n",
    "    smooth_peak = []\n",
    "    \n",
    "    for i in range(500):\n",
    "        data_array = data_matrix[:, i]\n",
    "        noise_array = add_noise(nm_array, data_array, noise_coefficient)\n",
    "        smooth_array = Earth_Smoothing(nm_array, data_array,noise_coefficient)\n",
    "        \n",
    "        indexes=findpeak(data_array, threshold, min_dist).tolist()\n",
    "        true_peak.append(indexes)\n",
    "        \n",
    "        indexes1=findpeak(smooth_array, threshold, min_dist).tolist()\n",
    "        smooth_peak.append(indexes1)\n",
    "                \n",
    "        # transfer to dataframe\n",
    "        true_df=pd.DataFrame(true_peak)\n",
    "        smooth_df=pd.DataFrame(smooth_peak)\n",
    "    \n",
    "    return true_df, smooth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn_contrib_py_earth-0.1.0-py3.5-linux-x86_64.egg/pyearth/earth.py:802: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  pruning_passer.run()\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn_contrib_py_earth-0.1.0-py3.5-linux-x86_64.egg/pyearth/earth.py:1055: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n"
     ]
    }
   ],
   "source": [
    "## get the peak position dataframe of true data set\n",
    "true_df, smooth_df = earth_peak_matrix(datanm, dataz_matx, 0.1, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>54</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>55</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>144</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>144</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>144</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>145</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>146</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>146</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>146</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>148</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>149</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>150</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>150</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1\n",
       "0     50  400\n",
       "1     50  400\n",
       "2     50  400\n",
       "3     50  400\n",
       "4     51  400\n",
       "5     51  400\n",
       "6     51  400\n",
       "7     51  400\n",
       "8     52  400\n",
       "9     52  400\n",
       "10    52  400\n",
       "11    52  400\n",
       "12    52  400\n",
       "13    53  400\n",
       "14    53  400\n",
       "15    53  400\n",
       "16    53  400\n",
       "17    53  400\n",
       "18    53  400\n",
       "19    54  400\n",
       "20    54  400\n",
       "21    54  400\n",
       "22    54  400\n",
       "23    55  400\n",
       "24    55  400\n",
       "25    55  400\n",
       "26    55  400\n",
       "27    55  400\n",
       "28    55  400\n",
       "29    56  400\n",
       "..   ...  ...\n",
       "470  144  400\n",
       "471  144  399\n",
       "472  144  400\n",
       "473  145  400\n",
       "474  145  400\n",
       "475  145  400\n",
       "476  145  400\n",
       "477  145  400\n",
       "478  146  399\n",
       "479  146  399\n",
       "480  146  400\n",
       "481  146  400\n",
       "482  146  400\n",
       "483  146  400\n",
       "484  147  400\n",
       "485  147  400\n",
       "486  147  400\n",
       "487  147  400\n",
       "488  147  400\n",
       "489  148  400\n",
       "490  148  400\n",
       "491  148  400\n",
       "492  148  400\n",
       "493  149  400\n",
       "494  149  400\n",
       "495  149  400\n",
       "496  149  400\n",
       "497  149  400\n",
       "498  150  400\n",
       "499  150  400\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>195.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>390.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56</td>\n",
       "      <td>211.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>213.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>212.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55</td>\n",
       "      <td>212.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55</td>\n",
       "      <td>388.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62</td>\n",
       "      <td>390.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>57</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>57</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>63</td>\n",
       "      <td>384.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>51</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>63</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>63</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>52</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>53</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>148</td>\n",
       "      <td>476.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>159</td>\n",
       "      <td>480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>159</td>\n",
       "      <td>480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>132</td>\n",
       "      <td>452.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>132</td>\n",
       "      <td>451.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>132</td>\n",
       "      <td>449.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>175</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>207</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>157</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>158</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>161</td>\n",
       "      <td>614.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>174</td>\n",
       "      <td>210.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>208</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2   3\n",
       "0     53  394.0    NaN NaN\n",
       "1     54  394.0    NaN NaN\n",
       "2     54  394.0    NaN NaN\n",
       "3     54  394.0    NaN NaN\n",
       "4     52  195.0  393.0 NaN\n",
       "5     55  389.0    NaN NaN\n",
       "6     56  390.0    NaN NaN\n",
       "7     60  389.0    NaN NaN\n",
       "8     56  389.0    NaN NaN\n",
       "9     56  211.0  389.0 NaN\n",
       "10    55  213.0  389.0 NaN\n",
       "11    55  212.0  389.0 NaN\n",
       "12    55  212.0  389.0 NaN\n",
       "13    59  389.0    NaN NaN\n",
       "14    55  388.0    NaN NaN\n",
       "15    59  389.0    NaN NaN\n",
       "16    59  389.0    NaN NaN\n",
       "17    59  389.0    NaN NaN\n",
       "18    60  389.0    NaN NaN\n",
       "19    62  390.0    NaN NaN\n",
       "20    57  385.0    NaN NaN\n",
       "21    57  385.0    NaN NaN\n",
       "22    57  385.0    NaN NaN\n",
       "23    63  384.0    NaN NaN\n",
       "24    51  389.0    NaN NaN\n",
       "25    63  389.0    NaN NaN\n",
       "26    63  389.0    NaN NaN\n",
       "27    52  389.0    NaN NaN\n",
       "28    53  389.0    NaN NaN\n",
       "29    52  389.0    NaN NaN\n",
       "..   ...    ...    ...  ..\n",
       "470  128    NaN    NaN NaN\n",
       "471  128    NaN    NaN NaN\n",
       "472  128    NaN    NaN NaN\n",
       "473  127    NaN    NaN NaN\n",
       "474  148  476.0    NaN NaN\n",
       "475  159  480.0    NaN NaN\n",
       "476  158    NaN    NaN NaN\n",
       "477  159  480.0    NaN NaN\n",
       "478  132  452.0    NaN NaN\n",
       "479  132  451.0    NaN NaN\n",
       "480  132  449.0    NaN NaN\n",
       "481  175  230.0    NaN NaN\n",
       "482  207  230.0    NaN NaN\n",
       "483  159    NaN    NaN NaN\n",
       "484  160    NaN    NaN NaN\n",
       "485  159    NaN    NaN NaN\n",
       "486  157  309.0    NaN NaN\n",
       "487  159    NaN    NaN NaN\n",
       "488  160    NaN    NaN NaN\n",
       "489  159    NaN    NaN NaN\n",
       "490  158  309.0    NaN NaN\n",
       "491  159    NaN    NaN NaN\n",
       "492  159    NaN    NaN NaN\n",
       "493  159    NaN    NaN NaN\n",
       "494  161    NaN    NaN NaN\n",
       "495  161  614.0    NaN NaN\n",
       "496  159    NaN    NaN NaN\n",
       "497  174  210.0  230.0 NaN\n",
       "498  208    NaN    NaN NaN\n",
       "499  208  230.0    NaN NaN\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 500)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataz_matx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak width and fwhm Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakchar(data_nm, data_z_array, peak_index):\n",
    "    \"\"\"find the peak width, and intensity\"\"\"\n",
    "    num_peaks = len(peak_index)\n",
    "    \n",
    "    #array of peak height\n",
    "    height = [data_z_array[idx] for idx in peak_index]\n",
    "    \n",
    "    #array of peak width\n",
    "    half_height = [ht / 2 for ht in height]\n",
    "\n",
    "    fwhm_idx_1 = np.empty_like(half_height)\n",
    "    fwhm_idx_2 = np.empty_like(fwhm_idx_1)\n",
    "    fwhm_nm_1 = np.empty_like(fwhm_idx_1)\n",
    "    fwhm_nm_2 = np.empty_like(fwhm_idx_1)\n",
    "    \n",
    "    for i in range(num_peaks):\n",
    "        #find the index and nmof the left side of the fwhm\n",
    "        if i == 0:\n",
    "            fwhm_idx_1[i] = find_nearest(data_z_array[0:peak_index[i]], half_height[i])\n",
    "        else:\n",
    "            fwhm_idx_1[i] = find_nearest(data_z_array[peak_index[i-1]:peak_index[i]], half_height[i]) + peak_index[i-1]\n",
    "\n",
    "        fwhm_nm_1[i] = data_nm[int(fwhm_idx_1[i])]\n",
    "        \n",
    "        #find the index and nm of the right side of the fwhm   \n",
    "        fwhm_idx_2[i] = find_nearest(data_z_array[peak_index[i]:], half_height[i]) + peak_index[i]\n",
    "\n",
    "        fwhm_nm_2[i] = data_nm[int(fwhm_idx_2[i])]\n",
    "    \n",
    "    #find fwhm\n",
    "    fwhm = fwhm_nm_2 - fwhm_nm_1\n",
    "\n",
    "    return height, fwhm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earth_peak_matrix(nm_array,data_matrix,noise_coefficient,threshold, min_dist):\n",
    "    num_array = np.shape(data_matrix)[1]\n",
    "    \n",
    "    true_peak = []\n",
    "    smooth_peak = []\n",
    "    \n",
    "    for i in range(num_array):\n",
    "        data_array = data_matrix[:, i]\n",
    "        noise_array = add_noise(nm_array, data_array, noise_coefficient)\n",
    "        smooth_array = Earth_Smoothing(nm_array, data_array,noise_coefficient)\n",
    "        \n",
    "        indexes=findpeak(data_array, threshold, min_dist).tolist()\n",
    "        true_peak.append(indexes)\n",
    "        \n",
    "        indexes1=findpeak(smooth_array, threshold, min_dist)\n",
    "        smooth_peak.append(indexes1)\n",
    "                \n",
    "        # transfer to dataframe\n",
    "        true_df=pd.DataFrame(true_peak)\n",
    "        smooth_df=pd.DataFrame(smooth_peak)\n",
    "    \n",
    "    return true_df, smooth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_matrix(nm_array,data_matrix, threshold, mindist):\n",
    "    \"\"\"find peaks in a data matrix\"\"\"\n",
    "    peak_idx_matx = []\n",
    "    peak_height_matx = []\n",
    "    peak_fwhm_matx = []\n",
    "    \n",
    "    for i in range(500):\n",
    "        data_timeslice = data_matrix.values[:, i]\n",
    "        \n",
    "        peak_idx = findpeak(data_timeslice, threshold, mindist).tolist()\n",
    "        peak_idx_matx.append(peak_idx)\n",
    "        \n",
    "        \n",
    "        peak_height, peak_fwhm = peakchar(nm_array, data_timeslice, peak_idx)\n",
    "        \n",
    "        peak_height_matx.append(peak_height)\n",
    "        peak_fwhm_matx.append(peak_fwhm)\n",
    "        \n",
    "        # transfer to dataframe\n",
    "        peak_idx_df=pd.DataFrame(peak_idx_matx)\n",
    "        peak_height_df=pd.DataFrame(peak_height_matx)\n",
    "        peak_fwhm_df=pd.DataFrame(peak_fwhm_matx)\n",
    "        \n",
    "    return peak_idx_df, peak_height_df, peak_fwhm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.80583890e-02,  7.80583890e-02,  7.80583890e-02,  7.80583890e-02,\n",
       "        7.80583890e-02,  7.80583890e-02,  7.80583890e-02,  7.80583890e-02,\n",
       "        7.80583890e-02,  7.80583890e-02,  7.80583890e-02,  7.80583890e-02,\n",
       "        7.84860726e-02,  7.97691233e-02,  8.19075411e-02,  8.49013260e-02,\n",
       "        8.87504780e-02,  9.34549972e-02,  9.85871999e-02,  1.03719403e-01,\n",
       "        1.08851605e-01,  1.13983808e-01,  1.19116011e-01,  1.24248213e-01,\n",
       "        1.29380416e-01,  1.34512619e-01,  1.39644822e-01,  1.44777024e-01,\n",
       "        1.49909227e-01,  1.55041430e-01,  1.60173632e-01,  1.65305835e-01,\n",
       "        1.70438038e-01,  1.75570241e-01,  1.80702443e-01,  1.85834646e-01,\n",
       "        1.90966849e-01,  1.96099051e-01,  2.01231254e-01,  2.06363457e-01,\n",
       "        2.11495660e-01,  2.16627862e-01,  2.21760065e-01,  2.26892268e-01,\n",
       "        2.32024470e-01,  2.37156673e-01,  2.42288876e-01,  2.47421079e-01,\n",
       "        2.52553281e-01,  2.57685484e-01,  2.62817687e-01,  2.67949889e-01,\n",
       "        2.73082092e-01,  2.78214295e-01,  2.83280615e-01,  2.88211957e-01,\n",
       "        2.93003500e-01,  2.97650424e-01,  3.02147906e-01,  3.06491128e-01,\n",
       "        3.10675268e-01,  3.14695506e-01,  3.18547021e-01,  3.22224992e-01,\n",
       "        3.25724599e-01,  3.29041021e-01,  3.32169438e-01,  3.35105028e-01,\n",
       "        3.37842971e-01,  3.40378446e-01,  3.42706634e-01,  3.44822712e-01,\n",
       "        3.46721861e-01,  3.48399259e-01,  3.49850087e-01,  3.51069523e-01,\n",
       "        3.52052747e-01,  3.52794939e-01,  3.53291276e-01,  3.53536940e-01,\n",
       "        3.53527109e-01,  3.53387923e-01,  3.53248737e-01,  3.53109552e-01,\n",
       "        3.52970366e-01,  3.52831181e-01,  3.52691995e-01,  3.52552809e-01,\n",
       "        3.52413624e-01,  3.52274438e-01,  3.52135253e-01,  3.51996067e-01,\n",
       "        3.51856881e-01,  3.51717696e-01,  3.51578510e-01,  3.51439325e-01,\n",
       "        3.51300139e-01,  3.51160953e-01,  3.51021768e-01,  3.50882582e-01,\n",
       "        3.50743397e-01,  3.50604211e-01,  3.50327325e-01,  3.49781921e-01,\n",
       "        3.48978329e-01,  3.47926874e-01,  3.46637886e-01,  3.45121691e-01,\n",
       "        3.43388617e-01,  3.41448991e-01,  3.39313142e-01,  3.36991396e-01,\n",
       "        3.34494082e-01,  3.31831527e-01,  3.29014058e-01,  3.26052003e-01,\n",
       "        3.22955689e-01,  3.19735445e-01,  3.16401597e-01,  3.12964474e-01,\n",
       "        3.09434403e-01,  3.05821711e-01,  3.02136726e-01,  2.98389776e-01,\n",
       "        2.94591187e-01,  2.90751289e-01,  2.86880408e-01,  2.82988871e-01,\n",
       "        2.79087007e-01,  2.75183422e-01,  2.71279837e-01,  2.67376252e-01,\n",
       "        2.63472667e-01,  2.59569081e-01,  2.55665496e-01,  2.51761911e-01,\n",
       "        2.47858326e-01,  2.43954741e-01,  2.40051155e-01,  2.36147570e-01,\n",
       "        2.32243985e-01,  2.28340400e-01,  2.24436815e-01,  2.20533229e-01,\n",
       "        2.16629644e-01,  2.12726059e-01,  2.08822474e-01,  2.04918889e-01,\n",
       "        2.01015303e-01,  1.97111718e-01,  1.93208133e-01,  1.89304548e-01,\n",
       "        1.85400963e-01,  1.81497377e-01,  1.77593792e-01,  1.73690207e-01,\n",
       "        1.69786622e-01,  1.65883037e-01,  1.61979451e-01,  1.58075866e-01,\n",
       "        1.54172281e-01,  1.50268696e-01,  1.46365111e-01,  1.42461526e-01,\n",
       "        1.38557940e-01,  1.34654355e-01,  1.30750770e-01,  1.26847185e-01,\n",
       "        1.22943600e-01,  1.19040014e-01,  1.15136429e-01,  1.11232844e-01,\n",
       "        1.07329259e-01,  1.03425674e-01,  9.95220884e-02,  9.56185032e-02,\n",
       "        9.17149180e-02,  8.78113328e-02,  8.39077476e-02,  8.00041624e-02,\n",
       "        7.61005772e-02,  7.21969920e-02,  6.82934068e-02,  6.43898216e-02,\n",
       "        6.04862364e-02,  5.65826512e-02,  5.30249183e-02,  5.01182015e-02,\n",
       "        4.78014679e-02,  4.60136849e-02,  4.46938197e-02,  4.37808396e-02,\n",
       "        4.32137117e-02,  4.29314033e-02,  4.28728817e-02,  4.29771142e-02,\n",
       "        4.31830678e-02,  4.34297100e-02,  4.36560080e-02,  4.38009289e-02,\n",
       "        4.38034401e-02,  4.37144021e-02,  4.36253641e-02,  4.35363262e-02,\n",
       "        4.34472882e-02,  4.33582503e-02,  4.32692123e-02,  4.31801743e-02,\n",
       "        4.30911364e-02,  4.30020984e-02,  4.29130604e-02,  4.28240225e-02,\n",
       "        4.27349845e-02,  4.26459465e-02,  4.25569086e-02,  4.24678706e-02,\n",
       "        4.23788326e-02,  4.22897947e-02,  4.22007567e-02,  4.21117187e-02,\n",
       "        4.20226808e-02,  4.19336428e-02,  4.18446048e-02,  4.17555669e-02,\n",
       "        4.16665289e-02,  4.15774909e-02,  4.14884530e-02,  4.13994150e-02,\n",
       "        4.13103771e-02,  4.12213391e-02,  4.11323011e-02,  4.10432632e-02,\n",
       "        4.09542252e-02,  4.08651872e-02,  4.07761493e-02,  4.06871113e-02,\n",
       "        4.05980733e-02,  4.05090354e-02,  4.04199974e-02,  4.03309594e-02,\n",
       "        4.02419215e-02,  4.01528835e-02,  4.00638455e-02,  3.99748076e-02,\n",
       "        3.98857696e-02,  3.97967316e-02,  3.97076937e-02,  3.96186557e-02,\n",
       "        3.95296177e-02,  3.94405798e-02,  3.93515418e-02,  3.92625039e-02,\n",
       "        3.91734659e-02,  3.90844279e-02,  3.89953900e-02,  3.89063520e-02,\n",
       "        3.88173140e-02,  3.87282761e-02,  3.86392381e-02,  3.85502001e-02,\n",
       "        3.84611622e-02,  3.83721242e-02,  3.82830862e-02,  3.81940483e-02,\n",
       "        3.81050103e-02,  3.80159723e-02,  3.79269344e-02,  3.78378964e-02,\n",
       "        3.77488584e-02,  3.76598205e-02,  3.75707825e-02,  3.74817445e-02,\n",
       "        3.73927066e-02,  3.73036686e-02,  3.72146307e-02,  3.71255927e-02,\n",
       "        3.70365547e-02,  3.69475168e-02,  3.68584788e-02,  3.67694408e-02,\n",
       "        3.66804029e-02,  3.65913649e-02,  3.65023269e-02,  3.64132890e-02,\n",
       "        3.63242510e-02,  3.62352130e-02,  3.61461751e-02,  3.60571371e-02,\n",
       "        3.59680991e-02,  3.58790612e-02,  3.57900232e-02,  3.57009852e-02,\n",
       "        3.56119473e-02,  3.55229093e-02,  3.54338713e-02,  3.53448334e-02,\n",
       "        3.52557954e-02,  3.51667575e-02,  3.50777195e-02,  3.49886815e-02,\n",
       "        3.48996436e-02,  3.48106056e-02,  3.47215676e-02,  3.46325297e-02,\n",
       "        3.45434917e-02,  3.44544537e-02,  3.43654158e-02,  3.42763778e-02,\n",
       "        3.41873398e-02,  3.40983019e-02,  3.40092639e-02,  3.39202259e-02,\n",
       "        3.38311880e-02,  3.37421500e-02,  3.36531120e-02,  3.35640741e-02,\n",
       "        3.34750361e-02,  3.33859981e-02,  3.32969602e-02,  3.32079222e-02,\n",
       "        3.31188843e-02,  3.30298463e-02,  3.29408083e-02,  3.28517704e-02,\n",
       "        3.27627324e-02,  3.26736944e-02,  3.25846565e-02,  3.24956185e-02,\n",
       "        3.24065805e-02,  3.23175426e-02,  3.22285046e-02,  3.21394666e-02,\n",
       "        3.20504287e-02,  3.19613907e-02,  3.18723527e-02,  3.17833148e-02,\n",
       "        3.16942768e-02,  3.16052388e-02,  3.15162009e-02,  3.14271629e-02,\n",
       "        3.13381249e-02,  3.12490870e-02,  3.11600490e-02,  3.10710111e-02,\n",
       "        3.09819731e-02,  3.08929351e-02,  3.08038972e-02,  3.07148592e-02,\n",
       "        3.06258212e-02,  3.05367833e-02,  3.04477453e-02,  3.03587073e-02,\n",
       "        3.02696694e-02,  3.01806314e-02,  3.00915934e-02,  3.00025555e-02,\n",
       "        2.99135175e-02,  2.98244795e-02,  2.97354416e-02,  2.96464036e-02,\n",
       "        2.95573656e-02,  2.94683277e-02,  2.93792897e-02,  2.92902517e-02,\n",
       "        2.92012138e-02,  2.91121758e-02,  2.90231379e-02,  2.89340999e-02,\n",
       "        2.88450619e-02,  2.87560240e-02,  2.86669860e-02,  2.85779480e-02,\n",
       "        2.84889101e-02,  2.83998721e-02,  2.83108341e-02,  2.82217962e-02,\n",
       "        2.81327582e-02,  2.80437202e-02,  2.79546823e-02,  2.78656443e-02,\n",
       "        2.77766063e-02,  2.76875684e-02,  2.75985304e-02,  2.75094924e-02,\n",
       "        2.74204545e-02,  2.73314165e-02,  2.72423785e-02,  2.71533406e-02,\n",
       "        2.70643026e-02,  2.69752647e-02,  2.68862267e-02,  2.67971887e-02,\n",
       "        2.67081508e-02,  2.66191128e-02,  2.65300748e-02,  2.64410369e-02,\n",
       "        2.63519989e-02,  2.62629609e-02,  2.61739230e-02,  2.60848850e-02,\n",
       "        2.59958470e-02,  2.59068091e-02,  2.58177711e-02,  2.57287331e-02,\n",
       "        2.56396952e-02,  2.55506572e-02,  2.54616192e-02,  2.53725813e-02,\n",
       "        2.52835433e-02,  2.51945053e-02,  2.51054674e-02,  2.50164294e-02,\n",
       "        2.49273915e-02,  2.48383535e-02,  2.47493155e-02,  2.46602776e-02,\n",
       "        2.45712396e-02,  2.44822016e-02,  2.43931637e-02,  2.43041257e-02,\n",
       "        2.42150877e-02,  2.41260498e-02,  2.40370118e-02,  2.39479738e-02,\n",
       "        2.38589359e-02,  2.37698979e-02,  2.36808599e-02,  2.35918220e-02,\n",
       "        2.35027840e-02,  2.34137460e-02,  2.33247081e-02,  2.32356701e-02,\n",
       "        2.31466321e-02,  2.30575942e-02,  2.29685562e-02,  2.28795183e-02,\n",
       "        2.27904803e-02,  2.27014423e-02,  2.26124044e-02,  2.25233664e-02,\n",
       "        2.24343284e-02,  2.23452905e-02,  2.22562525e-02,  2.21672145e-02,\n",
       "        2.20781766e-02,  2.19891386e-02,  2.19001006e-02,  2.18110627e-02,\n",
       "        2.17220247e-02,  2.16329867e-02,  2.15439488e-02,  2.14549108e-02,\n",
       "        2.13658728e-02,  2.12768349e-02,  2.11877969e-02,  2.10987589e-02,\n",
       "        2.10097210e-02,  2.09206830e-02,  2.08316451e-02,  2.07426071e-02,\n",
       "        2.06535691e-02,  2.05645312e-02,  2.04754932e-02,  2.03864552e-02,\n",
       "        2.02974173e-02,  2.02083793e-02,  2.01193413e-02,  2.00303034e-02,\n",
       "        1.99412654e-02,  1.98522274e-02,  1.97631895e-02,  1.96741515e-02,\n",
       "        1.95851135e-02,  1.94960756e-02,  1.94070376e-02,  1.93179996e-02,\n",
       "        1.92289617e-02,  1.91399237e-02,  1.90508857e-02,  1.89618478e-02,\n",
       "        1.88728098e-02,  1.87837719e-02,  1.86947339e-02,  1.86056959e-02,\n",
       "        1.85166580e-02,  1.84276200e-02,  1.83385820e-02,  1.82495441e-02,\n",
       "        1.81605061e-02,  1.80714681e-02,  1.79824302e-02,  1.78933922e-02,\n",
       "        1.78043542e-02,  1.77153163e-02,  1.76262783e-02,  1.75372403e-02,\n",
       "        1.74482024e-02,  1.73591644e-02,  1.72701264e-02,  1.71810885e-02,\n",
       "        1.70920505e-02,  1.70030125e-02,  1.69139746e-02,  1.68249366e-02,\n",
       "        1.67358987e-02,  1.66468607e-02,  1.65578227e-02,  1.64687848e-02,\n",
       "        1.63797468e-02,  1.62907088e-02,  1.62016709e-02,  1.61126329e-02,\n",
       "        1.60235949e-02,  1.59345570e-02,  1.58455190e-02,  1.57564810e-02,\n",
       "        1.56674431e-02,  1.55784051e-02,  1.54893671e-02,  1.54003292e-02,\n",
       "        1.53112912e-02,  1.52222532e-02,  1.51332153e-02,  1.50441773e-02,\n",
       "        1.49551393e-02,  1.48661014e-02,  1.47770634e-02,  1.46880255e-02,\n",
       "        1.45989875e-02,  1.45099495e-02,  1.44209116e-02,  1.43318736e-02,\n",
       "        1.42428356e-02,  1.41537977e-02,  1.40647597e-02,  1.39757217e-02,\n",
       "        1.38866838e-02,  1.37976458e-02,  1.37086078e-02,  1.36195699e-02,\n",
       "        1.35305319e-02,  1.34414939e-02,  1.33524560e-02,  1.32634180e-02,\n",
       "        1.31743800e-02,  1.30853421e-02,  1.29963041e-02,  1.29072661e-02,\n",
       "        1.28182282e-02,  1.27291902e-02,  1.26401523e-02,  1.25511143e-02,\n",
       "        1.24620763e-02,  1.23730384e-02,  1.22840004e-02,  1.21949624e-02,\n",
       "        1.21059245e-02,  1.20168865e-02,  1.19278485e-02,  1.18388106e-02,\n",
       "        1.17497726e-02,  1.16607346e-02,  1.15716967e-02,  1.14826587e-02,\n",
       "        1.13936207e-02,  1.13045828e-02,  1.12155448e-02,  1.11265068e-02,\n",
       "        1.10374689e-02,  1.09484309e-02,  1.08593929e-02,  1.07703550e-02,\n",
       "        1.06813170e-02,  1.05922791e-02,  1.05032411e-02,  1.04142031e-02,\n",
       "        1.03251652e-02,  1.02361272e-02,  1.01470892e-02,  1.00580513e-02,\n",
       "        9.96901330e-03,  9.87997533e-03,  9.79093737e-03,  9.70189940e-03,\n",
       "        9.61286143e-03,  9.52382347e-03,  9.43478550e-03,  9.34574754e-03,\n",
       "        9.25670957e-03,  9.16767161e-03,  9.07863364e-03,  8.98959568e-03,\n",
       "        8.90055771e-03,  8.81151975e-03,  8.72248178e-03,  8.63344382e-03,\n",
       "        8.54440585e-03,  8.45536789e-03,  8.36632992e-03,  8.27729196e-03,\n",
       "        8.18825399e-03,  8.09921603e-03,  8.01017806e-03,  7.92114010e-03,\n",
       "        7.83210213e-03,  7.74306417e-03,  7.65402620e-03,  7.56498823e-03,\n",
       "        7.47595027e-03,  7.38691230e-03,  7.29787434e-03,  7.20883637e-03,\n",
       "        7.11979841e-03,  7.03076044e-03,  6.94172248e-03,  6.85268451e-03,\n",
       "        6.76364655e-03,  6.67460858e-03,  6.58557062e-03,  6.49653265e-03,\n",
       "        6.40749469e-03,  6.31845672e-03,  6.22941876e-03,  6.14038079e-03,\n",
       "        6.05134283e-03,  5.96230486e-03,  5.87326690e-03,  5.78422893e-03,\n",
       "        5.69519097e-03,  5.60615300e-03,  5.51711503e-03,  5.42807707e-03,\n",
       "        5.33903910e-03,  5.25000114e-03,  5.16096317e-03,  5.07192521e-03,\n",
       "        4.98288724e-03,  4.89384928e-03,  4.80481131e-03,  4.71577335e-03,\n",
       "        4.62673538e-03,  4.53769742e-03,  4.44865945e-03,  4.35962149e-03,\n",
       "        4.27058352e-03,  4.18154556e-03,  4.09250759e-03,  4.00346963e-03,\n",
       "        3.91443166e-03,  3.82539370e-03,  3.73635573e-03,  3.64731777e-03,\n",
       "        3.55827980e-03,  3.46924183e-03,  3.38020387e-03,  3.29116590e-03,\n",
       "        3.20212794e-03,  3.11308997e-03,  3.02405201e-03,  2.93501404e-03,\n",
       "        2.84597608e-03,  2.75693811e-03,  2.66790015e-03,  2.57886218e-03,\n",
       "        2.48982422e-03,  2.40078625e-03,  2.31174829e-03,  2.22271032e-03,\n",
       "        2.13367236e-03,  2.04463439e-03,  1.95559643e-03,  1.86655846e-03,\n",
       "        1.77752050e-03,  1.68848253e-03,  1.59944457e-03,  1.51040660e-03,\n",
       "        1.42136863e-03,  1.33233067e-03,  1.24329270e-03,  1.15425474e-03,\n",
       "        1.06521677e-03,  9.76178809e-04,  8.87140844e-04,  7.98102878e-04,\n",
       "        7.09064913e-04,  6.20026948e-04,  5.30988983e-04,  4.41951018e-04,\n",
       "        3.52913052e-04,  2.63875087e-04,  1.74837122e-04,  8.57991566e-05,\n",
       "       -3.23880857e-06, -9.22767738e-05, -1.81314739e-04, -2.70352704e-04,\n",
       "       -3.59390669e-04, -4.48428635e-04, -5.37466600e-04, -6.26504565e-04])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_matx.values[:, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peak_idx_df, peak_height_df, peak_fwhm_df = peak_matrix(datanm,smooth_matx, 0.00, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001380</td>\n",
       "      <td>0.302165</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996455</td>\n",
       "      <td>0.299229</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991564</td>\n",
       "      <td>0.296316</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.986697</td>\n",
       "      <td>0.293449</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991028</td>\n",
       "      <td>0.071419</td>\n",
       "      <td>0.288715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.976676</td>\n",
       "      <td>0.284159</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.282529</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.951091</td>\n",
       "      <td>0.278439</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.959928</td>\n",
       "      <td>0.275873</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.949348</td>\n",
       "      <td>0.074311</td>\n",
       "      <td>0.279957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.952395</td>\n",
       "      <td>0.073777</td>\n",
       "      <td>0.276532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.947682</td>\n",
       "      <td>0.073361</td>\n",
       "      <td>0.273819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.942981</td>\n",
       "      <td>0.072949</td>\n",
       "      <td>0.271137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.263075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.960959</td>\n",
       "      <td>0.260410</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.964248</td>\n",
       "      <td>0.257910</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.960055</td>\n",
       "      <td>0.255360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.955871</td>\n",
       "      <td>0.252845</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.935662</td>\n",
       "      <td>0.249156</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.933385</td>\n",
       "      <td>0.251037</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.907024</td>\n",
       "      <td>0.234401</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.902008</td>\n",
       "      <td>0.232047</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.898273</td>\n",
       "      <td>0.229733</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.888356</td>\n",
       "      <td>0.227433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.896237</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.879443</td>\n",
       "      <td>0.236926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.876039</td>\n",
       "      <td>0.234282</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.875515</td>\n",
       "      <td>0.232361</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.871059</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.871220</td>\n",
       "      <td>0.226320</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.086808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.086376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.085947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.086380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.089813</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.092922</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.091235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.092203</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.084281</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.083866</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.083452</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.114708</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.099097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.093780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.087742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.093034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.098583</td>\n",
       "      <td>0.025633</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.092290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.086367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.092737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.097053</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.093289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.091609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.089682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.088273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.089833</td>\n",
       "      <td>0.020148</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.090194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.131066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.106752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.103360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2\n",
       "0    1.001380  0.302165       NaN\n",
       "1    0.996455  0.299229       NaN\n",
       "2    0.991564  0.296316       NaN\n",
       "3    0.986697  0.293449       NaN\n",
       "4    0.991028  0.071419  0.288715\n",
       "5    0.976676  0.284159       NaN\n",
       "6    0.973991  0.282529       NaN\n",
       "7    0.951091  0.278439       NaN\n",
       "8    0.959928  0.275873       NaN\n",
       "9    0.949348  0.074311  0.279957\n",
       "10   0.952395  0.073777  0.276532\n",
       "11   0.947682  0.073361  0.273819\n",
       "12   0.942981  0.072949  0.271137\n",
       "13   0.972625  0.263075       NaN\n",
       "14   0.960959  0.260410       NaN\n",
       "15   0.964248  0.257910       NaN\n",
       "16   0.960055  0.255360       NaN\n",
       "17   0.955871  0.252845       NaN\n",
       "18   0.935662  0.249156       NaN\n",
       "19   0.933385  0.251037       NaN\n",
       "20   0.907024  0.234401       NaN\n",
       "21   0.902008  0.232047       NaN\n",
       "22   0.898273  0.229733       NaN\n",
       "23   0.888356  0.227433       NaN\n",
       "24   0.896237  0.239017       NaN\n",
       "25   0.879443  0.236926       NaN\n",
       "26   0.876039  0.234282       NaN\n",
       "27   0.875515  0.232361       NaN\n",
       "28   0.871059  0.230103       NaN\n",
       "29   0.871220  0.226320       NaN\n",
       "..        ...       ...       ...\n",
       "470  0.086808       NaN       NaN\n",
       "471  0.086376       NaN       NaN\n",
       "472  0.085947       NaN       NaN\n",
       "473  0.086380       NaN       NaN\n",
       "474  0.089813  0.000358       NaN\n",
       "475  0.092922 -0.000051       NaN\n",
       "476  0.091235       NaN       NaN\n",
       "477  0.092203 -0.000051       NaN\n",
       "478  0.084281  0.000079       NaN\n",
       "479  0.083866  0.000099       NaN\n",
       "480  0.083452  0.000120       NaN\n",
       "481  0.114708  0.013247       NaN\n",
       "482  0.099097       NaN       NaN\n",
       "483  0.093780       NaN       NaN\n",
       "484  0.087742       NaN       NaN\n",
       "485  0.093034       NaN       NaN\n",
       "486  0.098583  0.025633       NaN\n",
       "487  0.092290       NaN       NaN\n",
       "488  0.086367       NaN       NaN\n",
       "489  0.092737       NaN       NaN\n",
       "490  0.097053  0.025522       NaN\n",
       "491  0.093289       NaN       NaN\n",
       "492  0.091609       NaN       NaN\n",
       "493  0.089682       NaN       NaN\n",
       "494  0.088273       NaN       NaN\n",
       "495  0.089833  0.020148       NaN\n",
       "496  0.090194       NaN       NaN\n",
       "497  0.131066       NaN       NaN\n",
       "498  0.106752       NaN       NaN\n",
       "499  0.103360       NaN       NaN\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_height_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>110.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>110.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>111.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>113.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>112.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>113.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>109.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>110.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>109.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>109.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>109.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>110.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>111.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>114.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>114.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>114.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>114.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>114.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>114.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>114.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>114.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>114.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>114.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>145.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>135.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>135.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>150.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>150.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>150.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>92.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>110.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>117.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>117.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>124.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>225.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2\n",
       "0    110.0  225.0    NaN\n",
       "1    110.0  366.0    NaN\n",
       "2    111.0  225.0    NaN\n",
       "3    111.0  225.0    NaN\n",
       "4    111.0  411.0  230.0\n",
       "5    110.0  232.0    NaN\n",
       "6    110.0  230.0    NaN\n",
       "7    112.0  233.0    NaN\n",
       "8    111.0  232.0    NaN\n",
       "9    113.0  402.0  221.0\n",
       "10   112.0  404.0  222.0\n",
       "11   112.0  402.0  222.0\n",
       "12   113.0  402.0  222.0\n",
       "13   109.0  353.0    NaN\n",
       "14   110.0  232.0    NaN\n",
       "15   109.0  231.0    NaN\n",
       "16   109.0  231.0    NaN\n",
       "17   109.0  231.0    NaN\n",
       "18   110.0  233.0    NaN\n",
       "19   111.0  226.0    NaN\n",
       "20   114.0  254.0    NaN\n",
       "21   114.0  254.0    NaN\n",
       "22   114.0  254.0    NaN\n",
       "23   114.0  353.0    NaN\n",
       "24   114.0  345.0    NaN\n",
       "25   114.0  224.0    NaN\n",
       "26   114.0  225.0    NaN\n",
       "27   114.0  341.0    NaN\n",
       "28   114.0  224.0    NaN\n",
       "29   114.0  226.0    NaN\n",
       "..     ...    ...    ...\n",
       "470  151.0    NaN    NaN\n",
       "471  151.0    NaN    NaN\n",
       "472  151.0    NaN    NaN\n",
       "473  148.0    NaN    NaN\n",
       "474  145.0   27.0    NaN\n",
       "475  135.0    1.0    NaN\n",
       "476  135.0    NaN    NaN\n",
       "477  135.0  160.0    NaN\n",
       "478  150.0   33.0    NaN\n",
       "479  150.0   42.0    NaN\n",
       "480  150.0   42.0    NaN\n",
       "481   92.0  147.0    NaN\n",
       "482  110.0    NaN    NaN\n",
       "483  129.0    NaN    NaN\n",
       "484  135.0    NaN    NaN\n",
       "485  129.0    NaN    NaN\n",
       "486  117.0  100.0    NaN\n",
       "487  129.0    NaN    NaN\n",
       "488  135.0    NaN    NaN\n",
       "489  125.0    NaN    NaN\n",
       "490  117.0  100.0    NaN\n",
       "491  124.0    NaN    NaN\n",
       "492  125.0    NaN    NaN\n",
       "493  127.0    NaN    NaN\n",
       "494  127.0    NaN    NaN\n",
       "495  225.0  366.0    NaN\n",
       "496  127.0    NaN    NaN\n",
       "497   94.0    NaN    NaN\n",
       "498   93.0    NaN    NaN\n",
       "499   95.0    NaN    NaN\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_fwhm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>394.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>195.0</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>390.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56</td>\n",
       "      <td>211.0</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>213.0</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>212.0</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55</td>\n",
       "      <td>212.0</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55</td>\n",
       "      <td>388.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62</td>\n",
       "      <td>390.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>57</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>57</td>\n",
       "      <td>385.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>63</td>\n",
       "      <td>384.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>51</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>63</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>63</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>52</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>53</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52</td>\n",
       "      <td>389.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>148</td>\n",
       "      <td>476.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>159</td>\n",
       "      <td>480.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>159</td>\n",
       "      <td>480.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>132</td>\n",
       "      <td>452.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>132</td>\n",
       "      <td>451.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>132</td>\n",
       "      <td>449.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>175</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>157</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>158</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>161</td>\n",
       "      <td>614.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2\n",
       "0     53  394.0    NaN\n",
       "1     54  394.0    NaN\n",
       "2     54  394.0    NaN\n",
       "3     54  394.0    NaN\n",
       "4     52  195.0  393.0\n",
       "5     55  389.0    NaN\n",
       "6     56  390.0    NaN\n",
       "7     60  389.0    NaN\n",
       "8     56  389.0    NaN\n",
       "9     56  211.0  389.0\n",
       "10    55  213.0  389.0\n",
       "11    55  212.0  389.0\n",
       "12    55  212.0  389.0\n",
       "13    59  389.0    NaN\n",
       "14    55  388.0    NaN\n",
       "15    59  389.0    NaN\n",
       "16    59  389.0    NaN\n",
       "17    59  389.0    NaN\n",
       "18    60  389.0    NaN\n",
       "19    62  390.0    NaN\n",
       "20    57  385.0    NaN\n",
       "21    57  385.0    NaN\n",
       "22    57  385.0    NaN\n",
       "23    63  384.0    NaN\n",
       "24    51  389.0    NaN\n",
       "25    63  389.0    NaN\n",
       "26    63  389.0    NaN\n",
       "27    52  389.0    NaN\n",
       "28    53  389.0    NaN\n",
       "29    52  389.0    NaN\n",
       "..   ...    ...    ...\n",
       "470  128    NaN    NaN\n",
       "471  128    NaN    NaN\n",
       "472  128    NaN    NaN\n",
       "473  127    NaN    NaN\n",
       "474  148  476.0    NaN\n",
       "475  159  480.0    NaN\n",
       "476  158    NaN    NaN\n",
       "477  159  480.0    NaN\n",
       "478  132  452.0    NaN\n",
       "479  132  451.0    NaN\n",
       "480  132  449.0    NaN\n",
       "481  175  230.0    NaN\n",
       "482  207    NaN    NaN\n",
       "483  159    NaN    NaN\n",
       "484  160    NaN    NaN\n",
       "485  159    NaN    NaN\n",
       "486  157  309.0    NaN\n",
       "487  159    NaN    NaN\n",
       "488  160    NaN    NaN\n",
       "489  159    NaN    NaN\n",
       "490  158  309.0    NaN\n",
       "491  159    NaN    NaN\n",
       "492  159    NaN    NaN\n",
       "493  159    NaN    NaN\n",
       "494  161    NaN    NaN\n",
       "495  161  614.0    NaN\n",
       "496  159    NaN    NaN\n",
       "497  210    NaN    NaN\n",
       "498  208    NaN    NaN\n",
       "499  208    NaN    NaN\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_idx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.special import gamma\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_idx_df.loc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_comp = np.array([[peak_idx_df.loc[i][0], peak_height_df.loc[i][0], peak_fwhm_df.loc[i][0], i] for i in range(500)])\n",
    "sec_comp = np.append(first_comp, [[peak_idx_df.loc[i][1], peak_height_df.loc[i][1], peak_fwhm_df.loc[i][1], i] for i in range(500)], axis=0)\n",
    "all_points = np.append(sec_comp, [[peak_idx_df.loc[i][2], peak_height_df.loc[i][2], peak_fwhm_df.loc[i][2], i] for i in range(500)], axis=0)\n",
    "np.shape(all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>912.000000</td>\n",
       "      <td>912.000000</td>\n",
       "      <td>912.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>217.962719</td>\n",
       "      <td>0.238122</td>\n",
       "      <td>165.328947</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>143.098627</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>77.781769</td>\n",
       "      <td>144.385415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>91.750000</td>\n",
       "      <td>0.061219</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>124.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>356.250000</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>374.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>654.000000</td>\n",
       "      <td>1.001380</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Position      Height       Width         Time\n",
       "count  912.000000  912.000000  912.000000  1500.000000\n",
       "mean   217.962719    0.238122  165.328947   249.500000\n",
       "std    143.098627    0.245597   77.781769   144.385415\n",
       "min     51.000000   -0.000840    1.000000     0.000000\n",
       "25%     91.750000    0.061219  117.000000   124.750000\n",
       "50%    154.000000    0.145488  129.000000   249.500000\n",
       "75%    356.250000    0.298507  212.000000   374.250000\n",
       "max    654.000000    1.001380  422.000000   499.000000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_points_df = pd.DataFrame(all_points, columns=['Position', 'Height', 'Width', 'Time'])\n",
    "all_points_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.001380</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.996455</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.991564</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.986697</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.991028</td>\n",
       "      <td>111.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.976676</td>\n",
       "      <td>110.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>110.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.951091</td>\n",
       "      <td>112.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.959928</td>\n",
       "      <td>111.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.949348</td>\n",
       "      <td>113.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.952395</td>\n",
       "      <td>112.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.947682</td>\n",
       "      <td>112.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.942981</td>\n",
       "      <td>113.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>109.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.960959</td>\n",
       "      <td>110.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.964248</td>\n",
       "      <td>109.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.960055</td>\n",
       "      <td>109.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.955871</td>\n",
       "      <td>109.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.935662</td>\n",
       "      <td>110.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.933385</td>\n",
       "      <td>111.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.907024</td>\n",
       "      <td>114.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.902008</td>\n",
       "      <td>114.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.898273</td>\n",
       "      <td>114.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>114.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>51.0</td>\n",
       "      <td>0.896237</td>\n",
       "      <td>114.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.879443</td>\n",
       "      <td>114.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.876039</td>\n",
       "      <td>114.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.875515</td>\n",
       "      <td>114.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.871059</td>\n",
       "      <td>114.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.871220</td>\n",
       "      <td>114.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>474.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>476.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>479.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>487.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>491.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Position    Height  Width   Time\n",
       "0         53.0  1.001380  110.0    0.0\n",
       "1         54.0  0.996455  110.0    1.0\n",
       "2         54.0  0.991564  111.0    2.0\n",
       "3         54.0  0.986697  111.0    3.0\n",
       "4         52.0  0.991028  111.0    4.0\n",
       "5         55.0  0.976676  110.0    5.0\n",
       "6         56.0  0.973991  110.0    6.0\n",
       "7         60.0  0.951091  112.0    7.0\n",
       "8         56.0  0.959928  111.0    8.0\n",
       "9         56.0  0.949348  113.0    9.0\n",
       "10        55.0  0.952395  112.0   10.0\n",
       "11        55.0  0.947682  112.0   11.0\n",
       "12        55.0  0.942981  113.0   12.0\n",
       "13        59.0  0.972625  109.0   13.0\n",
       "14        55.0  0.960959  110.0   14.0\n",
       "15        59.0  0.964248  109.0   15.0\n",
       "16        59.0  0.960055  109.0   16.0\n",
       "17        59.0  0.955871  109.0   17.0\n",
       "18        60.0  0.935662  110.0   18.0\n",
       "19        62.0  0.933385  111.0   19.0\n",
       "20        57.0  0.907024  114.0   20.0\n",
       "21        57.0  0.902008  114.0   21.0\n",
       "22        57.0  0.898273  114.0   22.0\n",
       "23        63.0  0.888356  114.0   23.0\n",
       "24        51.0  0.896237  114.0   24.0\n",
       "25        63.0  0.879443  114.0   25.0\n",
       "26        63.0  0.876039  114.0   26.0\n",
       "27        52.0  0.875515  114.0   27.0\n",
       "28        53.0  0.871059  114.0   28.0\n",
       "29        52.0  0.871220  114.0   29.0\n",
       "...        ...       ...    ...    ...\n",
       "1470       NaN       NaN    NaN  470.0\n",
       "1471       NaN       NaN    NaN  471.0\n",
       "1472       NaN       NaN    NaN  472.0\n",
       "1473       NaN       NaN    NaN  473.0\n",
       "1474       NaN       NaN    NaN  474.0\n",
       "1475       NaN       NaN    NaN  475.0\n",
       "1476       NaN       NaN    NaN  476.0\n",
       "1477       NaN       NaN    NaN  477.0\n",
       "1478       NaN       NaN    NaN  478.0\n",
       "1479       NaN       NaN    NaN  479.0\n",
       "1480       NaN       NaN    NaN  480.0\n",
       "1481       NaN       NaN    NaN  481.0\n",
       "1482       NaN       NaN    NaN  482.0\n",
       "1483       NaN       NaN    NaN  483.0\n",
       "1484       NaN       NaN    NaN  484.0\n",
       "1485       NaN       NaN    NaN  485.0\n",
       "1486       NaN       NaN    NaN  486.0\n",
       "1487       NaN       NaN    NaN  487.0\n",
       "1488       NaN       NaN    NaN  488.0\n",
       "1489       NaN       NaN    NaN  489.0\n",
       "1490       NaN       NaN    NaN  490.0\n",
       "1491       NaN       NaN    NaN  491.0\n",
       "1492       NaN       NaN    NaN  492.0\n",
       "1493       NaN       NaN    NaN  493.0\n",
       "1494       NaN       NaN    NaN  494.0\n",
       "1495       NaN       NaN    NaN  495.0\n",
       "1496       NaN       NaN    NaN  496.0\n",
       "1497       NaN       NaN    NaN  497.0\n",
       "1498       NaN       NaN    NaN  498.0\n",
       "1499       NaN       NaN    NaN  499.0\n",
       "\n",
       "[1500 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrected_output = all_points_df.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>912.000000</td>\n",
       "      <td>912.000000</td>\n",
       "      <td>912.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>217.962719</td>\n",
       "      <td>0.238122</td>\n",
       "      <td>165.328947</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>143.098627</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>77.781769</td>\n",
       "      <td>144.385415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>91.750000</td>\n",
       "      <td>0.061219</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>124.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>356.250000</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>374.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>654.000000</td>\n",
       "      <td>1.001380</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Position      Height       Width         Time\n",
       "count  912.000000  912.000000  912.000000  1500.000000\n",
       "mean   217.962719    0.238122  165.328947   249.500000\n",
       "std    143.098627    0.245597   77.781769   144.385415\n",
       "min     51.000000   -0.000840    1.000000     0.000000\n",
       "25%     91.750000    0.061219  117.000000   124.750000\n",
       "50%    154.000000    0.145488  129.000000   249.500000\n",
       "75%    356.250000    0.298507  212.000000   374.250000\n",
       "max    654.000000    1.001380  422.000000   499.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_points_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>132.521333</td>\n",
       "      <td>0.144778</td>\n",
       "      <td>100.520000</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>154.191936</td>\n",
       "      <td>0.224010</td>\n",
       "      <td>100.974063</td>\n",
       "      <td>144.385415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.500000</td>\n",
       "      <td>0.043407</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>211.250000</td>\n",
       "      <td>0.189934</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>374.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>654.000000</td>\n",
       "      <td>1.001380</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Position       Height        Width         Time\n",
       "count  1500.000000  1500.000000  1500.000000  1500.000000\n",
       "mean    132.521333     0.144778   100.520000   249.500000\n",
       "std     154.191936     0.224010   100.974063   144.385415\n",
       "min       0.000000    -0.000840     0.000000     0.000000\n",
       "25%       0.000000     0.000000     0.000000   124.750000\n",
       "50%      76.500000     0.043407   114.000000   249.500000\n",
       "75%     211.250000     0.189934   135.000000   374.250000\n",
       "max     654.000000     1.001380   422.000000   499.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.001380</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.996455</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.991564</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.986697</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.991028</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.976676</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.951091</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.959928</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.949348</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.952395</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.947682</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.942981</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.960959</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.964248</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.960055</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.955871</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.935662</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.933385</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.907024</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.902008</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.898273</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.888356</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>51.0</td>\n",
       "      <td>0.896237</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.879443</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.876039</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.875515</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.871059</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.871220</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Position    Height  Width\n",
       "0         53.0  1.001380  110.0\n",
       "1         54.0  0.996455  110.0\n",
       "2         54.0  0.991564  111.0\n",
       "3         54.0  0.986697  111.0\n",
       "4         52.0  0.991028  111.0\n",
       "5         55.0  0.976676  110.0\n",
       "6         56.0  0.973991  110.0\n",
       "7         60.0  0.951091  112.0\n",
       "8         56.0  0.959928  111.0\n",
       "9         56.0  0.949348  113.0\n",
       "10        55.0  0.952395  112.0\n",
       "11        55.0  0.947682  112.0\n",
       "12        55.0  0.942981  113.0\n",
       "13        59.0  0.972625  109.0\n",
       "14        55.0  0.960959  110.0\n",
       "15        59.0  0.964248  109.0\n",
       "16        59.0  0.960055  109.0\n",
       "17        59.0  0.955871  109.0\n",
       "18        60.0  0.935662  110.0\n",
       "19        62.0  0.933385  111.0\n",
       "20        57.0  0.907024  114.0\n",
       "21        57.0  0.902008  114.0\n",
       "22        57.0  0.898273  114.0\n",
       "23        63.0  0.888356  114.0\n",
       "24        51.0  0.896237  114.0\n",
       "25        63.0  0.879443  114.0\n",
       "26        63.0  0.876039  114.0\n",
       "27        52.0  0.875515  114.0\n",
       "28        53.0  0.871059  114.0\n",
       "29        52.0  0.871220  114.0\n",
       "...        ...       ...    ...\n",
       "1470       0.0  0.000000    0.0\n",
       "1471       0.0  0.000000    0.0\n",
       "1472       0.0  0.000000    0.0\n",
       "1473       0.0  0.000000    0.0\n",
       "1474       0.0  0.000000    0.0\n",
       "1475       0.0  0.000000    0.0\n",
       "1476       0.0  0.000000    0.0\n",
       "1477       0.0  0.000000    0.0\n",
       "1478       0.0  0.000000    0.0\n",
       "1479       0.0  0.000000    0.0\n",
       "1480       0.0  0.000000    0.0\n",
       "1481       0.0  0.000000    0.0\n",
       "1482       0.0  0.000000    0.0\n",
       "1483       0.0  0.000000    0.0\n",
       "1484       0.0  0.000000    0.0\n",
       "1485       0.0  0.000000    0.0\n",
       "1486       0.0  0.000000    0.0\n",
       "1487       0.0  0.000000    0.0\n",
       "1488       0.0  0.000000    0.0\n",
       "1489       0.0  0.000000    0.0\n",
       "1490       0.0  0.000000    0.0\n",
       "1491       0.0  0.000000    0.0\n",
       "1492       0.0  0.000000    0.0\n",
       "1493       0.0  0.000000    0.0\n",
       "1494       0.0  0.000000    0.0\n",
       "1495       0.0  0.000000    0.0\n",
       "1496       0.0  0.000000    0.0\n",
       "1497       0.0  0.000000    0.0\n",
       "1498       0.0  0.000000    0.0\n",
       "1499       0.0  0.000000    0.0\n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_output.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = KMeans(n_clusters=3).fit(corrected_output.iloc[:,:-1])\n",
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf399f0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fignum = 1\n",
    "fig = plt.figure(fignum, figsize=(7, 6))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=50, azim=123)\n",
    "cluster\n",
    "labels = cluster.labels_\n",
    "\n",
    "ax.scatter(all_points[:, 0], all_points[:, 1], all_points[:, 2],\n",
    "             c=labels.astype(np.float))\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Position?')\n",
    "ax.set_ylabel('Height?')\n",
    "ax.set_zlabel('Width?')\n",
    "ax.dist = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_list = []\n",
    "peak2_list = []\n",
    "peak3_list = []\n",
    "\n",
    "for i in range(1500):\n",
    "    peak = cluster.predict([corrected_output.iloc[i,:-1]])\n",
    "    signal = corrected_output.iloc[i][1]\n",
    "    if ( peak == 0 and (signal >= 0.001 or signal <= -0.001)):\n",
    "        peak1_list.append(corrected_output.iloc[i])\n",
    "    elif ( peak == 1 and (signal >= 0.001 or signal <= -0.001)):\n",
    "        peak2_list.append(corrected_output.iloc[i])\n",
    "    elif ( peak == 2 and (signal >= 0.001 or signal <= -0.001)):\n",
    "        peak3_list.append(corrected_output.iloc[i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_unfilt = pd.DataFrame(peak1_list, columns=['Position', 'Height', 'Width', 'Time'])\n",
    "peak1 = peak1_unfilt.drop_duplicates(subset='Time')\n",
    "\n",
    "peak2_unfilt = pd.DataFrame(peak2_list, columns=['Position', 'Height', 'Width', 'Time'])\n",
    "peak2 = peak2_unfilt.drop_duplicates(subset='Time')\n",
    "\n",
    "peak3_unfilt = pd.DataFrame(peak3_list, columns=['Position', 'Height', 'Width', 'Time'])\n",
    "peak3 = peak3_unfilt.drop_duplicates(subset='Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>357.896774</td>\n",
       "      <td>0.087878</td>\n",
       "      <td>236.990323</td>\n",
       "      <td>169.841935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>68.512695</td>\n",
       "      <td>0.070756</td>\n",
       "      <td>85.969043</td>\n",
       "      <td>111.604962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>195.000000</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>348.750000</td>\n",
       "      <td>0.033202</td>\n",
       "      <td>168.250000</td>\n",
       "      <td>77.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>362.500000</td>\n",
       "      <td>0.061039</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>154.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>377.000000</td>\n",
       "      <td>0.118023</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>256.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>654.000000</td>\n",
       "      <td>0.302165</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Position      Height       Width        Time\n",
       "count  310.000000  310.000000  310.000000  310.000000\n",
       "mean   357.896774    0.087878  236.990323  169.841935\n",
       "std     68.512695    0.070756   85.969043  111.604962\n",
       "min    195.000000    0.004731   19.000000    0.000000\n",
       "25%    348.750000    0.033202  168.250000   77.250000\n",
       "50%    362.500000    0.061039  232.000000  154.500000\n",
       "75%    377.000000    0.118023  291.000000  256.750000\n",
       "max    654.000000    0.302165  422.000000  495.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Position  Height  Width  Time\n",
       "count          0       0      0     0\n",
       "unique         0       0      0     0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>101.100000</td>\n",
       "      <td>0.370619</td>\n",
       "      <td>122.620000</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>32.284277</td>\n",
       "      <td>0.258692</td>\n",
       "      <td>13.405185</td>\n",
       "      <td>144.481833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.083452</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.151349</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>124.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.274295</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>249.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.557007</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>374.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>210.000000</td>\n",
       "      <td>1.001380</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Position      Height       Width        Time\n",
       "count  500.000000  500.000000  500.000000  500.000000\n",
       "mean   101.100000    0.370619  122.620000  249.500000\n",
       "std     32.284277    0.258692   13.405185  144.481833\n",
       "min     51.000000    0.083452   76.000000    0.000000\n",
       "25%     74.000000    0.151349  115.000000  124.750000\n",
       "50%     96.000000    0.274295  121.000000  249.500000\n",
       "75%    122.000000    0.557007  129.000000  374.250000\n",
       "max    210.000000    1.001380  225.000000  499.000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf2004978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(peak1['Time'], peak1['Height'], 'o', markersize = 2, label = 'Peak 1')\n",
    "plt.plot(peak3['Time'], peak3['Height'], 'o', markersize = 2, label = 'Peak 3')\n",
    "plt.title('Kinetics of Identified Peaks')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf1fd0320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(peak1['Time'], peak1['Position'], 'o', markersize = 2, label = 'Peak 1')\n",
    "plt.plot(peak3['Time'], peak3['Position'], 'o', markersize = 2, label = 'Peak 3')\n",
    "plt.title('Shift of Identified Peaks')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Wavelength $(nm)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleexpfunc(t, params):\n",
    "    exp_array = params[0] *np.exp((-1.0/params[1]) * t)\n",
    "\n",
    "    return exp_array\n",
    "\n",
    "def fit_single_exp_diffev(t, data, bounds):\n",
    "    time_array = t\n",
    "    data_array = data\n",
    "    def fit(params):\n",
    "        decaymodel = singleexpfunc(time_array, params[:])\n",
    "        cost = np.sum(((data_array - decaymodel) ** 2.0))\n",
    "        return cost\n",
    "    bestfit = differential_evolution(fit, bounds = bounds, polish = True)\n",
    "    bestfit_params = bestfit.x\n",
    "    def bestfit_decay(params):\n",
    "        decaymodel = singleexpfunc(time_array, params[:])\n",
    "        return decaymodel    \n",
    "    bestfit_model = bestfit_decay(bestfit_params)   \n",
    "    \n",
    "    ss_res = np.sum((data_array - bestfit_model) ** 2.0)\n",
    "    ss_tot = np.sum((data_array - np.mean(data_array)) ** 2.0)\n",
    "    rsquare = 1 - (ss_res / ss_tot)\n",
    "    #print '--Single exponential best fit parameters--'\n",
    "    print ('a = %.5f  \\ntau = %.5f ps  \\nR-square = %.5f' %(bestfit_params[0], bestfit_params[1], rsquare))\n",
    "    plt.figure()\n",
    "    plt.ylabel('-$\\Delta$T/T')   \n",
    "    plt.xlabel('Time (ps)')\n",
    "\n",
    "    plt.plot(time_array, data_array, 'o', color = 'b', label = 'Data')\n",
    "    plt.plot(time_array, bestfit_model, color = 'r', label = 'Monoexponential')\n",
    "#    plt.text(10, 0.002, 'tau = 3ps', fontsize = 14)\n",
    "\n",
    "    plt.legend(loc = 'best')\n",
    "\n",
    "    plt.figure()\n",
    "    #plt.xlim(0, 200)\n",
    "    plt.ylabel('-$\\Delta$T/T')   \n",
    "    plt.xlabel('Time (ps)')\n",
    "    plt.xscale('log')    \n",
    "    plt.plot(time_array, data_array, 'o', color = 'b', label = 'Data')\n",
    "    plt.plot(time_array, bestfit_model, color = 'r', label = 'single exp fit')\n",
    "    plt.legend(loc = 'best')\n",
    "\n",
    "    return bestfit_params, bestfit_model, data_array, time_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 0.25488  \n",
      "tau = 115.31177 ps  \n",
      "R-square = 0.85685\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xl4lNX1wPHvSQgBAiIEcEMSVFyQHUStoiioyKJW8Vc0IIoVRVHU2rpQRVG0rogKIq4UUvdaEbSKqFVcCSjKIoIQNkUCyB4gIef3x50kk2RmMpNk1pzP87xP8t53mfsOYc7cXVQVY4wxJlhJ0c6AMcaY+GKBwxhjTEgscBhjjAmJBQ5jjDEhscBhjDEmJBY4jDHGhMQChzHGmJBY4DDGGBMSCxzGGGNCUifaGQiHZs2aaWZmZrSzYYwxcWX+/PmbVLV5ZeclZODIzMwkJycn2tkwxpi4IiKrgznPqqqMMcaExAKHMcaYkFjgMMYYE5KEbOMwxlRdQUEB69atY8+ePdHOigmTevXq0bJlS1JSUqp0vQUOY0wZ69ato1GjRmRmZiIi0c6OqWGqyubNm1m3bh2tW7eu0j2sqspLdjZkZkJSkvuZnR3tHBkTeXv27CE9Pd2CRoISEdLT06tVooxY4BCRPiKyTERWiMhtPo5fIyI/iMh3IjJXRNp6Hbvdc90yETknHPnLzobhw2H1alB1P4cPt+BhaicLGomtuv++EQkcIpIMTATOBdoCl3gHBo9/qWp7Ve0EPAQ85rm2LTAIOB7oA0zy3K9GjR4Nu3eXTdu926UbY4wpFakSR3dghaquVNV9wCvA+d4nqOp2r900oHgx9POBV1R1r6quAlZ47lejVvsZ9uIv3RgTPsnJyXTq1Injjz+ejh078uijj1JUVBTwmtzcXP71r39FKIe1W6QCx2HAWq/9dZ60MkTkOhH5GVfiuCGUa6sr2U8ZxkrsxgQWjrbB+vXr891337F48WJmz57Ne++9xz333BPwGgsckRNTjeOqOlFVjwRuBf4eyrUiMlxEckQkJy8vL+TX3r/fX56sncMYfyLRNtiiRQumTJnCU089haqSm5tLjx496NKlC126dOGLL74A4LbbbuOzzz6jU6dOjB8/3u95pgaoatg34GTgfa/924HbA5yfBGzzdS7wPnByoNfr2rWrhiojQ9X96Vfc0tNDvp0xcWvJkiVBn+vv/01GRvXykJaWViGtcePGumHDBt21a5fm5+erqupPP/2kxf/fP/74Y+3Xr1/J+f7OM46vf2cgR4P4TI9UiWMe0EZEWotIXVxj9wzvE0SkjdduP2C55/cZwCARSRWR1kAb4JuazuC4cf6Pbd5spQ5jfFmzJrT0mlBQUMBVV11F+/btufjii1myZEm1zjOhi0jgUNVCYCSutLAUeE1VF4vIWBE5z3PaSBFZLCLfATcDQz3XLgZeA5YA/wWuU1U/FUtVl5UF6en+j1vvKmMqatUqtPSqWrlyJcnJybRo0YLx48dz0EEHsXDhQnJycti3b5/Pa4I9z4QuYiPHVfVd4N1yaXd5/T4qwLXjgABlgpoxYQIMHuz7WDi/QRkTr8aNc20a3l3ZGzQIXIIPVV5eHtdccw0jR45ERNi2bRstW7YkKSmJqVOnst/TQNmoUSN27NhRcp2/80z1xVTjeLQFKnXU9DcoYxJBVhZMmQIZGa4HYkaG28/Kqt598/PzS7rj9u7dm7PPPpsxY8YAcO211zJ16lQ6duzIjz/+SFpaGgAdOnQgOTmZjh07Mn78eL/nmeoT1x6SWLp166ZVXcipuJdI+W9QNfGfwZh4sHTpUo477rhoZ8OEma9/ZxGZr6rdKrvWShzlFH+D8i551K8fvfwYY0ysscDhx3avceybN8MVV1jPKmOMAQscPo0aBQUFZdMKCly6McbUdhY4fNi8ObR0Y4ypTSxwGGOMCYkFDh8CDQS0dg5jTG1ngcOHCRP8H7N2DmPCT0QY7DUat7CwkObNm9O/f/8o5ir67r///jL7f/jDHyq9pmHDhjWeDwscPgQar2HzVhkTfmlpaSxatIj8/HwAZs+ezWGH1fhqCnGnfOCI1oy/Fjj8yMjwf8zmrTIm/Pr27cusWbMAePnll7nkkktKjm3ZsoULLriADh06cNJJJ/H9998DcPfddzNs2DB69uzJEUccwRNPPFFyzWOPPUa7du1o164djz/+eEn69OnT6d69O506deLqq69m//79rF69mjZt2rBp0yaKioro0aMHH3zwAbm5uRx77LFkZWVx3HHHMXDgQHZ7RgvPmTOHzp070759e4YNG8bevXsByMzMZMyYMXTp0oX27dvz448/ArBr1y6GDRtG9+7d6dy5M2+//TYAL730EhdeeCF9+vShTZs2/O1vfwPctPHFI+qzPN9ui0sTO3fupFevXiWvUXyvsAlmCt1422pi+uTp0/1Psy5S7dsbE7PKTLc9apTq6afX7DZqVKV5SEtL04ULF+pFF12k+fn52rFjxzLTpo8cOVLvvvtuVVWdM2eOduzYUVVVx4wZoyeffLLu2bNH8/LytGnTprpv3z7NycnRdu3a6c6dO3XHjh3atm1bXbBggS5ZskT79++v+/btU1XVESNG6NSpU1VV9dlnn9WBAwfqQw89pMOHD1dV1VWrVimgc+fOVVXVK664Qh9++GHNz8/Xli1b6rJly1RVdciQITp+/HhVVc3IyNAnnnhCVVUnTpyoV155paqq3n777Tpt2jRVVf3999+1TZs2unPnTn3xxRe1devWunXrVs3Pz9dWrVrpmjVrSt6X8u+TqmpBQYFu27ZNVVXz8vL0yCOP1KKiIp/XFIuHadXjjs1bZUx0dejQgdzcXF5++WX69u1b5tjcuXMZMmQIAGeeeSabN29mu2fUbr9+/UhNTaVZs2a0aNGC3377jblz5/LHP/6RtLQ0GjZsyIUXXshnn33GnDlzmD9/PieccAKdOnVizpw5rFy5EoA///nPbN++ncmTJ/PII4+UvPbhhx/OKaecAsDgwYOZO3cuy5Yto3Xr1hx99NEADB06lE8//bTkmgsvvBCArl27kpubC8AHH3zAP/7xDzp16kTPnj3Zs2cPazyzqfbq1YvGjRtTr1492rZty+pK1rBWVe644w46dOhA7969Wb9+Pb/99luV3vdgRGx23Hg0YUL4Z/40JqZ5VelEw3nnncctt9zCJ598wuYgB1KlpqaW/J6cnExhYaHfc1WVoUOH8sADD1Q4tnv3btatWwe4qqBGjRoBruHeW/n9QHnyzo+q8uabb3LMMceUOffrr78O6RkAsrOzycvLY/78+aSkpJCZmcmePXsqzVdVWYkjgHDN/GmMCc6wYcMYM2YM7du3L5Peo0cPsj29VD755BOaNWvGAQcc4Pc+PXr04D//+Q+7d+9m165dvPXWW/To0YNevXrxxhtvsHHjRsC1nRR/u7/11lvJyspi7NixXHXVVSX3WrNmDV9++SUA//rXvzj11FM55phjyM3NZcWKFQBMmzaN008/PeCznXPOOTz55JPFK5vy7bffVvp+pKSkUFB+WgvcFPItWrQgJSWFjz/+uNISSnVZiaMSWVkWKIyJlpYtW3LDDTdUSC9uBO/QoQMNGjRg6tSpAe/TpUsXLr/8crp37w64aqjOnTsDcN9993H22WdTVFRESkoKEydOJDc3l3nz5vH555+TnJzMm2++yYsvvsgZZ5zBMcccw8SJExk2bBht27ZlxIgR1KtXjxdffJGLL76YwsJCTjjhBK655pqAebrzzju58cYb6dChA0VFRbRu3ZqZM2cGvGb48OF06NCBLl26lAROgKysLAYMGED79u3p1q0bxx57bMD7VJdNqx6C7GzXo2rNGtfOMW6cBRWTeGxadf9yc3Pp378/ixYtinZWqq0606pbiSNI5dfpWL3a7YMFD2NM7WJtHEEaPbpsIzm4fRvTYUztkZmZmRCljeqywBEkf21NYW6DMiYqErEK25Sq7r+vBY4gJSf7Tg+iJ54xcaVevXps3rzZgkeCUlU2b95MvXr1qnwPa+MI0v79vtNV4dprYdKkyObHmHBp2bIl69atIy8vL9pZMWFSr149WrZsWeXrLXAEKSPDf7XU5MlwyinWSG4SQ0pKCq1bt452NkwMs6qq8r75BrZsqZAcaLS4qjWSG2Nqj4gFDhHpIyLLRGSFiNzm4/jNIrJERL4XkTkikuF1bL+IfOfZZoQtkytWwIknuuHh5QSauwqskdwYU3tEJHCISDIwETgXaAtcIiJty532LdBNVTsAbwAPeR3LV9VOnu28sGX0qKOgVy+YOBF8DOsPtMAT2DodxpjaIVIlju7AClVdqar7gFeA871PUNWPVbV4pMRXQNVbbqpj1ChYtw7eeqvCocraMGx1QGNMbRCpwHEYsNZrf50nzZ8rgfe89uuJSI6IfCUiF4QjgyX69YMjj/RbvAi0wFOQk3caY0xci7nGcREZDHQDHvZKzvDMn3Ip8LiIHOnjuuGe4JJTrW6ESUlw/fXwxRfgY74rm1LdGFPbRSpwrAcO99pv6UkrQ0R6A6OB81R1b3G6qq73/FwJfAJ0Ln+tqk5R1W6q2q158+bVy+0VV0CjRj5LHVlZkJbm+7JAjefGGJMoIhU45gFtRKS1iNQFBgFlekeJSGfgGVzQ2OiV3kREUj2/NwNOAZaENbcHHOCCx6uvwq+/Vjj8zDOQklI2LSWl8sZzY4xJBBEJHKpaCIwE3geWAq+p6mIRGSsixb2kHgYaAq+X63Z7HJAjIguBj4F/qGp4Awe46qrCQje6r5ysLHjxxbILPL34og0ANMbUDrYeRyADBrgBgWvWgNdSjuXZOh3GmEQQ7HocMdc4HlNuuAE2bnRVVn4Ur9OxerUbQb56NQwZ4uavMsaYRGSBI5DevaFtW3jsMRcVfPC1Toeqq+GyAYHGmERkgSMQEfjLX2DhQvjwwwqHs7P9TzWiagMCjTGJyQJHZbKy4JBD4KGHyiQXV1EFsnmzlTqMMYnHAkdlUlPhxhtdiWPBgpJkX1VUvlipwxiTaCxwBOPqq92AwEceKUlasya4S20aEmNMorHAEYzGjV3weO01WLUKcN1ug2XVVcaYRGKBI1ijRrl5rMaPB0Kbs8oWeTLGJBILHMFq2dI1lD/3HGzaFNIAP1vkyRiTSCxwhOKWWyA/HyZNAoKf1DA5OYx5MsaYCLPAEYrjj3frdTz5JOzaFfSkhvv3WzuHMSZxWOAI1e23w6ZNMGVKpeuQexs61IKHMSYxWOAI1SmnwBlnuAGB+flMmAANGlR+2f79NqbDGJMYLHBUxZ13woYN8PzzZGXBlCmlU6wHYmM6jDGJwAJHVfTs6UoeDz4Ie/eSlQW5uVBUFHhNcrBZc40x8c8CR1WIwF13wbp1MHVqmUOVje94+mlr6zDGxDcLHFV11lnQvTs88AAUFJQkZ2XBiBGBL7366jDnzRhjwsgCR1WJuLaO3NwKRYhJkwL3ttq1y0odxpj4ZYGjOvr1g86dXf1UYWGZQ5WN8bBpSIwx8coCR3WIwN//DitWwMsvlzmUlQUNG/q/NNjZdY0xJtZY4KiuCy6ATp3g7rvLtHWAWz7WHxGrrjLGxCcLHNWVlAT33QcrV8ILL5Q5FKihvKjIRpMbY+KTqGq081DjunXrpjk5OZF7QVU3rmPNGldtVa9emcPNmvkf/JeWBjt3RiCPxhhTCRGZr6rdKjvPShw1QcQ1kK9f7wZqlLNli/9LrYeVMSbeRCxwiEgfEVkmIitE5DYfx28WkSUi8r2IzBGRDK9jQ0VkuWcbGqk8h+SMM6BXLzeuo1wRorLVAq2HlTEmnkQkcIhIMjAROBdoC1wiIm3LnfYt0E1VOwBvAA95rm0KjAFOBLoDY0SkSSTyHbJx4yAvr0Jf3MpGk9tCT8aYeBKpEkd3YIWqrlTVfcArwPneJ6jqx6q627P7FdDS8/s5wGxV3aKqvwOzgT4RyndoTjwRBgyAhx+G338vSa5sNHllkyMaY0wsiVTgOAxY67W/zpPmz5XAe1W8NrruvRe2bXPTrnvxLBrok6q1cxhj4kfMNY6LyGCgG/BwiNcNF5EcEcnJy8sLT+aC0bEjXHopPP44rF1b5lCgmXOtncMYEy8iFTjWA4d77bf0pJUhIr2B0cB5qro3lGtVdYqqdlPVbs2bN6+xjFfJuHFuoMadd1ZI9sdGkhtj4kWkAsc8oI2ItBaRusAgYIb3CSLSGXgGFzQ2eh16HzhbRJp4GsXP9qTFrsxMuOEG+Oc/YeHCkuRAS81W1vPKGGNiRUQCh6oWAiNxH/hLgddUdbGIjBWR8zynPQw0BF4Xke9EZIbn2i3AvbjgMw8Y60mLbXfcAU2awC23kD1dycx0g8wB6tYte2qDBpX3vDLGmFhhI8fD6fHH4aabuCD1Pd7eW9oRLCUFDjjADQxs1coFjaysKObTGGMIfuS4BY5w2reP1WnHsb2wAZ34jiKSSw5lZLilPIwxJlbYlCOxoG5d/lr4D9qziKGUXWLWGsONMfHKAkeYfdNqIF9yEvfxdxqyoyTdGsONMfHKAkeYjbtfuD11PIfyK3dwP2CN4caY+GaBI8yysuCq50/ijbSh3MxjnH7ocqZMcceKe1plZtrIcWNM/LDAEQFZWTBw+QOkNkrlk843AXDFFW5yQ1X384orLHgYY+KDBY5IOeQQuOsumDWLmSNmlV9lloICGDUqOlkzxphQWOCIpBtugGOO4Z4dN1GXvRUO+1sl0BhjYokFjkiqWxcef5yjWc4oJlR+vjHGxCALHJHWpw/vpZzHndzLoeXmavQ3j5UxxsQSCxxRsO/B8dShkMe5sSStbt0KCwcaY0xMssARBeffdATLLr6Ti3mDfswiIwNeeMHmqzLGxAebqypa9u2Dzp1h1y5YvBjS0qKdI2NMLWdzVcW6unX54KJnYPVqHm54jw0CNMbEjUoDh4jUiURGapvsbPjjo6fyLH/mJh6j8eqFDBkC114b7ZwZY0xgwZQ4vgl7Lmqh0aNh9264lQfZQlOe4WpE9/P001byMMbEtmACh4Q9F7VQ8bTqv9OUm3mMk/iaa5gMwNVXRzFjxhhTiWCqoZqLyM3+DqrqYzWYn1qjVSs3RxVANllcxj95kFuZRT9W78okO9t6WRljYlMwJY5k3FrgjfxspgrKTqsuXMWzKMKzXAWozVtljIlZlXbHFZEFqtolQvmpEXHRHRdo1Ah27izdH84zPMM1XMUUnuMqErCntDEmhtVkd9x2NZAf48PkyWX3pzCcOZzJo/yFw1ljPayMMTEpmMAxVUQWiMgrInK5iBwc9lzVEllZ0LChd4rwZ54jiSKmMJynn1brYWWMiTmVBg5VvcpTVXU30AR4SUS+FJH7ReQ0EUkOdyYTWflSRy6tuZUH6cP7XMGL1sPKGBNzgh4AqKo/qup4Ve0DnAnMBS4Gvg5vFhNbVlbFWXGfZgSfcDqPcyPNduUGLHVkZ9sStMaYyKrSAEBVzVfVd1X1+mAaUgBEpI+ILBORFSJym4/jp3mqxApFZGC5Y/tF5DvPNiOY14sn5WfFVZK4nJcA+CeXMWL4fp/XZWfD8OFll6C10efGmHCLyABAT3XWROBcoC1wiYi0LXfaGuBy4F8+bpGvqp0823nVzU+sqdjWAavJ5Domchqfcd3uh+jdu+J1xaPPvalio8+NMWEVqQGA3YEVqroSQEReAc4HlnjdJ9dzrCiI+yWcyZNh8OCyadMZTD9mMZa7OGnO2fTu3ZUPPyw9Xjz63JdRo2wAoTEmPCI1APAwYK3X/jpPWrDqiUiOiHwlIheEcF3c8FXqAGEET7OBg8kmiy/m7C5TDdWqlf/72frlxphwCabE8auqjg17TgLLUNX1InIE8JGI/KCqP3ufICLDgeEArQJ9osYwX6WOrTRhKFP5iF48wi1c9/QkTjnFBZq+fV21lDHGRFKkJjlcDxzutd/SkxYUVV3v+bkS+ATo7OOcKaraTVW7NW/evHq5jZKsLBgxomL6x5zJI/yFa3ma8/lPSRfdd9/1fy9bv9wYEy7BBI5evhJF5FQRmRjk68wD2ohIaxGpCwwCguodJSJNRCTV83sz4BS82kYSzaRJ0MvHOz6aceTQlRe5oqSLbqA2Dlu/3BgTLsEMANxS/LuIdBaRh0VkNfAoMNj/lWXuUQiMBN4HlgKvqepiERkrIud57n2CiKzDjQ15RkQWey4/DsgRkYXAx8A/VDVhAwfAhx9WDB77SOX/eI0kiniVPzFy+L6AbRyjR1vPKmNMeAQzyeHRwCXApcAO4HXcB/8qEVmlqq3Dn83QxMskh5UpPwkiwIW8yZsM5DFu4t1ej/HllxW75BYTgWuucaUYY4ypTE1Ocvgj0BcY6GlDeFBVV3mO2fytYVR+OhKAf3MRTzKSmxlP2py3GTrUf3uGqruHlTyMMTUpmMBxIbAK+EBEponIABFJCXO+DP666MItPMJ8uvASl/PJS7k+zymm6qqtjDGmpgTTxvEfVR0EHAW8h+vyuk5EXgQOCHP+aj1fpQ7v9o5p+Rfx2+r8gPcI1IhujDGhCqbEAYCq7lLVf6nqAKAT8CXwfdhyZgD/XXRXciRZZNOVBUzmGgLVGsbpsBZjTIwKOnCUM8szbuLMGs2N8WnSJN9VVrPozxjuZij/5Dp894wWKb9MrTHGVE9VA0dNDAo0IfBVZQVwL3cygwGM5yZO5bMyx4p7VdmcVcaYmlTVwPFsjebCVMpfQ7mSxBCmsZIjeJ2LOf7A9YhARgZMm2ZdcY0xNa9KgUNV7eMoCiZPhmQf6y1upzF/5C3S2MX03X+kaFc+ublW0jDGhEdVSxwmCrKyYOpUVwVV3lLaMpjpdNiXA5dfDkW1cnZ6Y0wEWOCIM1lZrgrKlxmczz8OfBBeew3uuSeyGTPG1BoWOOJQcRfd8iWPBg0g48lbYNgwGDvWhowbY8LCAkecmjTJlTwyMihpDJ8yBbIGi1uko2dPF0A+/zzaWTXGJBgLHHEsKwtyc11zRnFjeHY2ZB5dl/RP3mRlUQZ7zr0AVqyIdlaNMQnEAkcCyc6G4cNh9WrYQlPOKZzFjh2Q1/Uc2LAh2tkzxiQICxwJZPToslOsr6AN/ZhFg+0bWH50X9i+PXqZM8YkDAscCcTXZIbz6M5A3iBzxw981ORCXn5pb+QzZoxJKBY4Eoi/yQz/y7lcyfOcWTSHA6+4gDpSSGamdboyxlSNBY4EMm6c78GBANO4jLsZw7n8l0JSuHv15cwa9iavPrcjspk0xsS9SpeOjUeJsnRsVVx7reuN65ui5b4r7KUuqWf3hP79YcAAyMwMcw6NMbGqJpeONXFk0iTo1cvfUSGJ/WVSnuJ61w3rhhugdWto1w5uvx2++AL27/dzH2NMbWaBIwF9+KH/4KEkkUxhyf61TISlS+Gnn+Cxx6BFC3jkETjlFDj4YBg6FF5/3XpkGWNKWFVVAgtUbVWHAgqoC8DWlGYcuC+v9ODWrfD++/DOO/Duu/D775CSAqed5qqzBgyAI46IwBMYYyIp2KoqCxwJLlDwqMte9lIPgJ3NM2m4cVXFkwoL4csvYeZMF0iWLnXpxx3nAkj//nDyyVCnTpiewBgTKdbGYQDX5uFrzXKAfaRSHzdisGFeLhxxhJuyJBOSktzP7FfrQI8e8OCDsGSJm77k8cfh0ENd1dZpp8FBB8GQIfDqq7BtW6QezRgTJRELHCLSR0SWicgKEbnNx/HTRGSBiBSKyMByx4aKyHLPNjRSeU4UgYLHHuqTxk63s2oVPQa3YvVqUHVt5oMHuy6+JeM+jjwSRo1yDSmbNrkp3Pv1g/feg0GDoFkzOPNMGD/e5sgyJkFFpKpKRJKBn4CzgHXAPOASVV3idU4mcABwCzBDVd/wpDcFcoBugALzga6q+ru/17OqKt9694Y5c3wfq89udpMGwK8czKH8WuGcBg08M/D6Wllw/3746qvSKq3Fi136MceUVmmdcopVaRkTw2Ktqqo7sEJVV6rqPuAV4HzvE1Q1V1W/B8ovXXcOMFtVt3iCxWygTyQynWgC9bbKp0FJtdUhbKCAOrg4XWr3bjcflk/JyS4wPPAALFoEK1fCE0+44ewTJrhp3ps3h0svhZdfdg3uxpi4FKnAcRiw1mt/nSct3Neacj78ENLTfR/bQ31S2QNAHfajJFUY9+FrPiyfWreG66+HDz6AzZvhjTfgggtcBi691AWRnj3h0Udh2bKqP5AxJuISpnFcRIaLSI6I5OTl5VV+QS02YYL/qUn2kVpmnMd+6lCP/JJ9f/NhBdSoEVx0Ebz4Ivz6qxtceOutsGUL3HILHHssHH003HwzfPwxFBRU4UWMMZESqcCxHjjca7+lJ63GrlXVKaraTVW7NW/evMoZrQ2ysuCaa/wHjyKSEYrYQyrgqrGa4YLxuHHVfPHkZNd9d9w4+P57twLVU0+5RveJE13DevPmrqE9O9uVVowxMSVSgWMe0EZEWotIXWAQMCPIa98HzhaRJiLSBDjbk2aqofzSs+npULeu9xlCffaQQ1cA8mjBUSz33TBeHRkZcN11rlfW5s3w73+70sknn7guXS1auC6/Dz/sxpAk4LgjY+JNRAKHqhYCI3Ef+EuB11R1sYiMFZHzAETkBBFZB1wMPCMiiz3XbgHuxQWfecBYT5qpJu+lZydMcDVK5Z1ADtMYDMByjoaXXgpfhho2hD/+EZ5/Hn75Bb7+Gu64w0138re/Qdu20KYN3Hij6x62b1/48mKM8ctGjpuSJWe9Vw8s70bGM56bAfiizmmcWfgBB2ekMm6cn+65NW3tWtfVd+ZMFzT27oUDDoBzznHdfc89140hMcZUmU05YoEjaJmZbrBfecnJZSfI7UoOOZxQst+OH1jVoJ3/sR3hsmuXCx7vvOMCyYYNbqj7ySeXTg/ftq3/RhxjjE8WOCxwBC0pKfimg2bkkUeLkv2beIz/tBrFqtVR6qBXVAQLFpQGkQULXHrr1qVB5PTTyzfgGGN8iLUBgCaGhdLFdhPNSWUPs+gLwHhuZtqa0zip5brS+a0iuSRtUhJ06wb33APz58O6dTB5Mhx/PDz7LJx9tqvCGjgQpk60cMKsAAAdbElEQVQF66ptTLVZicP4bONo0CBwmwcof+MhHqR02rH/41Ve5/8Qcd19J00KW5aDs3s3fPRRaWnkl19c9dVJJ5WWRtq1syotYzysxGGClpXl5qAq7pqbkeH2k5MDXSU8xK2cz9slKa/xJ6ZyGY10G08/7ebGiqoGDVyAeOYZVxKZPx/GjHEDDEePhg4dXJXWyJFu/ZG9e6OcYWPig5U4jF/+1vJITXU9YVu1co3qbfiJGZzHsbipQ1bTiiFM4zNOo23b0vkOY8ovv7hFqt55B2bPhvx8SEtzVVv9+7sZfw86KNq5NCairMRhqq14Ovbikkdystvfs8e1SY8b50ooyzmarsznZQYBkMEaPuV0HuA2li/ZR/36EW73CMahh8Kf/wxvv+0GHs6a5dYUmTcPrrwSDjnEVWnddx8sXGgDD43xpqoJt3Xt2lVN+GVkqLpP1OKtSEcxvkziAjrpcSwuSUpPV50+Pdo5D6CoSPXbb1XvvVe1e/fSZzn8cNURI1TffVc1P9/v5dOnu/dFxP2M6Wc1phwgR4P4jLWqKlNl/rrx/oHP+ReXkoGbSncvdbmFR5jIdainkNuwoev8FNHxH1WxYUPZKq1du1zbyVlnlVZpHXII4EpVw4ZVHNA+YkQMdBQwJgjBVlVFvXQQjq0mSxzTp7tvyXHzjTmCKpY4SrcD2aJvcGGZxP9yth7C+pKkunXj7L3Mz1d97z3V665TbdWq9NlOOEH1nnu0Z+MFCkV+3xMrgZhYR5Aljqh/yIdjq6nAMX26akqK7w+BESNq5CXi2vTpqg0a+A8eUKRX87Tmk1qSuImmeiFvlPkwjUtFRaoLF6qOG6d60kmubgp0LYfp01yt/XhH67G7wnvSoIEFDxO7LHBUQ3E9tf8PRLfF8wdATdXFl79P27YV36d2fK+LKHvgBS7XRmxTkZp7pqj67Tcdyov6Bhfqdhqqgu6ivr7NAL2KZ8qUtOI2WJqEZ4Gjiir/Fq1x/wHg6xlr8pvwiBEV36tU8vUhbimTuJJMveigz2rmRWNAcZVmXfboWbyvE7heV5JZ8rw5dNExjNGu5Kju3x/t7BpTQbCBwxrHy/E34Z8vIq5barzx94wZGW6a9ZqQnQ2jRlVch+kU5vISl3MUP5cm3nGHG5gX5/NJZWfDZZeV/5tQjmcx/ZnJAN7hZL4kCXUN6v37u613b9fgbkyUWeN4FXmqqhO6xOHvGWu62shf6a0BO/W51GvLJnbporp0aUj3jsVur+U7U5TfWtXfqJ9fPVV14EDVRo1cYr16qn37qj79tOratdF+BFOLYVVVVRNM20Zc9gjy4u8ZqxoI/X2IV/ZeXtjoA93RPLNs4lNPuYbnSl4vnFVtNSlggNu7V3X2bNVRo1SPOKL0YTp1Ur3zTtWvv7YqLRNRFjiqyNeHUkqKalpa6X68d8mt6gevrw/BQPcKpvTWgJ06PuWvZRP79FH99Ve/+ajpwBcTiopUlyxRffBB1R49VJOS3EMdfLDqlVeqvvWW6s6d0c6lSXAWOKohVqtBalIozzh9etnAGWw1XrClN1DtyLeak9StTOIno/5d5nWTklzDe6Sq2qJq0ybVadNU//Qn1caN3QOmprqgOnGi6urV0c6hSUAWOExQytfJF3/R9S5R+BvLEmgTCa2HGqgmUag3yBNlEl/mT9qQ7UEHq2CeN+6+FOzbpzpnjupNN6kedVTpA3fooDp6tOqXX1qVlqkRFjhMpSoLCg0aBG7oDeZDPNgxMd7bIazXlxlUJvFkPg9c5RVkVVv5QCYSZ4M5i4pUf/xR9eGHVU8/XTU52T1IixaqV1yh+uabqtu3RzuXJk5Z4DCVCvUDPdjNV8eB6dNDv8+JfFkm4V36aB32+SzdBPPh7+95i0tHcWnzZtXsbNVLLlE98MDSf4Czz1Z98knVVauinUMTR4INHDaOoxYLZa3xUKSnw6ZNFdP9re8RiFDESJ7iCUaVpPVlFu95lq4t/7oTJvifODHQ89bkGJaoKShg9j1fsOrJdzh9+zscw08uvV07t9ph//5w4omVrdBlajEbx2EqFUyJIz099DaOQI3UVWloB9VGbNOlHFMm8e+M1XTy/Oa7fCki0PMmQsN6+aq4NizTW1Me1V/bnlFapdWsmepll6m+/rrqtm3RzrKJMVhVlalMZdVHxe0GoX7YB9tIXZUA0pn5FRI/5VQdwlRtxLYK53tXYQXqIhzXXXk9AnVTfvWZ3/W6Zq/oNLL096Qm7kBKimrv3qoTJqj+/HO0s29iQMwFDqAPsAxYAdzm43gq8Krn+NdApic9E8gHvvNskyt7LQscwevVy/eHja9v7JWNii6uXg+lvcDf61e2tWGZ/sRRFQ68yR91IK9pfXb5zZ+/ABnvAo2b8S6JJFOgvVM/1cX9/qp63HGlB9q2Vf3b31Q/+0y1sDDaj2OiIKYCB5AM/AwcAdQFFgJty51zbXFQAAYBr2pp4FgUyutZ4AhOdXsZ1dRaJb4mRQx268o8nUnfCgd2kKbTuVT78Y6msNfv9fE+mNObvxJHcS2V31LWihWq48e7KF6njjvYtKnq4MGqr76qunVrFJ/KRFKsBY6Tgfe99m8Hbi93zvvAyZ7f6wCbALHAET6xNgK7qqUPUO1Cjv6H88ok7sd9Bd/CgfosV2ovZmsyBQlZ2lD1P4rf33vms11n61bV117Tn08dopuT3LeCfdTRX9ue6YLL8uURfy4TObEWOAYCz3ntDwGeKnfOIqCl1/7PQDNP4NgFfAv8D+hR2etZ4AhOLI7Ark7pA1Q7sUD/zQWqoPmk6hKO1Y/oWbJGxgZa6BOM1D8wV4X9IQXKeBg86CuPgUoixeeNGFF6XvHfRRKF+gfm6gPcqovl+NILjz1W9ZZbVP/3P9WCgqg+r6lZiRQ4UoF0T1pXYC1wgI/XGA7kADmtWrUKx3uacGKtxOHN1wdgKIMRO/CdTudSLSBZ91FHX+ViHcMYfZ2LdDf1VEFzaaUP8lftwvyEmlixvFBH8PvbTj30Z9UnnlA966zSrnZNmqheeqnqyy+rbtkS7Uc11RRrgaPKVVU+7vUJ0C3Q61mJIzjx9mEYqPF3+nTfpZUMVunj3KA7cQ/6IWfqEKbqUF7Ud+in+3B1+stoo/dwpx7LEm3YMLSuvN5b8XxascY7EPtr86hsK1MS3b5d9Y03VIcOdV18i4swPXuqPvKI6rJlUXpSUx2xFjjqACuB1l6N48eXO+e6co3jr3l+bw4ke34/AlgPNA30ehY4ghcP1S/Fgikh+SuZNGWT3sb9uprDtbi0cSsP6LHyo/6ZKfohZ5a0iXxHB70j6X79z2OlXVRDWaeleIvFAKJatWcJWBItLFT94gvVO+5Qbd++9IKjj1a9+WbVjz92822ZmBdTgcPlh77AT54qqNGetLHAeZ7f6wGv47rjfgMc4Um/CFiM64q7ABhQ2WtZ4EhMNTEdfIumBTqk4b91Dmeo4tpBXuBy7UKOHswvej0T9HNOLnmBlS1O1DsajC+zZnhVv63HSiCpylQzIZVEV61y66qcc05p/+cDD1QdNMhNj7J5cxifzlRHzAWOSG4WOBJXjZaQFi3SiYzQHbiRiF9zgl7FM9qQ7ZrBKv0rD+oCOqniemh9RE8dzmS/o9WD2XxVg0VaqG0e1eqyvGOH6r//rctPH6Ybk1qoghaQrBuOPU31oYd0xkNLNaNVUVyUeGsDCxzGBCE5WbUxv+v1TNDvaaeKGwPyHMP0ND5RYb8ew1K9i7tLpjwpIFnfpY9exkt6AFvD++09TMoHYO9eVcVtIDX1QV4cqIT92p2vdCx/14XSseQN+Ymj9DFu1DOYowfU3xf196Y2CzZw2CSHplYrO/Gi0p1v+DPPMYhXaMROcslgOoOZxhB+4mg68D2X8DKDeIVMVrOHVN6lL68wiJn0J58GQb1uQkyqGKTMTFi9umJ6RtJazi2ayQDe4Uw+oh572cYBfNagD/0n94e+fd3MlSZigp3k0AKHqfWuvRYmT3Zff4s1YBcX8B+GMI2zmE0yRXzDCUxjCK8wiE0040S+5hJe5v94jUPYwE7SeJvzeYVBfMDZ7CPV72uKQFFRBB4uBgQzC3MDdtGbD+nPTPozk0PY4C78wx/crL4DBsBxx7k3zoSNzY5rTIj8LWx1ML/oTTxa0t5RQLJ+yJk6MnmSvvHUr6qFhTr7jo/0+TrDdRNNVXGj1Z9jmM/R6uV7KMVTz7aqCHUqlMxW+1W/+Ub1rrtUO3cuPXDEEao33KA6e7bq3r3RfqyEhLVxGBOa6dMrToKYnFx2Ft92fK/3Mlp/SvJM8S6i2qOHm2F27VrX7XTWLP3q6CG6jUaquNHqT3KdnsJnKuzXlBTX4CzifpZ/zVhoA6lJ/nrDjRhRMd37vSkJomvXqk6erNqvn2o9N3hTGzVSHThQ9aWXVDdujPYjJgwLHMaEKKSR9EVFqosWqd59d9mxCyed5AbALV+uunu36ptvam73gZov7gNvbdLh+mjSLdqFHIUivw3olY3ej7dSir/8eqenp5fOsVi8VZhtedcu1RkzVIcPVz300NLgffLJqvffr/rDD0HNAhDu9y7e/n2KWeAwJkTVmrvrxx9Vx41T7dKl9MI2bVRvvNFVrWzapDp9us6u37/CaPXjWByw624w061EspQSrg9Ff8+Ynu7ngqIi1fnzXfDu1q30gsxM1ZEjVf/7X9U9eyrkPdyzJcTbjAzeLHAYE6Iam7tr5Uq33nefPqqpqaVR4IIL9M88q+34Xq/kWZ1NLy0kSRV0Ie31dsZpa34OuXtvKKWU6k6DX92p+AMJ9GxBWb9edcoU1fPOU61fv/R9v/BC1RdeUP3tt4jMzxbLc8BVxgKHMSEKyzfFnTtd1co116gefnjJjZdwrD7JdXo1T+vfGatz+UPJsa/orjfymB7KupCDh7cRI0KblyotzUf7Qjn+PhRFQnuffJVaqh04vO3erTpzpnvfW7YsyeSXnKh3cJ924Dv1riqsyRmhwz3rdPnqvcr+zUJhgcOYKghr3XRRkc584Hu9mUd0FueWjFgvJEm/oZtmc4m+z1m6hGNVcaPVP+Z0vZqntRkbK/3gT04OPI16qJuvoBnofO8qpUDvo78A7W8pYb9VVSG87/rtt6pjx+q3dbuX3DiXVvoU12of3tWjW+VX80VKhbPEUdmo/+p+0bHAYUwM8fWBnsJePYXPdAxj9FNOLWn72EuKbqZJmZMLSNb3OKfS0eo1MX26r5JIUlJw5xa3yfgruU2f7r8UlJ5esTt0SkrNtz+0rv+rXsHz+m8uKJk1eV9qmuoFF6g+95zqr79W6d/WexR+uNo4gvlSUJ0AZYHDmBgR7NxQaezQPryr93ObfsqpugcfC6R7trc4Xy/m1TJrq1d1uvSa3NLT/QeZ9PTKVySMdI+no1vl65y/vqd67bVlqhL1hBNUx45VXbAgYC+tQF2Nw/EcwcxsXJ0qsWADh40cNybM/E25UZlU9tCV+ZzC55zKXE5lLk35vcJ5u6nP7SmPMrlgWMDR6rEu6tOwqMIPP8A778DMmfD11y7tsMNKR6+feSbUr19yid/pVML0LMH+LWVkwLhxkJUV2v1tyhELHCZGBDPlhgi0auX+swOMGgWbN5c7hyKOYRmnMpeT+IorecHnvf7OveTQjYV0ZAMHA7E/TUeDBjBlSugfdGH122/w3nsukHzwAezc6YJG794uiPTrR1LLQ33+24ZrSpnsbBg+HHbvrvzcqrynNuWIMTGisnrpQHXSlXah3b1b9X//03Udz/V5899orrPppY9yk17GS9qJBZpKftSqsXylFzfql3/umBpAt2eP6vvvu/EhXv+gC+t21bu4u8KAznB2vfXVq6oqf1u+YG0cxsSGQG0cNdFoWv7+B7BVT+djvYEJ+ixX6nd1u2lBSr2SE/YnJevS5LaazSV6G/fr+bylR/OjJlOgycmqvXqF3sgu4oZMBAoawXZ3jvkBdEVFboT6Aw/oxjZ/KFk5ch2H6jNcpRfVnaEvP78rolmqqS7AFjiMiSHevapqer2LoLp/Fha60e2vvqo6erTqgAGqrVqVvSAlRbVtW9WLLtLvz/+7Xt8sW7swX9NTtlVaYiruMeVrkkjvaUOCKUnE2wC61ydt1JvSp+prDNTt4uYn03r13Nxakye7ubbCrKbes2ADh7VxGBPn/LWhBFXPvn07/PgjLF1auv34I/z8M+zfX3Ja/gEt+H73UfxYeBQrOIrfGh7FH/96FOdefxQ0aVJyXnZ22faZ9HSYMCG0evZqPU+07dsHn37q2kXeeQdWrXLpnTu7dpH+/aFrV/eQNchX20c42zgscBgTIdnZMHo0rFlT2hBeE43BYenZs3cvrFgBy5a5n97b2rVlz23aFFq3di/oa2vSJKR1NCLdUylsVF0gnjnTBZEvvnCR7+CDoV8/F0h694a0tBp5uZr4+7LAYYHDxJCa+kYY6Xv7lJ/vvkmvWAHLl7stN9d92q9e7Y57a9jQfZJ5B5NWrVw310MPdT8blK6cGPHniZTNm0t7af33v660l5rquvgWl0YOPzyqWbTAYYHDxJBwf4sOV2km5Ne8VGHTptIgsnq1O8F7f8uWijdr3LhMIFm05VBe/fwwlm45mKSDmjPkLy0YMKy5K71UsZonXO9Rle5bUACffVZaGlmxwqV37FgaRE44ocartCpjgcMCh4kh4aq3j0bAKH7d8qUCEbjmGpg0qZKLd+50Gf7lF7etX1/x919/hcLCitcmJ7uGk+bNg9vS06FOnbCVYmrkvqquSrA4iHz+uWtfatGitErrrLNcyS3MLHBY4DAxJBwlDp9rpUeoSsff84jAtGmhv36FAHhvEVln58GGDZCXV7pt3Fh2v3jzVYopzlCTJizf2pwNRc3Jo3TbQlPqNG3MQ880diWe4u3AA93PevWq/D5UqyS5ZYurypo501Vtbd0KdevCGWeUjmDPyKjizQOzwGGBw8SQmv7Gm50NQ4b4LsVEohE50Gj4UF+/Rt6bwkLXhuAjqPz0eR7ffegdMvJoxiaSqaSoV7du2YDiY7t57IFspTHbym3bacyG/MauDSOEjgEVFBS4EkhxaeSnn1x6+/alQaR7d1cSqwEWOCxwmBhTk9VKgeYsikS31Zp8/XC3//i6v1BEI3bQ7rCtfP7uNtjmZ9u61f+xHTsqf/GkJBcF09Lcz+r+vn49fPSR25Ytc6/RvDn07VtapXXAAVV+r2IucIhIH2ACkAw8p6r/KHc8Ffgn0BXYDPxJVXM9x24HrgT2Azeo6vuBXssCh0l0NfmNvypqssQT7nEbgd6r6dOrUa23fz+vPb+DMaO2UndPaXmjRd1tXHPJNk44epsrRu3eDbt2Vf578VYdKSkwZoz7hlIFwQaOOlW6e+iZSQYmAmcB64B5IjJDVZd4nXYl8LuqHiUig4AHgT+JSFtgEHA8cCjwoYgcrar7MaaWatXKfxtD8USJ4ZSV5WpQfLWxhPr6/p6lVavq5bGy+6enV7MtKDmZ/xt+IAVpBzJ6NPzgVZI8oar3LSqCPXsqDzCBAlGbNtV4qCAFM7y8uhtwMvC+1/7twO3lznkfONnzex1gE25azzLnep/nb7MpR0yiC+fa36Hmo7qTEYZ7bqqYn/sqhhDklCOR6iR8GOA93HSdJ83nOapaCGwD0oO8FhEZLiI5IpKTl5dXg1k3JvZkZbnG44wMV8rIyHC9mSrtChuGfOTmui/KublV+wbv61lqsmdYuO9fG0WkqioSVHUKMAVcG0eUs2NM2GVlJc6HX7ifJZHeq1gQqRLHesB7LH1LT5rPc0SkDtAY10gezLXGGGMiJFKBYx7QRkRai0hdXGP3jHLnzACGen4fCHzkqXObAQwSkVQRaQ20Ab6JUL6NMcaUE5GqKlUtFJGRuIbtZOAFVV0sImNxjTEzgOeBaSKyAtiCCy54znsNWAIUAtep9agyxpiosQGAxhhjgODHcUR26kVjjDFxzwKHMcaYkCRkVZWI5AF+ZtIJSjPcAMTapDY+M9TO566Nzwy187lDfeYMVW1e2UkJGTiqS0RygqnnSyS18Zmhdj53bXxmqJ3PHa5ntqoqY4wxIbHAYYwxJiQWOHybEu0MREFtfGaonc9dG58Zaudzh+WZrY3DGGNMSKzEYYwxJiQWOLyISB8RWSYiK0TktmjnpyaJyAsislFEFnmlNRWR2SKy3POziSddROQJz/vwvYh0iV7Oq05EDheRj0VkiYgsFpFRnvSEfW4RqSci34jIQs8z3+NJby0iX3ue7VXPnHF45oB71ZP+tYhkRjP/1SUiySLyrYjM9Own9HOLSK6I/CAi34lIjict7H/fFjg8vFYpPBdoC1ziWX0wUbwE9CmXdhswR1XbAHM8++DegzaebTjwdITyWNMKgb+oalvgJOA6z79pIj/3XuBMVe0IdAL6iMhJuBU1x6vqUcDvuBU3wWvlTWC857x4NgpY6rVfG577DFXt5NXtNvx/38Gs9lQbNoJYpTDeNyATWOS1vww4xPP7IcAyz+/PAJf4Oi+eN+Bt3PLFteK5gQbAAuBE3CCwOp70kr91/Ky8Ge28V/F5W3o+KM8EZuJWEE3o5wZygWbl0sL+920ljlJBrTSYYA5S1V89v28ADvL8nnDvhacqojPwNQn+3J7qmu+AjcBs4Gdgq7qVNaHsc/lbeTMePQ78DSjy7KeT+M+twAciMl9EhnvSwv73nTArAJrqUVUVkYTsYiciDYE3gRtVdbuIlBxLxOdWt+xAJxE5EHgLODbKWQo7EekPbFTV+SLSM9r5iaBTVXW9iLQAZovIj94Hw/X3bSWOUrVxpcHfROQQAM/PjZ70hHkvRCQFFzSyVfXfnuSEf24AVd0KfIyrojnQs7ImlH0ufytvxptTgPNEJBd4BVddNYEEf25VXe/5uRH3JaE7Efj7tsBRKphVChON96qLQ3FtAMXpl3l6YZwEbPMq+sYNcUWL54GlqvqY16GEfW4Rae4paSAi9XFtOktxAWSg57Tyz+xr5c24oqq3q2pLVc3E/d/9SFWzSODnFpE0EWlU/DtwNrCISPx9R7txJ5Y2oC/wE65OeHS081PDz/Yy8CtQgKvbvBJXpzsHWA58CDT1nCu4HmY/Az8A3aKd/yo+86m4OuDvge88W99Efm6gA/Ct55kXAXd50o/ALbm8AngdSPWk1/Psr/AcPyLaz1AD70FPYGaiP7fn2RZ6tsXFn1mR+Pu2kePGGGNCYlVVxhhjQmKBwxhjTEgscBhjjAmJBQ5jjDEhscBhjDEmJBY4jClHRNI9s41+JyIbRGS91/4XYXrNziLyfBWv/bB4BlRjIsG64xoTgIjcDexU1UfC/DqvA/ep6sIqXDsUaKmq42o+Z8ZUZCUOY0IgIjs9P3uKyP9E5G0RWSki/xCRLM9aGD+IyJGe85qLyJsiMs+zneLjno2ADsVBQ0TuFpFpIvKlZ02Fqzzph4jIp56SzyIR6eG5xQzgkoi8AcZgkxwaUx0dgeOALcBK4DlV7S5uwajrgRtx8yWNV9W5ItIKN533ceXu0w03yttbB9waImnAtyIyCxcc3lfVcZ71YxoAqOrvnoWJ0lU17uZbMvHHAocxVTdPPXP9iMjPwAee9B+AMzy/9wbaes3Ie4CINFTVnV73OQTIK3fvt1U1H8gXkY9xk9fNA17wTNz4H1X9zuv8jcChxOFEfSb+WFWVMVW31+v3Iq/9Ikq/lCUBJ6lboa2Tqh5WLmgA5OPmTvJWvvFRVfVT4DTcjKYvichlXsfree5jTNhZ4DAmvD7AVVsBICKdfJyzFDiqXNr54tYPT8dN2jdPRDKA31T1WeA5oIvnngIcjFsNzpiws8BhTHjdAHQTke9FZAlwTfkTVPVHoHHxFNke3+OmBP8KuFdVf8EFkIUi8i3wJ1z7CUBX4CstXenOmLCy7rjGxAARuQnYoarPhdoFWEQmADNUdU4482hMMStxGBMbnqZsm0koFlnQMJFkJQ5jjDEhsRKHMcaYkFjgMMYYExILHMYYY0JigcMYY0xILHAYY4wJiQUOY4wxIfl/CwmHnVGIcoAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf1e457b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf1e1c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"load TA data\"\"\"\n",
    "#experiment name\n",
    "experiment = ''\n",
    "\n",
    "times, decaytrace = peak1['Time'], peak1['Height']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"exponential decay parameters\"\"\"\n",
    "a1_bounds = (0, 2)\n",
    "tau1_bounds = (0, 1000)\n",
    "beta1_bounds = (0,1)\n",
    "\n",
    "sing_expdec_bounds = [a1_bounds, tau1_bounds]\n",
    "exp_stret_bounds = [a1_bounds, tau1_bounds, beta1_bounds]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"fit data\"\"\"\n",
    "fit_data_sing_expdec = fit_single_exp_diffev(times, decaytrace, sing_expdec_bounds)\n",
    "\n",
    "#fit_data_exp_stretch = fit_exp_stretch_diffev(times, decaytrace, exp_stret_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1.01900  \n",
      "tau = 196.65891 ps  \n",
      "R-square = 0.99740\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf1efb6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bf1ed0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"load TA data\"\"\"\n",
    "#experiment name\n",
    "experiment = ''\n",
    "\n",
    "times, decaytrace = peak3['Time'], peak3['Height']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"exponential decay parameters\"\"\"\n",
    "a1_bounds = (0, 2)\n",
    "tau1_bounds = (0, 10000)\n",
    "beta1_bounds = (0,1)\n",
    "\n",
    "sing_expdec_bounds = [a1_bounds, tau1_bounds]\n",
    "exp_stret_bounds = [a1_bounds, tau1_bounds, beta1_bounds]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"fit data\"\"\"\n",
    "fit_data_sing_expdec = fit_single_exp_diffev(times, decaytrace, sing_expdec_bounds)\n",
    "\n",
    "#fit_data_exp_stretch = fit_exp_stretch_diffev(times, decaytrace, exp_stret_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
